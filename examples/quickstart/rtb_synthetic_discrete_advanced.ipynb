{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Example of Advanced Off-Policy Evaluation and Selection (Discrete)\n",
    "This notebook provides an example of conducting advanced **Off-policy Evaluation (OPE) and Off-Policy Selection OPS)** with a synthetic Real-Time Bidding (RTB) dataset.\n",
    "\n",
    "This example consists of the following 3 steps: \n",
    "1. Setup, Synthetic Data Generation, and Offline Policy Learning\n",
    "2. Various Off-Policy Evaluation (Policy Value Estimate, Cumulative Distribution Function Estimate, Distributionally Robust Estimate)\n",
    "3. Off-Policy Selection\n",
    "\n",
    "\\* This library uses [d3rlpy](https://github.com/takuseno/d3rlpy)'s algorithm implementations of offline rl policies and model-based evaluation.  \n",
    "\\* Also, our implementations of OPE are highly inspired by [Open Bandit Pipeline](https://github.com/st-tech/zr-obp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OFRL modules\n",
    "import ofrl\n",
    "from rtbgym import RTBEnv, CustomizedRTBEnv\n",
    "from ofrl.dataset import SyntheticDataset\n",
    "from ofrl.policy import OnlineHead\n",
    "from ofrl.policy import DiscreteEpsilonGreedyHead as EpsilonGreedyHead\n",
    "from ofrl.policy import DiscreteSoftmaxHead as SoftmaxHead\n",
    "from ofrl.ope.online import (\n",
    "    calc_on_policy_policy_value,\n",
    "    visualize_on_policy_policy_value,\n",
    ")\n",
    "\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.algos import DiscreteRandomPolicy as RandomPolicy\n",
    "from d3rlpy.preprocessing import MinMaxActionScaler\n",
    "\n",
    "# import from other libraries\n",
    "import gym\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version\n",
    "print(ofrl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "random_state = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log directory\n",
    "from pathlib import Path\n",
    "Path(\"logs/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup, Synthetic Data Generation, and Offline Policy Learning\n",
    "To begin with, we briefly describe how the RTB environment works.\n",
    "\n",
    "#### RL setup for RTB\n",
    "In RTB, the objective of the RL agent is to maximize some KPIs (number of clicks or conversions) within an episode under given budget constraints.  \n",
    "\n",
    "We often try to achieve this by adjusting bidding price function parameter $\\alpha$. By using $\\alpha$, we can adjust the bid price as follows.  \n",
    "$bid_{t,i} = \\alpha \\cdot r^{\\ast}$, \n",
    "where $r^{\\ast}$ denotes a predicted or expected reward (KPIs).\n",
    "\n",
    "We often formulate this RTB problem as the following Constrained Markov Decision Process (CMDP):\n",
    "- `timestep`: One episode (a day or a week) consists of several timesteps (24 hours or seven days, for instance).\n",
    "- `state`: We observe some feedback from the environment at each timestep, which includes the following.\n",
    "  - timestep\n",
    "  - remaining budget\n",
    "  - impression level features (budget consumption rate, cost per mille of impressions, auction winning rate, reward) at the previous timestep\n",
    "  - adjust rate (RL agent's decision making) at the previous timestep\n",
    "- `action`: Agent chooses adjust rate parameter $\\alpha$ to maximize KPIs.\n",
    "- `reward`: Total number of clicks or conversions obtained during the timestep.\n",
    "- `constraints`: The pre-determined episodic budget should not be exceeded.\n",
    "\n",
    "For more about the environmental configuration and its customization, please refer to [examples/quickstart/rtb_synthetic_customize_env.ipynb](https://github.com/negocia-inc/rtb_reinforcement_learing/blob/ope/examples/quickstart/rtb_synthetic_customize_env.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized environment for discrete action\n",
    "env = gym.make(\"RTBEnv-discrete-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data Generation\n",
    "Next, we collect logged data with the uniform random agent.\n",
    "\n",
    "`SyntheticDataset` class has the following arguments:\n",
    "- `env`: RTB environment for RL defined in the previous section.\n",
    "- `behavior_policy`: RL agent (or algorithm) used for the data collection.\n",
    "- `n_samples_pretrain_reward_predictor`: Numbers of samples used to pretrain reward predictor.\n",
    "\n",
    "For more about the data collection and visualization, please refer to [examples/quickstart/rtb_synthetic_data_collection.ipynb](https://github.com/negocia-inc/rtb_reinforcement_learing/blob/ope/examples/quickstart/rtb_synthetic_data_collection.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behavior policy\n",
    "from d3rlpy.algos import DoubleDQN\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "\n",
    "# model\n",
    "ddqn = DoubleDQN(\n",
    "    encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    target_update_interval=100,\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    ")\n",
    "# replay buffer\n",
    "buffer = ReplayBuffer(\n",
    "    maxlen=10000,\n",
    "    env=env,\n",
    ")\n",
    "# explorers\n",
    "explorer = LinearDecayEpsilonGreedy(\n",
    "    start_epsilon=1.0,\n",
    "    end_epsilon=0.1,\n",
    "    duration=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "# skip if there is a pre-trained model\n",
    "ddqn.fit_online(\n",
    "    env,\n",
    "    buffer,\n",
    "    explorer=explorer,\n",
    "    eval_env=env,\n",
    "    n_steps=100000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    update_start_step=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "ddqn.save_model(\"d3rlpy_logs/ddqn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "ddqn.build_with_env(env)\n",
    "ddqn.load_model(\"d3rlpy_logs/ddqn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy = EpsilonGreedyHead(\n",
    "    ddqn, \n",
    "    n_actions=env.action_space.n,\n",
    "    epsilon=0.3,\n",
    "    name=\"ddqn_epsilon_0.3\",\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset class\n",
    "dataset = SyntheticDataset(\n",
    "    env=env,\n",
    "    behavior_policy=behavior_policy,\n",
    "    is_rtb_env=True,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect logged data by a behavior policy\n",
    "# skip if there is a preserved logged dataset\n",
    "logged_dataset = dataset.obtain_trajectories(n_episodes=10000, obtain_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/dataset_discrete_ddpn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(logged_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/dataset_discrete_ddpn.pkl\", \"rb\") as f:\n",
    "    logged_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline Policy Learning\n",
    "Here, we learn several \"candidate\" policies to be evaluated and selected using [d3rlpy](https://github.com/takuseno/d3rlpy)'s algorithm implementation.\n",
    "\n",
    "For more about the offline RL procedure, please refer to [examples/quickstart/rtb_synthetic_discrete_basic.ipynb](https://github.com/negocia-inc/rtb_reinforcement_learing/blob/ope/examples/quickstart/rtb_synthetic_discrete_basic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules from d3rlpy\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from d3rlpy.algos import DiscreteCQL as CQL\n",
    "# transform offline dataset for d3rlpy\n",
    "offlinerl_dataset = MDPDataset(\n",
    "    observations=logged_dataset[\"state\"],\n",
    "    actions=logged_dataset[\"action\"],\n",
    "    rewards=logged_dataset[\"reward\"],\n",
    "    terminals=logged_dataset[\"done\"],\n",
    "    episode_terminals=logged_dataset[\"done\"],\n",
    "    discrete_action=True,\n",
    ")\n",
    "train_episodes, test_episodes = train_test_split(offlinerl_dataset, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Conservative Q-Learning (CQL) policy\n",
    "# base policy 1: hidden_units = [30, 30]\n",
    "cql_b1 = CQL(\n",
    "    encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql_b1.fit(\n",
    "    train_episodes,\n",
    "    eval_episodes=test_episodes,\n",
    "    n_steps=10000,\n",
    "    scorers={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "cql_b1.save_model(\"d3rlpy_logs/cql_discrete_b1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "cql_b1.build_with_env(env)\n",
    "cql_b1.load_model(\"d3rlpy_logs/cql_discrete_b1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Conservative Q-Learning (CQL) policy\n",
    "# base policy 2: hidden_units = [100]\n",
    "cql_b2 = CQL(\n",
    "    encoder_factory=VectorEncoderFactory(hidden_units=[100]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql_b2.fit(\n",
    "    train_episodes,\n",
    "    eval_episodes=test_episodes,\n",
    "    n_steps=10000,\n",
    "    scorers={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "cql_b2.save_model(\"d3rlpy_logs/cql_discrete_b2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "cql_b2.build_with_env(env)\n",
    "cql_b2.load_model(\"d3rlpy_logs/cql_discrete_b2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Conservative Q-Learning (CQL) policy\n",
    "# base policy 3: hidden_units = [50, 10]\n",
    "cql_b3 = CQL(\n",
    "    encoder_factory=VectorEncoderFactory(hidden_units=[50, 10]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql_b3.fit(\n",
    "    train_episodes,\n",
    "    eval_episodes=test_episodes,\n",
    "    n_steps=10000,\n",
    "    scorers={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "cql_b3.save_model(\"d3rlpy_logs/cql_discrete_b3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "cql_b3.build_with_env(env)\n",
    "cql_b3.load_model(\"d3rlpy_logs/cql_discrete_b3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Various Off-Policy Evaluation\n",
    "In this section, we aim at evaluating the various performance statistics of policies in an offline manner.\n",
    "\n",
    "#### Basic Estimation\n",
    "\n",
    "The most common approach in OPE is to estimate the estimators' average policy performance, the *policy value*.\n",
    "\n",
    "$$ V(\\pi) := \\mathbb{E}\\left[\\sum_{t=1}^T \\gamma^{t-1} r_t \\mid \\pi \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ope modules from OFRL\n",
    "from ofrl.ope import CreateOPEInput\n",
    "from ofrl.ope import DiscreteOffPolicyEvaluation as BasicOPE\n",
    "from ofrl.ope import DiscreteDirectMethod as DM\n",
    "from ofrl.ope import DiscreteTrajectoryWiseImportanceSampling as TIS\n",
    "from ofrl.ope import DiscretePerDecisionImportanceSampling as PDIS\n",
    "from ofrl.ope import DiscreteDoublyRobust as DR\n",
    "from ofrl.ope import DiscreteSelfNormalizedTrajectoryWiseImportanceSampling as SNTIS\n",
    "from ofrl.ope import DiscreteSelfNormalizedPerDecisionImportanceSampling as SNPDIS\n",
    "from ofrl.ope import DiscreteSelfNormalizedDoublyRobust as SNDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation policies (epsilon-greedy and softmax)\n",
    "cql_b1_00 = EpsilonGreedyHead(base_policy=cql_b1, n_actions=env.action_space.n, name=\"cql_b1_epsilon00\", epsilon=0.0, random_state=random_state)\n",
    "cql_b1_03 = EpsilonGreedyHead(base_policy=cql_b1, n_actions=env.action_space.n, name=\"cql_b1_epsilon03\", epsilon=0.3, random_state=random_state)\n",
    "cql_b1_07 = EpsilonGreedyHead(base_policy=cql_b1, n_actions=env.action_space.n, name=\"cql_b1_epsilon07\", epsilon=0.7, random_state=random_state)\n",
    "cql_b1_soft = SoftmaxHead(base_policy=cql_b1, n_actions=env.action_space.n, name=\"cql_b1_softmax\", random_state=random_state)\n",
    "\n",
    "cql_b2_00 = EpsilonGreedyHead(base_policy=cql_b2, n_actions=env.action_space.n, name=\"cql_b2_epsilon00\", epsilon=0.0, random_state=random_state)\n",
    "cql_b2_03 = EpsilonGreedyHead(base_policy=cql_b2, n_actions=env.action_space.n, name=\"cql_b2_epsilon03\", epsilon=0.3, random_state=random_state)\n",
    "cql_b2_07 = EpsilonGreedyHead(base_policy=cql_b2, n_actions=env.action_space.n, name=\"cql_b2_epsilon07\", epsilon=0.7, random_state=random_state)\n",
    "cql_b2_soft = SoftmaxHead(base_policy=cql_b2, n_actions=env.action_space.n, name=\"cql_b2_softmax\", random_state=random_state)\n",
    "\n",
    "cql_b3_00 = EpsilonGreedyHead(base_policy=cql_b3, n_actions=env.action_space.n, name=\"cql_b3_epsilon00\", epsilon=0.0, random_state=random_state)\n",
    "cql_b3_03 = EpsilonGreedyHead(base_policy=cql_b3, n_actions=env.action_space.n, name=\"cql_b3_epsilon03\", epsilon=0.3, random_state=random_state)\n",
    "cql_b3_07 = EpsilonGreedyHead(base_policy=cql_b3, n_actions=env.action_space.n, name=\"cql_b3_epsilon07\", epsilon=0.7, random_state=random_state)\n",
    "cql_b3_soft = SoftmaxHead(base_policy=cql_b3, n_actions=env.action_space.n, name=\"cql_b3_softmax\", random_state=random_state)\n",
    "\n",
    "evaluation_policies = [\n",
    "    cql_b1_00, cql_b1_03, cql_b1_07, cql_b1_soft, \n",
    "    cql_b2_00, cql_b2_03, cql_b2_07, cql_b2_soft, \n",
    "    cql_b3_00, cql_b3_03, cql_b3_07, cql_b3_soft,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, prepare OPE inputs\n",
    "prep = CreateOPEInput(\n",
    "    logged_dataset=logged_dataset,\n",
    "    use_base_model=True,  # use model-based prediction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes time\n",
    "input_dict = prep.obtain_whole_inputs(\n",
    "    evaluation_policies=evaluation_policies,\n",
    "    env=env,\n",
    "    n_episodes_on_policy_evaluation=100,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/ope_input_dict_discrete_advanced.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/ope_input_dict_discrete_advanced.pkl\", \"rb\") as f:\n",
    "    input_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope = BasicOPE(\n",
    "    logged_dataset=logged_dataset,\n",
    "    ope_estimators=[DM(), TIS(), PDIS(), DR(), SNTIS(), SNPDIS(), SNDR()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy value estimation by ope\n",
    "policy_value_df_dict, policy_value_interval_df_dict = ope.summarize_off_policy_estimates(input_dict, random_state=random_state)\n",
    "# dictionary of the estimation\n",
    "policy_value_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize and compare the result\n",
    "ope.visualize_off_policy_estimates(input_dict, random_state=random_state, sharey=False)\n",
    "# relative policy value to the behavior policy\n",
    "# ope.visualize_off_policy_estimates(input_dict, random_state=random_state, is_relative=True, sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also compare the policy value of candidate OPE estimators estimated by a single OPE estimator\n",
    "ope.visualize_off_policy_estimates(input_dict, random_state=random_state, hue=\"policy\", sharey=False)\n",
    "# relative policy value to the behavior policy\n",
    "# ope.visualize_off_policy_estimates(input_dict, random_state=random_state, is_relative=True, hue=\"policy\", sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Distribution Function Estimation\n",
    "We can also estimate various statistics regarding the policy performance based on the cumulative distribution function of the trajectory wise reward.\n",
    "\n",
    "$$ F(t, \\pi) := \\mathbb{E}\\left[ \\mathbb{I} \\left \\{ \\sum_{t=1}^T \\gamma^{t-1} r_t \\leq t \\right \\} \\mid \\pi \\right] $$\n",
    "\n",
    "Specifically, we can estimate the following performance metrics.\n",
    "- Cumulative Distribution Function\n",
    "- Mean of the return (i.e., policy value)\n",
    "- Variance of the return\n",
    "- Conditional Value at Risk\n",
    "- Interquartile Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ofrl.ope import DiscreteCumulativeDistributionalOffPolicyEvaluation as CumulativeDistributionalOPE\n",
    "from ofrl.ope import DiscreteCumulativeDistributionalDirectMethod as CD_DM\n",
    "from ofrl.ope import DiscreteCumulativeDistributionalTrajectoryWiseImportanceSampling as CD_IS\n",
    "from ofrl.ope import DiscreteCumulativeDistributionalTrajectoryWiseDoublyRobust as CD_DR\n",
    "from ofrl.ope import DiscreteCumulativeDistributionalSelfNormalizedTrajectoryWiseImportanceSampling as CD_SNIS\n",
    "from ofrl.ope import DiscreteCumulativeDistributionalSelfNormalizedTrajectoryWiseDoublyRobust as CD_SNDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_ope = CumulativeDistributionalOPE(\n",
    "    logged_dataset=logged_dataset,\n",
    "    ope_estimators=[\n",
    "        CD_DM(estimator_name=\"cdf_dm\"), \n",
    "        CD_IS(estimator_name=\"cdf_is\"), \n",
    "        CD_DR(estimator_name=\"cdf_dr\"), \n",
    "        CD_SNIS(estimator_name=\"cdf_snis\"), \n",
    "        CD_SNDR(estimator_name=\"cdf_sndr\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative distribution function\n",
    "cd_ope.visualize_cumulative_distribution_function(input_dict, n_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative distribution function comparison by each OPE estimator\n",
    "# legend (evaluation policy name) is omitted here\n",
    "cd_ope.visualize_cumulative_distribution_function(input_dict, hue=\"policy\", n_cols=6, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy value calculated from cumulative distribution function\n",
    "mean_dict = cd_ope.estimate_mean(input_dict)\n",
    "variance_dict = cd_ope.estimate_variance(input_dict)\n",
    "\n",
    "# visualize the policy value and its confidence intervals based on the mean and variance\n",
    "# cd_ope.visualize_policy_value(input_dict, sharey=True)\n",
    "# comparison among candidate policies by an OPE estimator\n",
    "cd_ope.visualize_policy_value(input_dict, hue=\"policy\", sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional value at risk\n",
    "cvar_dict = cd_ope.estimate_conditional_value_at_risk(input_dict, alphas=0.3)\n",
    "# visualize conditional value at risk\n",
    "cd_ope.visualize_conditional_value_at_risk(input_dict, alphas=np.linspace(0, 1, 20), n_cols=4, sharey=True)\n",
    "# comparison among candidate policies by an OPE estimator\n",
    "# cd_ope.visualize_conditional_value_at_risk(input_dict, alphas=np.linspace(0, 1, 20), n_cols=4, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interquartile range\n",
    "interquartile_range_dict = cd_ope.estimate_interquartile_range(input_dict, alpha=0.10)\n",
    "# visualize interquartile range\n",
    "# cd_ope.visualize_interquartile_range(input_dict, alpha=0.10, sharey=True)\n",
    "# comparison among candidate policies by an OPE estimator\n",
    "cd_ope.visualize_interquartile_range(input_dict, alpha=0.10, hue=\"policy\", sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Off-Policy Selection\n",
    "\n",
    "Off Policy Selection aims to select the ''best'' policy among several candidate policies based on some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ofrl.ope import OffPolicySelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = OffPolicySelection(\n",
    "    ope=ope,\n",
    "    cumulative_distributional_ope=cd_ope,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.obtain_oracle_selection_result(\n",
    "    input_dict=input_dict,\n",
    "    return_variance=True,\n",
    "    return_lower_quartile=True,\n",
    "    return_conditional_value_at_risk=True,\n",
    "    return_by_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df, metric_df = ops.select_by_policy_value(\n",
    "    input_dict=input_dict,\n",
    "    return_metrics=True,\n",
    "    return_by_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df, metric_df = ops.select_by_policy_value_via_cumulative_distributional_ope(\n",
    "    input_dict=input_dict,\n",
    "    return_metrics=True,\n",
    "    return_by_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df, metric_df = ops.select_by_policy_value_lower_bound(\n",
    "    input_dict=input_dict,\n",
    "    cis=[\"bootstrap\", \"bernstein\", \"hoeffding\", \"ttest\"],\n",
    "    return_metrics=True,\n",
    "    return_by_dataframe=True,\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df, metric_df = ops.select_by_lower_quartile(\n",
    "    input_dict=input_dict,\n",
    "    return_metrics=True,\n",
    "    return_by_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df, metric_df = ops.select_by_conditional_value_at_risk(\n",
    "    input_dict=input_dict,\n",
    "    return_metrics=True,\n",
    "    return_by_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.visualize_policy_value_for_validation(\n",
    "    input_dict=input_dict,\n",
    "    n_cols=4,\n",
    "    share_axes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.visualize_policy_value_of_cumulative_distributional_OPE_for_validation(\n",
    "    input_dict=input_dict,\n",
    "    share_axes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.visualize_policy_value_lower_bound_for_validation(\n",
    "    input_dict=input_dict,\n",
    "    cis=[\"bootstrap\", \"bernstein\", \"hoeffding\", \"ttest\"],\n",
    "    alpha=0.30,\n",
    "    share_axes=True,\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.visualize_variance_for_validation(\n",
    "    input_dict=input_dict,\n",
    "    share_axes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.visualize_lower_quartile_for_validation(\n",
    "    input_dict=input_dict,\n",
    "    share_axes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.visualize_conditional_value_at_risk_for_validation(\n",
    "    input_dict=input_dict,\n",
    "    share_axes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Audrey Huang, Liu Leqi, Zachary C. Lipton, and Kamyar Azizzadenesheli. \\\n",
    "\"Off-Policy Risk Assessment for Markov Decision Processes.\", 2022.\n",
    "\n",
    "- Nathan Kallus, Xiaojie Mao, Kaiwen Wang, and Zhengyuan Zhou. \\\n",
    "\"Doubly Robust Distributionally Robust Off-Policy Evaluation and Learning.\", 2022.\n",
    "\n",
    "- Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. \\\n",
    "\"Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation.\", 2021.\n",
    "\n",
    "- Takuma Seno and Michita Imai. \\\n",
    "\"d3rlpy: An Offline Deep Reinforcement Library.\", 2021.\n",
    "\n",
    "- Shengpu Tang and Jenna Wiens. \\\n",
    "\"Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings.\", 2021.\n",
    "\n",
    "- Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, and Tom Le Paine. \\\n",
    "\"Benchmarks for Deep Off-Policy Evaluation.\", 2021.\n",
    "\n",
    "- Audrey Huang, Liu Leqi, Zachary C. Lipton, and Kamyar Azizzadenesheli. \\\n",
    "\"Off-Policy Risk Assessment in Contextual Bandits.\", 2021.\n",
    "\n",
    "- Yash Chandak, Scott Niekum, Bruno Castro da Silva, Erik Learned-Miller, Emma Brunskill, and Philip S. Thomas. \\\n",
    "\"Universal Off-Policy Evaluation.\", 2021.\n",
    "\n",
    "- Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. \\\n",
    "\"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.\" 2020.\n",
    "\n",
    "- Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. \\\n",
    "\"Hyperparameter Selection for Offline Reinforcement Learning.\", 2020.\n",
    "\n",
    "- Nian Si, Fan Zhang, Zhengyuan Zhou, and Jose Blanchet. \\\n",
    "\"Distributional Robust Batch Contextual Bandits.\", 2020.\n",
    "\n",
    "- Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. \\\n",
    "\"Conservative Q-Learning for Offline Reinforcement Learning.\", 2020.\n",
    "\n",
    "- Nathan Kallus and Masatoshi Uehara. \\\n",
    "\"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning.\", 2019.\n",
    "\n",
    "- Hoang Le, Cameron Voloshin, and Yisong Yue. \\\n",
    "\"Batch Policy Learning under Constraints.\", 2019.\n",
    "\n",
    "- Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and Kun Gai. \\\n",
    "\"Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising.\", 2018.\n",
    "\n",
    "- Jun Zhao, Guang Qiu, Ziyu Guan, Wei Zhao, and Xiaofei He. \\\n",
    "\"Deep Reinforcement Learning for Sponsored Search Real-time Bidding.\", 2018.\n",
    "\n",
    "- Josiah P. Hanna, Peter Stone, and Scott Niekum. \\\n",
    "\"Bootstrapping with Models: Confidence Intervals for Off-Policy Evaluation.\", 2017.\n",
    "\n",
    "- Nan Jiang and Lihong Li. \\\n",
    "\"Doubly Robust Off-policy Value Evaluation for Reinforcement Learning.\", 2016.\n",
    "\n",
    "- Philip S. Thomas and Emma Brunskill. \\\n",
    "\"Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning.\", 2016.\n",
    "\n",
    "- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. \\\n",
    "\"OpenAI Gym.\", 2016.\n",
    "\n",
    "- Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. \\\n",
    "\"High Confidence Policy Improvement.\", 2015.\n",
    "\n",
    "- Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. \\\n",
    "\"High Confidence Off-Policy Evaluation.\", 2015.\n",
    "\n",
    "- Adith Swaminathan and Thorsten Joachims. \\\n",
    "\"The Self-Normalized Estimator for Counterfactual Learning.\", 2015.\n",
    "\n",
    "- Hado van Hasselt, Arthur Guez, and David Silver. \\\n",
    "\"Deep Reinforcement Learning with Double Q-learning.\", 2015.\n",
    "\n",
    "- Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li. \\\n",
    "\"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n",
    "\n",
    "- Alex Strehl, John Langford, Sham Kakade, and Lihong Li. \\\n",
    "\"Learning from Logged Implicit Exploration Data.\", 2010.\n",
    "\n",
    "- Alina Beygelzimer and John Langford. \\\n",
    "\"The Offset Tree for Learning with Partial Labels.\", 2009.\n",
    "\n",
    "- Doina Precup, Richard S. Sutton, and Satinder P. Singh. \\\n",
    "\"Eligibility Traces for Off-Policy Policy Evaluation.\", 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('3.10.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "38edd61b629bdc0bcee2dce449bef832ec37626df5f85727532abb1c987a6a29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

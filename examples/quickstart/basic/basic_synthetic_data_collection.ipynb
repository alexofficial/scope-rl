{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Example with Basic Synthetic Simulation and Dataset (Data Collection)\n",
    "This notebook provides an example of visualizing the logged dataset collected on an basic environment.\n",
    "\n",
    "This example on consists of the following 3 cases:\n",
    "1. Discrete Action Case\n",
    "2. Continuous Action Case\n",
    "3. Collecting Logged Datasets with Multiple Behavior Policies and Random Seeds\n",
    "\n",
    "\\* This library uses [d3rlpy](https://github.com/takuseno/d3rlpy)'s algorithm implementations.  \n",
    "\\* Also, our data collection module is highly inspired by [Open Bandit Pipeline](https://github.com/st-tech/zr-obp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# delete later\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "import sys\n",
    "sys.path.append('.../')\n",
    "sys.path.append('.../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ofrl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import OFRL modules\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mofrl\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbasicgym\u001b[39;00m \u001b[39mimport\u001b[39;00m BasicEnv\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mofrl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m SyntheticDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ofrl'"
     ]
    }
   ],
   "source": [
    "# import OFRL modules\n",
    "import ofrl\n",
    "from basicgym import BasicEnv\n",
    "from ofrl.dataset import SyntheticDataset\n",
    "from ofrl.policy import OnlineHead\n",
    "from ofrl.ope.online import (\n",
    "    calc_on_policy_policy_value,\n",
    "    visualize_on_policy_policy_value,\n",
    ")\n",
    "\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.algos import DiscreteRandomPolicy\n",
    "from d3rlpy.algos import RandomPolicy as ContinuousRandomPolicy\n",
    "from d3rlpy.preprocessing import MinMaxActionScaler\n",
    "\n",
    "# import from other libraries\n",
    "import gym\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Union, Optional\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version\n",
    "print(ofrl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "random_state = 12345\n",
    "random_ = check_random_state(random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Description of Synthetic Basic Simulation Environment\n",
    "To begin with, we briefly describe the basic usage of the environment.\n",
    "\n",
    "#### RL setup for Synthetic\n",
    "In Synthetic , the objective of the RL agent is to maximize reward\n",
    "\n",
    "We often formulate this synthetic  problem as the following (Partially Observable) Markov Decision Process ((PO)MDP):\n",
    "- `state`: State observation, which may be noisy in POMDPs.\n",
    "- `action`:  Indicating the action to presented by the RL agent.\n",
    "- `reward`: Reward observation.\n",
    "\n",
    "Let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "env = BasicEnv(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random agent\n",
    "agent = OnlineHead(\n",
    "    ContinuousRandomPolicy(\n",
    "        action_scaler=MinMaxActionScaler(\n",
    "            minimum=env.action_space.low,  # minimum value that policy can take\n",
    "            maximum=env.action_space.high,  # maximum value that policy can take\n",
    "        )\n",
    "    ),\n",
    "    name=\"random\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact agent with the environment\n",
    "# only 6 lines are needed for RL interaction\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.predict_online(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state \n",
    "print(obs.shape)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize the transition of the reward\n",
    "# our goal is to obtain a policy (or an agent) that maximizes the reward\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "reward_list = []\n",
    "\n",
    "while not done:\n",
    "    action = agent.sample_action_online(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    reward_list.append(reward)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(reward_list[:-1], label='reward', color='tab:orange')\n",
    "ax1.set_xlabel('timestep')\n",
    "ax1.set_ylabel('reward')\n",
    "ax1.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more about the environmental configuration , please refer to [examples/quickstart/basic/basic_synthetic_customize_env.ipynb](https://github.com/negocia-inc/ofrl/blob/ope/examples/quickstart/basic/basic_synthetic_customize_env.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discrete Action Case\n",
    "Here, we present how to collect logged data by a behavior policy in the case of discrete action.\n",
    "\n",
    "The procedure requires two steps:\n",
    "\n",
    "1. Learn a base deterministic policy\n",
    "2. Convert the deterministic policy into a stochastic policy.\n",
    "\n",
    "Below, we first learn a deterministic policy using [d3rlpy](https://github.com/takuseno/d3rlpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized environment for discrete action\n",
    "env = gym.make(\"BasicEnv-discrete-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for api compatibility to d3rlpy\n",
    "from ofrl.utils import OldGymAPIWrapper\n",
    "env_ = OldGymAPIWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn a base deterministic policy for data collection\n",
    "from d3rlpy.algos import DoubleDQN\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "\n",
    "# model\n",
    "ddqn = DoubleDQN(\n",
    "    encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    target_update_interval=100,\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    ")\n",
    "# replay buffer\n",
    "buffer = ReplayBuffer(\n",
    "    maxlen=10000,\n",
    "    env=env_,\n",
    ")\n",
    "# explorers\n",
    "explorer = LinearDecayEpsilonGreedy(\n",
    "    start_epsilon=1.0,\n",
    "    end_epsilon=0.1,\n",
    "    duration=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "# skip if there is a pre-trained model\n",
    "ddqn.fit_online(\n",
    "    env_,\n",
    "    buffer,\n",
    "    explorer=explorer,\n",
    "    eval_env=env_,\n",
    "    n_steps=100000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    update_start_step=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "ddqn.save_model(\"d3rlpy_logs/ddqn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "ddqn.build_with_env(env)\n",
    "ddqn.load_model(\"d3rlpy_logs/ddqn.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Epsilon-Greedy behavior policy\n",
    "\n",
    "Let's now convert the deterministic policy (i.e., ddqn policy) into a stochastic behavior policy.\n",
    "\n",
    "We use epsilon-greedy policy to collect logged data using `DiscreteEpsilonGreedyHead`, where the behavior policy greedily takes an action chosen by the deterministic policy with probability $1 - \\epsilon$ and takes an action randomly with probability $\\epsilon$ as follows.\n",
    "\n",
    "$$\\pi(a | s) := (1 - \\epsilon) * \\pi_{\\mathrm{det}}(a | s) + \\epsilon / |\\mathcal{A}|,$$\n",
    "\n",
    "where $a \\in \\mathcal{A}$ is an action, $s \\in \\mathcal{S}$ is a state, and $\\pi$ is a stochastic policy defined with the (deterministic) behavior policy $\\pi_{\\mathrm{det}}$.\n",
    "\n",
    "`SyntheticDataset` has the following arguments:\n",
    "- `env`: Synthetic environment for RL defined in the previous section.\n",
    "- `max_episode_steps`: Maximum number of timesteps in an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the base ddqn policy into a stochastic data collection policy\n",
    "from ofrl.policy import DiscreteEpsilonGreedyHead\n",
    "\n",
    "behavior_policy = DiscreteEpsilonGreedyHead(\n",
    "    ddqn, \n",
    "    n_actions=env.action_space.n,\n",
    "    epsilon=0.3,  # probability of taking random action\n",
    "    name=\"ddqn_epsilon_0.3\",\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "dataset = SyntheticDataset(\n",
    "    env=env,\n",
    "    max_episode_steps=env.step_per_episode,\n",
    ")\n",
    "logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000, \n",
    "    obtain_info=False,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Softmax behavior policy\n",
    "We can also use `DiscreteSoftmaxHead` to derive a stochastic behavior policy.\n",
    "\n",
    "This algorithm uses Q function of the original algorithm, which estimates the value of a given context and action pair (i.e., $(s, a)$) as $Q(s, a)$. \\\n",
    "Specifically, the behavior policy chooses actions stochastically as:\n",
    "\n",
    "$$\\pi(a \\mid s) = \\frac{\\exp(Q(s, a) / \\tau)}{\\sum_{a' \\in A} \\exp(Q(s, a') / \\tau)},$$\n",
    "\n",
    "where $A$ indicates the set discrete actions and $\\tau$ is an inverse temperature parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert base ddqn policy into a stochastic data collection policy\n",
    "from ofrl.policy import DiscreteSoftmaxHead\n",
    "\n",
    "behavior_policy = DiscreteSoftmaxHead(\n",
    "    ddqn, \n",
    "    n_actions=env.action_space.n,\n",
    "    tau=1.0,  # temperature parameter\n",
    "    name=\"ddqn_softmax_tau_1.0\",\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "dataset = SyntheticDataset(\n",
    "    env=env,\n",
    "    max_episode_steps=env.step_per_episode,\n",
    ")\n",
    "logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy, \n",
    "    n_trajectories=10000, \n",
    "    obtain_info=False,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For offline RL and OPE procedures, please refer to [examples/quickstart/basic/basic_synthetic_discrete_basic.ipynb](https://github.com/negocia-inc/ofrl/blob/master/examples/quickstart/basic/basic_synthetic_discrete_basic.ipynb).\n",
    "\n",
    "For more advanced topic in OPE and OPS, please refer to [examples/quickstart/basic/basic_synthetic_discrete_advanced.ipynb](https://github.com/negocia-inc/ofrl/blob/ope/examples/quickstart/basic/basic_synthetic_discrete_advanced.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continuous Action Space\n",
    "We also describe the case where a continuous behavior policy is used. \\\n",
    "Here, we first learn a base deterministic policy in a similar manner with the discrete action case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized environment for continuous action\n",
    "env = gym.make(\"BasicEnv-continuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for api compatibility to d3rlpy\n",
    "env_ = OldGymAPIWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn base deterministic policy for data collection\n",
    "from d3rlpy.algos import SAC\n",
    "from d3rlpy.models.encoders import VectorEncoderFactory\n",
    "from d3rlpy.models.q_functions import MeanQFunctionFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "\n",
    "# model\n",
    "sac = SAC(\n",
    "    actor_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    critic_encoder_factory=VectorEncoderFactory(hidden_units=[30, 30]),\n",
    "    q_func_factory=MeanQFunctionFactory(),\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    action_scaler=MinMaxActionScaler(\n",
    "        minimum=env.action_space.low,   # 0.1\n",
    "        maximum=env.action_space.high,  # 10\n",
    "    ),\n",
    ")\n",
    "# setup replay buffer\n",
    "buffer = ReplayBuffer(\n",
    "    maxlen=10000,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "# skip if there is a pre-trained model\n",
    "sac.fit_online(\n",
    "    env_,\n",
    "    buffer,\n",
    "    eval_env=env_,\n",
    "    n_steps=100000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    update_start_step=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "sac.save_model(\"d3rlpy_logs/sac.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "sac.build_with_env(env)\n",
    "sac.load_model(\"d3rlpy_logs/sac.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Gaussian behavior policy\n",
    "\n",
    "Then, we convert a deterministic policy (i.e., greedy action choice of sac policy) into a stochastic policy. \n",
    "\n",
    "As the action space of `BasicEnv` is bounded, we use `ContinuousTruncatedGaussianHead`. \\\n",
    "Given the deterministic action $\\pi(s)$, this behavior policy samples actions from a truncated gaussian distribution as:\n",
    "\n",
    "$$a \\sim Truncnorm(\\pi(s), \\sigma),$$\n",
    "\n",
    "where $\\sigma$ indicates the noise level. \n",
    "\n",
    "Note that, when action space is not bounded, we can use `ContinuousGaussianHead` in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert base ddqn policy into a stochastic data collection policy\n",
    "from ofrl.policy import ContinuousTruncatedGaussianHead\n",
    "\n",
    "behavior_policy = ContinuousTruncatedGaussianHead(\n",
    "    sac, \n",
    "    minimum=env.action_space.low,  # lower bound of the action space\n",
    "    maximum=env.action_space.high,  # upper bound of the action space\n",
    "    sigma=np.array([1.0]),  # noise level of a gaussian distribution\n",
    "    name=\"sac_sigma_1.0\",\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "dataset = SyntheticDataset(\n",
    "    env=env,\n",
    "    state_keys=env.obs_keys,\n",
    "    max_episode_steps=env.step_per_episode,\n",
    "    info_keys={\n",
    "        \"search_volume\": int,\n",
    "        \"impression\": int,\n",
    "        \"click\": int,\n",
    "        \"conversion\": int,\n",
    "        \"average_bid_price\": float,\n",
    "    },\n",
    ")\n",
    "logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000, \n",
    "    obtain_info=True,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collecting Logged Datasets with Multiple Behavior Policies and Random Seeds\n",
    "Finally, we show how to collect logged data by several behavior policies and random seeds.\n",
    "\n",
    "We show the example in the discrete action case, but the continuous action case can also be handled in a simmilar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized environment for discrete action\n",
    "env = gym.make(\"BasicEnv-discrete-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define several behavior policies\n",
    "behavior_policy_01 = DiscreteEpsilonGreedyHead(\n",
    "    ddqn, \n",
    "    n_actions=env.action_space.n,\n",
    "    epsilon=0.1,  # probability of taking random action\n",
    "    name=\"ddqn_eps_0.1\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "behavior_policy_03 = DiscreteEpsilonGreedyHead(\n",
    "    ddqn, \n",
    "    n_actions=env.action_space.n,\n",
    "    epsilon=0.3,  # probability of taking random action\n",
    "    name=\"ddqn_eps_0.3\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "behavior_policy_05 = DiscreteEpsilonGreedyHead(\n",
    "    ddqn, \n",
    "    n_actions=env.action_space.n,\n",
    "    epsilon=0.5,  # probability of taking random action\n",
    "    name=\"ddqn_eps_0.5\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "behavior_policy_07 = DiscreteEpsilonGreedyHead(\n",
    "    ddqn, \n",
    "    n_actions=env.action_space.n,\n",
    "    epsilon=0.7,  # probability of taking random action\n",
    "    name=\"ddqn_eps_0.7\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "behavior_policies = [behavior_policy_01, behavior_policy_03, behavior_policy_05, behavior_policy_07]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset class\n",
    "dataset = SyntheticDataset(\n",
    "    env=env,\n",
    "    max_episode_steps=env.step_per_episode,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_datasets = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policies,\n",
    "    n_datasets=2,  # number of random states\n",
    "    n_trajectories=10000, \n",
    "    obtain_info=False,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_dataset = logged_datasets.get(behavior_policy_name=behavior_policies[0].name, dataset_id=0)\n",
    "logged_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For offline RL and OPE procedures, please refer to [examples/quickstart/basic/basic_synthetic_continuous_basic.ipynb](https://github.com/negocia-inc/ofrl/blob/ope/examples/quickstart/basic/basic_synthetic_continuous_basic.ipynb). \n",
    "\n",
    "For advanced topics regarding OPE and OPS, please refer to [examples/quickstart/basic/basic_synthetic_continuous_advanced.ipynb](https://github.com/negocia-inc/ofrl/blob/ope/examples/quickstart/basic/basic_synthetic_continuous_advanced.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Yuta Saito, Shunsuke Aihara, Megumi Matsutani, and Yusuke Narita. \\\n",
    "\"Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation.\", 2021.\n",
    "\n",
    "- Takuma Seno and Michita Imai. \\\n",
    "\"d3rlpy: An Offline Deep Reinforcement Library.\", 2021.\n",
    "\n",
    "- Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. \\\n",
    "\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\" 2018.\n",
    "\n",
    "- Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and Kun Gai. \\\n",
    "\"Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising.\", 2018.\n",
    "\n",
    "- Jun Zhao, Guang Qiu, Ziyu Guan, Wei Zhao, and Xiaofei He. \\\n",
    "\"Deep Reinforcement Learning for Sponsored Search Real-time Bidding.\", 2018.\n",
    "\n",
    "- Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. \\\n",
    "\"OpenAI Gym.\", 2016.\n",
    "\n",
    "- Hado van Hasselt, Arthur Guez, and David Silver. \\\n",
    "\"Deep Reinforcement Learning with Double Q-learning.\", 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "70404ee114725fce8ed9e697d67827f8546c678889944e6d695790702cbfe1f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

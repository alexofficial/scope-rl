

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Supported Implementation &#8212; SCOPE-RL</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Visualization Tools" href="visualization.html" />
    <link rel="prev" title="Overview" href="ope_ops.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">SCOPE-RL</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="installation.html">
                        Installation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="quickstart.html">
                        Quickstart
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Documentation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="examples/index.html">
                        Usage
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="subpackages/index.html">
                        Sub-packages
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="frequently_asked_questions.html">
                        FAQs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="news.html">
                        News
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-external" href="https://github.com/hakuhodo-technologies/scope-rl/releases">
                        Release Notes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-external" href="https://github.com/hakuhodo-technologies/scope-rl/404">
                        Proceedings
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/hakuhodo-technologies/scope-rl" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://speakerdeck.com/aiueola/ofrl-designing-an-offline-reinforcement-learning-and-policy-evaluation-platform-from-practical-perspectives" title="Speaker Deck" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-speaker-deck"></i></span>
            <label class="sr-only">Speaker Deck</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="installation.html">
                        Installation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="quickstart.html">
                        Quickstart
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Documentation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="examples/index.html">
                        Usage
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="subpackages/index.html">
                        Sub-packages
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="frequently_asked_questions.html">
                        FAQs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="news.html">
                        News
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-external" href="https://github.com/hakuhodo-technologies/scope-rl/releases">
                        Release Notes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-external" href="https://github.com/hakuhodo-technologies/scope-rl/404">
                        Proceedings
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/hakuhodo-technologies/scope-rl" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://speakerdeck.com/aiueola/ofrl-designing-an-offline-reinforcement-learning-and-policy-evaluation-platform-from-practical-perspectives" title="Speaker Deck" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-speaker-deck"></i></span>
            <label class="sr-only">Speaker Deck</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>

<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="distinctive_features.html">Why SCOPE-RL?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Online &amp; Offline RL:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="online_offline_rl.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="learning_implementation.html">Supported Implementation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Off-Policy Evaluation &amp; Selection:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ope_ops.html">Overview</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Supported Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Our Proposal:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="sharpe_ratio.html">Risk-Return Assessments of OPE via SharpRatio@k</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sub-packages:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="subpackages/index.html">Gallery of Sub-packages</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Package References:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="scope_rl_api.html">SCOPR-RL Package Reference</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/dataset/scope_rl.dataset.base.html">scope_rl.dataset.base</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/dataset/scope_rl.dataset.base.BaseDataset.html">scope_rl.dataset.base.BaseDataset</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/dataset/scope_rl.dataset.synthetic.html">scope_rl.dataset.synthetic</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/dataset/scope_rl.dataset.synthetic.SyntheticDataset.html">scope_rl.dataset.synthetic.SyntheticDataset</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.policy.head.html">scope_rl.policy.head</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.policy.head.BaseHead.html">scope_rl.policy.head.BaseHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.policy.head.ContinuousEvalHead.html">scope_rl.policy.head.ContinuousEvalHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.policy.head.EpsilonGreedyHead.html">scope_rl.policy.head.EpsilonGreedyHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.policy.head.GaussianHead.html">scope_rl.policy.head.GaussianHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.policy.head.OnlineHead.html">scope_rl.policy.head.OnlineHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.policy.head.SoftmaxHead.html">scope_rl.policy.head.SoftmaxHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.policy.head.TruncatedGaussianHead.html">scope_rl.policy.head.TruncatedGaussianHead</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.input.html">scope_rl.ope.input</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.input.CreateOPEInput.html">scope_rl.ope.input.CreateOPEInput</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.ope.html">scope_rl.ope.ope</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.ope.CumulativeDistributionOPE.html">scope_rl.ope.ope.CumulativeDistributionOPE</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.ope.OffPolicyEvaluation.html">scope_rl.ope.ope.OffPolicyEvaluation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.ops.html">scope_rl.ope.ops</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.ops.OffPolicySelection.html">scope_rl.ope.ops.OffPolicySelection</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.html">scope_rl.ope.estimators_base</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.html">scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.BaseMarginalOPEEstimator.html">scope_rl.ope.estimators_base.BaseMarginalOPEEstimator</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.BaseOffPolicyEstimator.html">scope_rl.ope.estimators_base.BaseOffPolicyEstimator</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator.html">scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator.html">scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.html">scope_rl.ope.discrete.basic_estimators</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.DirectMethod.html">scope_rl.ope.discrete.basic_estimators.DirectMethod</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.DoublyRobust.html">scope_rl.ope.discrete.basic_estimators.DoublyRobust</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling.html">scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR.html">scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS.html">scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS.html">scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling.html">scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.html">scope_rl.ope.continuous.basic_estimators</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.DirectMethod.html">scope_rl.ope.continuous.basic_estimators.DirectMethod</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.DoublyRobust.html">scope_rl.ope.continuous.basic_estimators.DoublyRobust</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling.html">scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR.html">scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS.html">scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS.html">scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling.html">scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.html">scope_rl.ope.discrete.marginal_estimators</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning.html">scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR.html">scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS.html">scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR.html">scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS.html">scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalDM.html">scope_rl.ope.discrete.marginal_estimators.StateMarginalDM</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalDR.html">scope_rl.ope.discrete.marginal_estimators.StateMarginalDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalIS.html">scope_rl.ope.discrete.marginal_estimators.StateMarginalIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR.html">scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS.html">scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.html">scope_rl.ope.continuous.marginal_estimators</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning.html">scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR.html">scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS.html">scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR.html">scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS.html">scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalDM.html">scope_rl.ope.continuous.marginal_estimators.StateMarginalDM</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalDR.html">scope_rl.ope.continuous.marginal_estimators.StateMarginalDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalIS.html">scope_rl.ope.continuous.marginal_estimators.StateMarginalIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR.html">scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS.html">scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.html">scope_rl.ope.discrete.cumulative_distribution_estimators</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.html">scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.html">scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.html">scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.html">scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.html">scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.html">scope_rl.ope.continuous.cumulative_distribution_estimators</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.html">scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.html">scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.html">scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.html">scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.html">scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.base.html">scope_rl.ope.weight_value_learning.base</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.html">scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.function.html">scope_rl.ope.weight_value_learning.function</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.function.ContinuousQFunction.html">scope_rl.ope.weight_value_learning.function.ContinuousQFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction.html">scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.function.DiscreteQFunction.html">scope_rl.ope.weight_value_learning.function.DiscreteQFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction.html">scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.function.StateWeightFunction.html">scope_rl.ope.weight_value_learning.function.StateWeightFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.function.VFunction.html">scope_rl.ope.weight_value_learning.function.VFunction</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.html">scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.html">scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.html">scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.html">scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.html">scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.html">scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.html">scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.html">scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.html">scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.html">scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.html">scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.html">scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.html">scope_rl.ope.weight_value_learning.minimax_value_learning_discrete</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.html">scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.html">scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.html">scope_rl.ope.weight_value_learning.minimax_value_learning_continuous</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.html">scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.html">scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.ope.online.html">scope_rl.ope.online</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.calc_on_policy_conditional_value_at_risk.html">scope_rl.ope.online.calc_on_policy_conditional_value_at_risk</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.calc_on_policy_cumulative_distribution_function.html">scope_rl.ope.online.calc_on_policy_cumulative_distribution_function</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.calc_on_policy_interquartile_range.html">scope_rl.ope.online.calc_on_policy_interquartile_range</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.calc_on_policy_policy_value.html">scope_rl.ope.online.calc_on_policy_policy_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.calc_on_policy_policy_value_interval.html">scope_rl.ope.online.calc_on_policy_policy_value_interval</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.calc_on_policy_statistics.html">scope_rl.ope.online.calc_on_policy_statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.calc_on_policy_variance.html">scope_rl.ope.online.calc_on_policy_variance</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.rollout_policy_online.html">scope_rl.ope.online.rollout_policy_online</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk.html">scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function.html">scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.visualize_on_policy_interquartile_range.html">scope_rl.ope.online.visualize_on_policy_interquartile_range</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.visualize_on_policy_policy_value.html">scope_rl.ope.online.visualize_on_policy_policy_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.ope.online.visualize_on_policy_policy_value_with_variance.html">scope_rl.ope.online.visualize_on_policy_policy_value_with_variance</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/scope_rl.utils.html">scope_rl.utils</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.check_array.html">scope_rl.utils.check_array</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.check_input_dict.html">scope_rl.utils.check_input_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.check_logged_dataset.html">scope_rl.utils.check_logged_dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.cosine_kernel.html">scope_rl.utils.cosine_kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.defaultdict_to_dict.html">scope_rl.utils.defaultdict_to_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.epanechnikov_kernel.html">scope_rl.utils.epanechnikov_kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.estimate_confidence_interval_by_bootstrap.html">scope_rl.utils.estimate_confidence_interval_by_bootstrap</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein.html">scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.estimate_confidence_interval_by_hoeffding.html">scope_rl.utils.estimate_confidence_interval_by_hoeffding</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.estimate_confidence_interval_by_t_test.html">scope_rl.utils.estimate_confidence_interval_by_t_test</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.gaussian_kernel.html">scope_rl.utils.gaussian_kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.l2_distance.html">scope_rl.utils.l2_distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.triangular_kernel.html">scope_rl.utils.triangular_kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.uniform_kernel.html">scope_rl.utils.uniform_kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.MinMaxActionScaler.html">scope_rl.utils.MinMaxActionScaler</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.MinMaxScaler.html">scope_rl.utils.MinMaxScaler</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.MultipleInputDict.html">scope_rl.utils.MultipleInputDict</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.MultipleLoggedDataset.html">scope_rl.utils.MultipleLoggedDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.NewGymAPIWrapper.html">scope_rl.utils.NewGymAPIWrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/scope_rl.utils.OldGymAPIWrapper.html">scope_rl.utils.OldGymAPIWrapper</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="subpackages/rtbgym_api.html">RTBGym Package Reference</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/rtbgym/env/rtbgym.envs.rtb.html">rtbgym.envs.rtb</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/env/rtbgym.envs.rtb.RTBEnv.html">rtbgym.envs.rtb.RTBEnv</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/rtbgym/env/rtbgym.envs.wrapper_rtb.html">rtbgym.envs.wrapper_rtb</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/env/rtbgym.envs.wrapper_rtb.CustomizedRTBEnv.html">rtbgym.envs.wrapper_rtb.CustomizedRTBEnv</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.html">rtbgym.envs.simulator.base</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseClickAndConversionRate.html">rtbgym.envs.simulator.base.BaseClickAndConversionRate</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseSimulator.html">rtbgym.envs.simulator.base.BaseSimulator</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseWinningPriceDistribution.html">rtbgym.envs.simulator.base.BaseWinningPriceDistribution</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.html">rtbgym.envs.simulator.function</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.ClickThroughRate.html">rtbgym.envs.simulator.function.ClickThroughRate</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.ConversionRate.html">rtbgym.envs.simulator.function.ConversionRate</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.WinningPriceDistribution.html">rtbgym.envs.simulator.function.WinningPriceDistribution</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.bidder.html">rtbgym.envs.simulator.bidder</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.bidder.Bidder.html">rtbgym.envs.simulator.bidder.Bidder</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.rtb_synthetic.html">rtbgym.envs.simulator.rtb_synthetic</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/simulation/rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.html">rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/rtbgym/utils/rtbgym.utils.html">rtbgym.utils</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/utils/rtbgym.utils.check_array.html">rtbgym.utils.check_array</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/utils/rtbgym.utils.sigmoid.html">rtbgym.utils.sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/rtbgym/utils/rtbgym.utils.NormalDistribution.html">rtbgym.utils.NormalDistribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="subpackages/recgym_api.html">RECGym Package Reference</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/recgym/env/recgym.envs.rec.html">recgym.envs.rec</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/recgym/env/recgym.envs.rec.RECEnv.html">recgym.envs.rec.RECEnv</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/recgym/simulation/recgym.envs.simulator.base.html">recgym.envs.simulator.base</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/recgym/simulation/recgym.envs.simulator.base.BaseUserModel.html">recgym.envs.simulator.base.BaseUserModel</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/recgym/simulation/recgym.envs.simulator.function.html">recgym.envs.simulator.function</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/recgym/simulation/recgym.envs.simulator.function.UserModel.html">recgym.envs.simulator.function.UserModel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="subpackages/basicgym_api.html">BasicGym Package Reference</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/basicgym/env/basicgym.envs.synthetic.html">basicgym.envs.synthetic</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/basicgym/env/basicgym.envs.synthetic.BasicEnv.html">basicgym.envs.synthetic.BasicEnv</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/basicgym/simulation/basicgym.envs.simulator.base.html">basicgym.envs.simulator.base</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/basicgym/simulation/basicgym.envs.simulator.base.BaseRewardFunction.html">basicgym.envs.simulator.base.BaseRewardFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/basicgym/simulation/basicgym.envs.simulator.base.BaseStateTransitionFunction.html">basicgym.envs.simulator.base.BaseStateTransitionFunction</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="_autosummary/basicgym/simulation/basicgym.envs.simulator.function.html">basicgym.envs.simulator.function</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/basicgym/simulation/basicgym.envs.simulator.function.RewardFunction.html">basicgym.envs.simulator.function.RewardFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/basicgym/simulation/basicgym.envs.simulator.function.StateTransitionFunction.html">basicgym.envs.simulator.function.StateTransitionFunction</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">See also:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/hakuhodo-technologies/scope-rl">Github</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/hakuhodo-technologies/scope-rl/blob/main/LICENSE">LICENSE</a></li>
<li class="toctree-l1"><a class="reference internal" href="frequently_asked_questions.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="news.html">News</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/hakuhodo-technologies/scope-rl/releases">Release Notes</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/hakuhodo-technologies/scope-rl/404">Proceedings</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">SCOPE-RL</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Supported Implementation</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="supported-implementation">
<h1>Supported Implementation<a class="headerlink" href="#supported-implementation" title="Permalink to this heading">#</a></h1>
<section id="create-ope-input">
<span id="implementation-create-ope-input"></span><h2>Create OPE Input<a class="headerlink" href="#create-ope-input" title="Permalink to this heading">#</a></h2>
<p>Before proceeding to OPE/OPS, we first create <code class="xref py py-class docutils literal notranslate"><span class="pre">input_dict</span></code> to enable a smooth implementation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create input for OPE class</span>
<span class="kn">from</span> <span class="nn">scope_rl.ope</span> <span class="kn">import</span> <span class="n">CreateOPEInput</span>
<span class="n">prep</span> <span class="o">=</span> <span class="n">CreateOPEInput</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">input_dict</span> <span class="o">=</span> <span class="n">prep</span><span class="o">.</span><span class="n">obtain_whole_inputs</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">evaluation_policies</span><span class="o">=</span><span class="n">evaluation_policies</span><span class="p">,</span>
    <span class="n">require_value_prediction</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># use model-based prediction</span>
    <span class="n">n_trajectories_on_policy_evaluation</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip" id="tip-create-input-dict">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to create input_dict for multiple logged datasets?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">When obtaining <code class="xref py py-class docutils literal notranslate"><span class="pre">input_dict</span></code> from the same evaluation policies across multiple datasets, try the following command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">multiple_input_dict</span> <span class="o">=</span> <span class="n">prep</span><span class="o">.</span><span class="n">obtain_whole_inputs</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>            <span class="c1"># MultipleLoggedDataset</span>
    <span class="n">evaluation_policies</span><span class="o">=</span><span class="n">evaluation_policies</span><span class="p">,</span>  <span class="c1"># single list</span>
    <span class="o">...</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">When obtaining <code class="xref py py-class docutils literal notranslate"><span class="pre">input_dict</span></code> from different evaluation policies for each logged dataset, try the following command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">multiple_input_dict</span> <span class="o">=</span> <span class="n">prep</span><span class="o">.</span><span class="n">obtain_whole_inputs</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>                                 <span class="c1"># MultipleLoggedDataset (two logged dataset in this case)</span>
    <span class="n">evaluation_policies</span><span class="o">=</span><span class="n">evaluation_policies</span><span class="p">,</span>                       <span class="c1"># nested list or dict that have the same keys with logged_datasets</span>
    <span class="o">...</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">In both cases, <code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleInputDict</span></code> will be returned.</p>
<p class="sd-card-text"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleInputDict</span></code> saves the paths to each input_dict and make it accessible through the following command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_dict_</span> <span class="o">=</span> <span class="n">multiple_input_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">behavior_policy_name</span><span class="o">=</span><span class="n">behavior_policy</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">dataset_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tips-synthetic-dataset"><span class="std std-ref">How to obtain MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tip-opl"><span class="std std-ref">How to handle OPL with MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="_autosummary/scope_rl.utils.MultipleInputDict.html"><span class="doc">API reference of MultipleInputDict</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="examples/multiple.html"><span class="doc">Examples with MultipleLoggedDataset</span></a></p></li>
</ul>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to select models for value/weight learning methods?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">To enable value prediction (for model-based estimators) and weight prediction (for marginal estimators), set <code class="docutils literal notranslate"><span class="pre">True</span></code> for the following arguments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_dict</span> <span class="o">=</span> <span class="n">prep</span><span class="o">.</span><span class="n">obtain_whole_inputs</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span>
    <span class="n">require_value_prediction</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">require_weight_prediction</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">...</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">Then, we can customize the choice of weight and value functions using the following arguments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_dict</span> <span class="o">=</span> <span class="n">prep</span><span class="o">.</span><span class="n">obtain_whole_inputs</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span>
    <span class="n">q_function_method</span><span class="o">=</span><span class="s2">&quot;fqe&quot;</span><span class="p">,</span>   <span class="c1"># one of {&quot;fqe&quot;, &quot;dice&quot;, &quot;mql&quot;}, default=&quot;fqe&quot;</span>
    <span class="n">v_function_method</span><span class="o">=</span><span class="s2">&quot;fqe&quot;</span><span class="p">,</span>   <span class="c1"># one of {&quot;fqe&quot;, &quot;dice_q&quot;, &quot;dice_v&quot;, &quot;mql&quot;, &quot;mvl&quot;}, default=&quot;fqe&quot;</span>
    <span class="n">w_function_method</span><span class="o">=</span><span class="s2">&quot;dice&quot;</span><span class="p">,</span>  <span class="c1"># one of {&quot;dice&quot;, &quot;mwl&quot;}, default=&quot;dice&quot;</span>
    <span class="o">...</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">To further customize the models, please specify <code class="docutils literal notranslate"><span class="pre">model_args</span></code> when initializing <code class="xref py py-class docutils literal notranslate"><span class="pre">CreateOPEInput</span></code> as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">d3rlpy.models.encoders</span> <span class="kn">import</span> <span class="n">VectorEncoderFactory</span>
<span class="kn">from</span> <span class="nn">d3rlpy.models.q_functions</span> <span class="kn">import</span> <span class="n">MeanQFunctionFactory</span>

<span class="n">prep</span> <span class="o">=</span> <span class="n">CreateOPEInput</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
    <span class="n">model_args</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;fqe&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;encoder_factory&quot;</span><span class="p">:</span> <span class="n">VectorEncoderFactory</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span>
            <span class="s2">&quot;q_func_factory&quot;</span><span class="p">:</span> <span class="n">MeanQFunctionFactory</span><span class="p">(),</span>
            <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;state_action_dual&quot;</span> <span class="p">:</span> <span class="p">{</span>  <span class="c1"># &quot;dice&quot;</span>
            <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;dual_dice&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;state_action_value&quot;</span><span class="p">:</span> <span class="p">{</span>  <span class="c1"># &quot;mql&quot;</span>
            <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
            <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">where the keys of <code class="docutils literal notranslate"><span class="pre">model_args</span></code> are the following.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">key</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&quot;fqe&quot;</span><span class="p">,</span>                  <span class="c1"># fqe</span>
    <span class="s2">&quot;state_action_dual&quot;</span><span class="p">,</span>    <span class="c1"># dice_q</span>
    <span class="s2">&quot;state_action_value&quot;</span><span class="p">,</span>   <span class="c1"># mql</span>
    <span class="s2">&quot;state_action_weight&quot;</span><span class="p">,</span>  <span class="c1"># mwl</span>
    <span class="s2">&quot;state_dual&quot;</span><span class="p">,</span>           <span class="c1"># dice_v</span>
    <span class="s2">&quot;state_value&quot;</span><span class="p">,</span>          <span class="c1"># mvl</span>
    <span class="s2">&quot;state_weight&quot;</span><span class="p">,</span>         <span class="c1"># mwl</span>
    <span class="s2">&quot;hidden_dim&quot;</span><span class="p">,</span>           <span class="c1"># hidden dim of value/weight function, except FQE</span>
<span class="p">]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="_autosummary/scope_rl.ope.input.html"><span class="doc">API reference of CreateInputDict</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="scope_rl_api.html#scope-rl-api-ope-weight-and-value-learning"><span class="std std-ref">API reference of value/weight learning methods</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#tip-mariginal-iw"><span class="std std-ref">Logics behind value and weight learning methods (How to obtain state(-action) marginal importance weight?)</span></a></p></li>
</ul>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to collect input_dict in a non-episodic setting?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">When the goal is to evaluate the policy under a stationary distribution (<span class="math notranslate nohighlight">\(d^{\pi}(s)\)</span>) rather than in a episodic setting
(i.e., cartpole or taxi used in <span id="id1">[<a class="reference internal" href="references.html#id40" title="Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, 9659–9668. PMLR, 2020.">12</a>, <a class="reference internal" href="references.html#id42" title="Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: infinite-horizon off-policy estimation. Advances in Neural Information Processing Systems, 2018.">13</a>]</span>), we need to (re-)collect initial states from evaluation policies stationary distribution.</p>
<p class="sd-card-text">In this case, please turn the following options.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_dict</span> <span class="o">=</span> <span class="n">prep</span><span class="o">.</span><span class="n">obtain_whole_inputs</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span>
    <span class="n">resample_initial_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">use_stationary_distribution_on_policy_evaluation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># when env is provided</span>
    <span class="o">...</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details></div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="learning_implementation.html"><span class="doc">Supported Implementation (learning)</span></a> describes how to obtain <code class="xref py py-class docutils literal notranslate"><span class="pre">logged_dataset</span></code> using a behavior policy in detail.</p>
</div>
</section>
<section id="basic-off-policy-evaluation-ope">
<span id="implementation-basic-ope"></span><h2>Basic Off-Policy Evaluation (OPE)<a class="headerlink" href="#basic-off-policy-evaluation-ope" title="Permalink to this heading">#</a></h2>
<p>The goal of (basic) OPE is to evaluate the following expected trajectory-wise reward of a policy (referred to as policy value).</p>
<div class="math notranslate nohighlight">
\[J(\pi) := \mathbb{E}_{\tau} \left [ \sum_{t=0}^{T-1} \gamma^t r_{t} \mid \pi \right ],\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi\)</span> is the (evaluation) policy, <span class="math notranslate nohighlight">\(\tau\)</span> is the trajectory observed by the evaluation policy, and <span class="math notranslate nohighlight">\(r_t\)</span> is the immediate reward at each timestep.
(Please refer to the <a class="reference internal" href="ope_ops.html"><span class="doc">problem setup</span></a> for additional notations.)</p>
<p>Here, we describe the class for conducting OPE and the implemented OPE estimators for estimating the policy value.
We begin with the <code class="xref py py-class docutils literal notranslate"><span class="pre">OffPolicyEvaluation</span></code> class to streamline the OPE procedure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize the OPE class</span>
<span class="kn">from</span> <span class="nn">scope_rl.ope</span> <span class="kn">import</span> <span class="n">OffPolicyEvaluation</span> <span class="k">as</span> <span class="n">OPE</span>
<span class="n">ope</span> <span class="o">=</span> <span class="n">OPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">DM</span><span class="p">(),</span> <span class="n">TIS</span><span class="p">(),</span> <span class="n">PDIS</span><span class="p">(),</span> <span class="n">DR</span><span class="p">()],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Using the OPE class, we can obtain the OPE results of various estimators at once as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope_dict</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip" id="tip-ope">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to conduct OPE with multiple logged datasets?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Conducting OPE with multiple logged datasets requires no additional efforts.</p>
<p class="sd-card-text">First, the same command with the single logged dataset case also works with multiple logged datasets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">OPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>  <span class="c1"># MultipleLoggedDataset</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">DM</span><span class="p">(),</span> <span class="n">TIS</span><span class="p">(),</span> <span class="n">PDIS</span><span class="p">(),</span> <span class="n">DR</span><span class="p">()],</span>
<span class="p">)</span>
<span class="n">multiple_ope_dict</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>  <span class="c1"># MultipleInputDict</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">The returned value is dictionary containing the ope result.</p>
<p class="sd-card-text">In addition, we can specify which logged dataset and input_dict to use by setting <code class="docutils literal notranslate"><span class="pre">behavior_policy_name</span></code> and <code class="docutils literal notranslate"><span class="pre">dataset_id</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">multiple_ope_dict</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>
    <span class="n">behavior_policy_name</span><span class="o">=</span><span class="n">behavior_policy</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>  <span class="c1">#</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># specify which logged dataset and input_dict to use</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">The basic visualization function also work by specifying the dataset id.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span><span class="o">.</span><span class="n">visualize_off_policy_estimates</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>
    <span class="n">behavior_policy_name</span><span class="o">=</span><span class="n">behavior_policy</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1">#</span>
    <span class="o">...</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_policy_value_basic.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">policy value estimated with the specified dataset</p>
</div>
</div>
<p class="sd-card-text">Moreover, we provide additional visualization functions for the multiple logged dataset case.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span><span class="o">.</span><span class="n">visualize_policy_value_with_multiple_estimates</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>      <span class="c1"># MultipleInputDict</span>
    <span class="n">behavior_policy_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>                   <span class="c1"># compare estimators with multiple behavior policies</span>
    <span class="c1"># behavior_policy_name=behavior_policy.name  # compare estimators with a single behavior policy</span>
    <span class="n">plot_type</span><span class="o">=</span><span class="s2">&quot;ci&quot;</span><span class="p">,</span>  <span class="c1"># one of {&quot;ci&quot;, &quot;violin&quot;, &quot;scatter&quot;}, default=&quot;ci&quot;</span>
    <span class="o">...</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">When the <code class="docutils literal notranslate"><span class="pre">plot_type</span></code> is “ci”, the plot is somewhat similar to the basic visualization.
(The star indicates the ground-truth policy value and the confidence intervals are derived by multiple estimates across datasets.)</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_policy_value_basic_multiple.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">policy value estimated with the multiple datasets</p>
</div>
</div>
<p class="sd-card-text">When the <code class="docutils literal notranslate"><span class="pre">plot_type</span></code> is “violin”, the plot visualizes the distribution of multiple estimates.
This is particularly useful to see how the estimation result can vary depending on different datasets or random seeds.</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_policy_value_basic_multiple_violin.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">policy value estimated with the multiple datasets (violin)</p>
</div>
</div>
<p class="sd-card-text">Finally, when the <code class="docutils literal notranslate"><span class="pre">plot_type</span></code> is “scatter”, the plot visualizes each estimation with its color specifying the behavior policy.
This function is particularly useful to see how the choice of behavior policy (e.g., their stochasticity) affects the estimation result.</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_policy_value_basic_multiple_scatter.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">policy value estimated with the multiple datasets (scatter)</p>
</div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tips-synthetic-dataset"><span class="std std-ref">How to obtain MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tip-opl"><span class="std std-ref">How to handle OPL with MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#tip-create-input-dict"><span class="std std-ref">How to create input_dict for MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="examples/multiple.html"><span class="doc">Examples with MultipleLoggedDataset</span></a></p></li>
</ul>
</div>
</div>
</details></div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="quickstart.html"><span class="doc">Quickstart</span></a> and <a class="reference internal" href="examples/basic_ope.html"><span class="doc">related example codes</span></a></p></li>
</ul>
</div>
<p>The OPE class implements the following functions.</p>
<p>(OPE)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_policy_value</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_intervals</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">summarize_off_policy_estimates</span></code></p></li>
</ul>
<p>(Evaluation of OPE estimators)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">evaluate_performance_of_ope_estimators</span></code></p></li>
</ul>
<p>(Visualization)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_off_policy_estimates</span></code></p></li>
</ul>
<p>(Visualization with multiple estimates on multiple logged datasets)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_with_multiple_estimates</span></code></p></li>
</ul>
<p>Below, we describe the implemented OPE estimators.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Standard OPE estimators</p></th>
<th class="head"></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#implementation-dm"><span class="std std-ref">Direct Method (DM)</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#implementation-tis"><span class="std std-ref">Trajectory-wise Importance Sampling (TIS)</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#implementation-pdis"><span class="std std-ref">Per-Decision Importance Sampling (PDIS)</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#implementation-dr"><span class="std std-ref">Doubly Robust (DR)</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#implementation-sn"><span class="std std-ref">Self-Normalized estimators</span></a></p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Marginal OPE estimators</p></th>
<th class="head"></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#implementation-marginal-ope"><span class="std std-ref">State Marginal estimators</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#implementation-marginal-ope"><span class="std std-ref">State-Action Marginal estimators</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#implementation-drl"><span class="std std-ref">Double Reinforcement Learning</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#implementation-sope"><span class="std std-ref">Spectrum of Off-Policy Evaluation</span></a></p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Extensions</p></th>
<th class="head"></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#implementation-high-confidence-ope"><span class="std std-ref">High Confidence Off-Policy Evaluation</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#implementation-continuous-ope"><span class="std std-ref">Extension to the continuous action space</span></a></p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to define my own OPE estimator?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">To define your own OPE estimator, use <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseOffPolicyEstimator</span></code>.</p>
<p class="sd-card-text">Basically, the common inputs for each functions are the following keys from <code class="docutils literal notranslate"><span class="pre">logged_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">input_dict</span></code>.</p>
<p class="sd-card-text">(logged_dataset)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">key</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">size</span><span class="p">,</span>
    <span class="n">step_per_trajectory</span><span class="p">,</span>
    <span class="n">action</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text">(input_dict)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">key</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">evaluation_policy_action</span><span class="p">,</span>
    <span class="n">evaluation_policy_action_dist</span><span class="p">,</span>
    <span class="n">state_action_value_prediction</span><span class="p">,</span>
    <span class="n">initial_state_value_prediction</span><span class="p">,</span>
    <span class="n">state_action_marginal_importance_weight</span><span class="p">,</span>
    <span class="n">state_marginal_importance_weight</span><span class="p">,</span>
    <span class="n">on_policy_policy_value</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">n_step_pdis</span></code> is also applicable to marginal estimators and <code class="docutils literal notranslate"><span class="pre">action_scaler</span></code> and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> are added in the continuous-action case.</p>
<p class="sd-card-text">If you want to add other arguments, please add them in the initialization arguments for API consistency.</p>
<p class="sd-card-text">Finally, contribution to SCOPE-RL with a new OPE estimator is more than welcome! Please read <a class="reference external" href="https://github.com/hakuhodo-technologies/scope-rl/blob/main/CONTRIBUTING.md">the guidelines for contribution (CONTRIBUTING.md)</a>.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p class="sd-card-text"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.html"><span class="doc">API reference of BaseOffPolicyEstimator</span></a> and <a class="reference internal" href="examples/custom_estimators.html"><span class="doc">example codes for implementing custom OPE estimators</span></a> explain the abstract methods.</p>
</div>
</div>
</details></div>
<section id="direct-method-dm">
<span id="implementation-dm"></span><h3>Direct Method (DM)<a class="headerlink" href="#direct-method-dm" title="Permalink to this heading">#</a></h3>
<p>DM <span id="id2">[<a class="reference internal" href="references.html#id23" title="Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 129–138. 2009.">3</a>]</span> is a model-based approach which uses the initial state value (estimated by e.g., Fitted Q Evaluation (FQE) <span id="id3">[<a class="reference internal" href="references.html#id20" title="Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Proceedings of the 36th International Conference on Machine Learning, volume 97, 3703–3712. PMLR, 2019.">4</a>]</span>).
It first learns the Q-function and then leverages the learned Q-function as follows.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{DM}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_{0}^{(i)}) \hat{Q}(s_{0}^{(i)}, a) = \frac{1}{n} \sum_{i=1}^n \hat{V}(s_{0}^{(i)}),\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}=\{\{(s_t, a_t, r_t)\}_{t=0}^{T-1}\}_{i=1}^n\)</span> is the logged dataset with <span class="math notranslate nohighlight">\(n\)</span> trajectories.
<span class="math notranslate nohighlight">\(T\)</span> indicates step per episode. <span class="math notranslate nohighlight">\(\hat{Q}(s_t, a_t)\)</span> is the estimated state-action value and <span class="math notranslate nohighlight">\(\hat{V}(s_t)\)</span> is the estimated state value.</p>
<p>DM has low variance compared to other estimators, but can produce larger bias due to approximation errors.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DirectMethod</span></code></p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We use the implementation of FQE provided by <a class="reference external" href="https://github.com/takuseno/d3rlpy">d3rlpy</a>.</p>
</div>
</section>
<section id="trajectory-wise-importance-sampling-tis">
<span id="implementation-tis"></span><h3>Trajectory-wise Importance Sampling (TIS)<a class="headerlink" href="#trajectory-wise-importance-sampling-tis" title="Permalink to this heading">#</a></h3>
<p>TIS <span id="id4">[<a class="reference internal" href="references.html#id26" title="Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of the 17th International Conference on Machine Learning, 759–-766. 2000.">5</a>]</span> uses importance sampling technique to correct the distribution shift between <span class="math notranslate nohighlight">\(\pi\)</span> and <span class="math notranslate nohighlight">\(\pi_0\)</span> as follows.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t w_{1:T-1}^{(i)} r_t^{(i)},\]</div>
<p>where <span class="math notranslate nohighlight">\(w_{0:T-1} := \prod_{t=0}^{T-1} (\pi(a_t | s_t) / \pi_0(a_t | s_t))\)</span> is the trajectory-wise importance weight.</p>
<p>TIS enables an unbiased estimation of the policy value. However, when the trajectory length <span class="math notranslate nohighlight">\(T\)</span> is large, TIS suffers from high variance
due to the product of importance weights over the entire horizon.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TrajectoryWiseImportanceSampling</span></code></p></li>
</ul>
</div></blockquote>
</section>
<section id="per-decision-importance-sampling-pdis">
<span id="implementation-pdis"></span><h3>Per-Decision Importance Sampling (PDIS)<a class="headerlink" href="#per-decision-importance-sampling-pdis" title="Permalink to this heading">#</a></h3>
<p>PDIS <span id="id5">[<a class="reference internal" href="references.html#id26" title="Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of the 17th International Conference on Machine Learning, 759–-766. 2000.">5</a>]</span> leverages the sequential nature of the MDP to reduce the variance of TIS.
Specifically, since <span class="math notranslate nohighlight">\(s_t\)</span> only depends on <span class="math notranslate nohighlight">\(s_0, \ldots, s_{t-1}\)</span> and <span class="math notranslate nohighlight">\(a_0, \ldots, a_{t-1}\)</span> and is independent of <span class="math notranslate nohighlight">\(s_{t+1}, \ldots, s_{T}\)</span> and <span class="math notranslate nohighlight">\(a_{t+1}, \ldots, a_{T}\)</span>,
PDIS only considers the importance weight of the past interactions when estimating <span class="math notranslate nohighlight">\(r_t\)</span> as follows.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t w_{0:t}^{(i)} r_t^{(i)},\]</div>
<p>where <span class="math notranslate nohighlight">\(w_{0:t} := \prod_{t'=0}^t (\pi(a_{t'} | s_{t'}) / \pi_b(a_{t'} | s_{t'}))\)</span> is the importance weight for each time step wrt the previous actions.</p>
<p>PDIS remains unbiased while reducing the variance of TIS. However, when <span class="math notranslate nohighlight">\(t\)</span> is large, PDIS still suffers from high variance.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">PerDecisionImportanceSampling</span></code></p></li>
</ul>
</div></blockquote>
</section>
<section id="doubly-robust-dr">
<span id="implementation-dr"></span><h3>Doubly Robust (DR)<a class="headerlink" href="#doubly-robust-dr" title="Permalink to this heading">#</a></h3>
<p>DR <span id="id6">[<a class="reference internal" href="references.html#id28" title="Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, volume 48, 652–661. PMLR, 2016.">6</a>, <a class="reference internal" href="references.html#id32" title="Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, volume 48, 2139–2148. PMLR, 2016.">7</a>]</span> is a hybrid of model-based estimation and importance sampling.
It introduces <span class="math notranslate nohighlight">\(\hat{Q}\)</span> as a baseline estimation in the recursive form of PDIS and applies importance weighting only on its residual.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
:= \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \left(w_{0:t}^{(i)} (r_t^{(i)} - \hat{Q}(s_t^{(i)}, a_t^{(i)})) + w_{0:t-1}^{(i)} \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_t^{(i)}, a) \right),\]</div>
<p>DR is unbiased and has lower variance than PDIS when <span class="math notranslate nohighlight">\(\hat{Q}(\cdot)\)</span> is reasonably accurate to satisfy <span class="math notranslate nohighlight">\(0 &lt; \hat{Q}(\cdot) &lt; 2 Q(\cdot)\)</span>.
However, when the importance weight is quite large, it may still suffer from a high variance.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DoublyRobust</span></code></p></li>
</ul>
</div></blockquote>
</section>
<section id="self-normalized-estimators">
<span id="implementation-sn"></span><h3>Self-Normalized estimators<a class="headerlink" href="#self-normalized-estimators" title="Permalink to this heading">#</a></h3>
<p>Self-normalized estimators <span id="id7">[<a class="reference internal" href="references.html#id47" title="Nathan Kallus and Masatoshi Uehara. Intrinsically efficient, stable, and bounded off-policy evaluation for reinforcement learning. In Advances in Neural Information Processing Systems, 3325–3334. 2019.">11</a>]</span> aim to reduce the scale of importance weight for the variance reduction purpose.
Specifically, self-normalized versions of PDIS and DR is defined as follows.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{SNPDIS}} (\pi; \mathcal{D}) := \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \frac{w_{0:t}^{(i)}}{\sum_{i'=1}^n w_{0:t}^{(i')}} r_t^{(i)},\]</div>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{SNDR}} (\pi; \mathcal{D})
:= \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \left(\frac{w_{0:t}^{(i)}}{\sum_{i'=1}^n w_{0:t}^{(i')}} (r_t^{(i)} - \hat{Q}(s_t^{(i)}, a_t^{(i)})) + \frac{w_{0:t-1}^{(i)}}{\sum_{i'=1}^n w_{0:t-1}^{(i')}} \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_t^{(i)}, a) \right),\]</div>
<p>In more general, self-normalized estimators substitute importance weight <span class="math notranslate nohighlight">\(w_{\ast}\)</span> as follows.</p>
<div class="math notranslate nohighlight">
\[\tilde{w}_{\ast} := \frac{w_{\ast}}{\sum_{i=1}^n w_{\ast}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{w}_{\ast}\)</span> is the self-normalized importance weight.</p>
<p>Self-normalized estimators are no longer unbiased, but has variance bounded by <span class="math notranslate nohighlight">\(r_{max}^2\)</span> while also remaining consistent.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">SelfNormalizedTIS</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">SelfNormalizedPDIS</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">SelfNormalizedDR</span></code></p></li>
</ul>
</div></blockquote>
</section>
<section id="marginalized-importance-sampling-estimators">
<span id="implementation-marginal-ope"></span><h3>Marginalized Importance Sampling Estimators<a class="headerlink" href="#marginalized-importance-sampling-estimators" title="Permalink to this heading">#</a></h3>
<p>When the length of trajectory (<span class="math notranslate nohighlight">\(T\)</span>) is large, even per-decision importance weights can exponentially large in the latter part of the trajectory.
To alleviate this, state marginal or state-action marginal importance weights can be used instead of the per-decision importance weight as follows <span id="id8">[<a class="reference internal" href="references.html#id40" title="Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, 9659–9668. PMLR, 2020.">12</a>, <a class="reference internal" href="references.html#id42" title="Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: infinite-horizon off-policy estimation. Advances in Neural Information Processing Systems, 2018.">13</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\rho(s, a) &amp;:= d^{\pi}(s, a) / d^{\pi_0}(s, a) \\
\rho(s) &amp;:= d^{\pi}(s) / d^{\pi_0}(s)\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(d^{\pi}(s, a)\)</span> and <span class="math notranslate nohighlight">\(d^{\pi}(s)\)</span> is the marginal visitation probability of the policy <span class="math notranslate nohighlight">\(\pi\)</span> on <span class="math notranslate nohighlight">\((s, a)\)</span> or <span class="math notranslate nohighlight">\(s\)</span>, respectively.
The use of marginal importance weights is particularly beneficial when policy visits the same or similar states among different trajectories or different timestep.
(e.g., when the state transition is something like <span class="math notranslate nohighlight">\(\cdots \rightarrow s_1 \rightarrow s_2 \rightarrow s_1 \rightarrow s_2 \rightarrow \cdots\)</span> or when the trajectories always visits some particular state as <span class="math notranslate nohighlight">\(\cdots \rightarrow s_{*} \rightarrow s_{1} \rightarrow s_{*} \rightarrow \cdots\)</span>).
Then, State-Action Mariginal Importance Sampling (SMIS) and State Marginal Doubly Robust (SMDR) are defined as follows.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{SAM-IS}} (\pi; \mathcal{D})
:= \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}, a_t^{(i)}) r_t^{(i)},\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{J}_{\mathrm{SAM-DR}} (\pi; \mathcal{D})
&amp;:= \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_0^{(i)}) \hat{Q}(s_0^{(i)}, a) \\
&amp; \quad \quad + \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}, a_t^{(i)}) \left(r_t^{(i)} + \gamma \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_{t+1}^{(i)}, a) - \hat{Q}(s_t^{(i)}, a_t^{(i)}) \right),\end{split}\]</div>
<p>Similarly, State-Marginal Importance Sampling (SAMIS) and State Action-Marginal Doubly Robust (SAMDR) are defined as follows.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{SM-IS}} (\pi; \mathcal{D})
:= \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}) w_t(s_t^{(i)}, a_t^{(i)}) r_t^{(i)},\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{J}_{\mathrm{SM-DR}} (\pi; \mathcal{D})
&amp;:= \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_0^{(i)}) \hat{Q}(s_0^{(i)}, a) \\
&amp; \quad \quad + \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}) w_t(s_t^{(i)}, a_t^{(i)}) \left(r_t^{(i)} + \gamma \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_{t+1}^{(i)}, a) - \hat{Q}(s_t^{(i)}, a_t^{(i)}) \right),\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_t(s_t, a_t) := \pi(a_t | s_t) / \pi_0(a_t | s_t)\)</span> is the immediate importance weight at timestep <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="admonition tip" id="tip-mariginal-iw">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to obtain state(-action) marginal importance weight?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">To use marginalized importance sampling estimators, we need to first estimate the state marginal or state-action marginal importance weight.
A dominant way to do this is to leverage the following relationship between the importance weights and the state-action value function under the assumption that the state visitation probability is consistent across various timesteps <span id="id9">[<a class="reference internal" href="references.html#id40" title="Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, 9659–9668. PMLR, 2020.">12</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;\mathbb{E}_{(s, a, r, s') \sim \mathcal{D_{\pi_0}}}[w(s, a) r] \\
&amp;= \mathbb{E}_{(s, a, r, s') \sim \mathcal{D_{\pi_0}}}[w(s, a)(Q_{\pi}(s, a) - \gamma \mathbb{E}_{a' \sim \pi(a' | s')}[Q(s', a')])] \\
&amp;= (1 - \gamma) \mathbb{E}_{s_0 \sim d^{\pi}(s_0), a_0 \sim \pi(a_0 | s_0)}[Q_{\pi}(s_0, a_0)]\end{split}\]</div>
<p class="sd-card-text">The objective of weight learning is to minimize the difference between the middle term and the last term of the above equation when Q-function adversarially maximizes the difference.
In particular, we provide the following algorithms to estimate state marginal and state-action marginal importance weights (and corresponding state-action value function) via minimax learning.</p>
<ul class="simple">
<li><dl class="simple">
<dt>Augmented Lagrangian Method (ALM/DICE) <span id="id10">[<a class="reference internal" href="references.html#id41" title="Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via the regularized lagrangian. Advances in Neural Information Processing Systems, 33:6551–6561, 2020.">16</a>]</span>:</dt><dd><p class="sd-card-text">This method simultaneously optimize both <span class="math notranslate nohighlight">\(w(s, a)\)</span> and <span class="math notranslate nohighlight">\(Q(s, a)\)</span>. By setting different hyperparameters,
ALM can be identical to BestDICE <span id="id11">[<a class="reference internal" href="references.html#id41" title="Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via the regularized lagrangian. Advances in Neural Information Processing Systems, 33:6551–6561, 2020.">16</a>]</span>, DualDICE <span id="id12">[<a class="reference internal" href="references.html#id46" title="Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: behavior-agnostic estimation of discounted stationary distribution corrections. Advances in Neural Information Processing Systems, 2019.">20</a>]</span>, GenDICE <span id="id13">[<a class="reference internal" href="references.html#id44" title="Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: generalized offline estimation of stationary values. Proceedings of the 8th International Conference on Learning Representations, 2020.">18</a>]</span>,
AlgaeDICE <span id="id14">[<a class="reference internal" href="references.html#id45" title="Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.">19</a>]</span>, and MQL/MWL <span id="id15">[<a class="reference internal" href="references.html#id40" title="Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, 9659–9668. PMLR, 2020.">12</a>]</span>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Minimax Q-Learning and Weight Learning (MQL/MWL) <span id="id16">[<a class="reference internal" href="references.html#id40" title="Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, 9659–9668. PMLR, 2020.">12</a>]</span>:</dt><dd><p class="sd-card-text">This method assumes that one of the value function or weight function is expressed by a function class in a reproducing kernel Hilbert space (RKHS)
and optimizes only either value function or weight function.</p>
</dd>
</dl>
</li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="#tip-create-input-dict"><span class="std std-ref">How to select models for value/weight learning methods?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="scope_rl_api.html#scope-rl-api-ope-weight-and-value-learning"><span class="std std-ref">API reference of value/weight learning methods</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="_autosummary/scope_rl.ope.input.html"><span class="doc">API reference of CreateInputDict</span></a></p></li>
</ul>
</div>
</div>
</details></div>
<p>We implement state marginal and state-action marginal OPE estimators in the following classes (both for <code class="xref py py-class docutils literal notranslate"><span class="pre">Discrete-</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">Continuous-</span></code> action spaces).</p>
<p>(State Marginal Estimators)</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateMarginalDM</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateMarginalIS</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateMarginalDR</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateMarginalSNIS</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateMarginalSNDR</span></code></p></li>
</ul>
</div></blockquote>
<p>(State-Action Marginal Estimators)</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateActionMarginalIS</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateActionMarginalDR</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateActionMarginalSNIS</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">StateActionMarginalSNDR</span></code></p></li>
</ul>
</div></blockquote>
</section>
<section id="double-reinforcement-learning-drl">
<span id="implementation-drl"></span><h3>Double Reinforcement Learning (DRL)<a class="headerlink" href="#double-reinforcement-learning-drl" title="Permalink to this heading">#</a></h3>
<p>Comparing DR in the standard and marginal OPE, we notice that their formulation is slightly different as follows.</p>
<p>(DR in standard OPE)</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
:= \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \left( w_{0:t}^{(i)} (r_t^{(i)} - \hat{Q}(s_t^{(i)}, a_t^{(i)})) + w_{0:t-1}^{(i)} \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_t^{(i)}, a) \right),\]</div>
<p>(DR in marginal OPE)</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{J}_{\mathrm{SAM-DR}} (\pi; \mathcal{D})
&amp;:= \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_0^{(i)}) \hat{Q}(s_0^{(i)}, a) \\
&amp; \quad \quad + \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{T-1} \gamma^t \rho(s_t^{(i)}, a_t^{(i)}) \left(r_t^{(i)} + \gamma \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_{t+1}^{(i)}, a) - \hat{Q}(s_t^{(i)}, a_t^{(i)}) \right),\end{split}\]</div>
<p>Then, a natural question arises, would it be possible to use marginal importance weight in DR in the standard formulation?</p>
<p>DRL <span id="id17">[<a class="reference internal" href="references.html#id39" title="Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evaluation in markov decision processes. Journal of Machine Learning Research, 2020.">15</a>]</span> leverages the marginal importance sampling in the standard OPE formulation as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{J}_{\mathrm{DRL}} (\pi; \mathcal{D})
&amp; := \frac{1}{n} \sum_{k=1}^K \sum_{i=1}^{n_k} \sum_{t=0}^{T-1} (\rho^j(s_{t}^{(i)}, a_{t}^{(i)}) (r_{t}^{(i)} - Q^j(s_{t}^{(i)}, a_{t}^{(i)})) \\
&amp; \quad \quad + \rho^j(s_{t-1}^{(i)}, a_{t-1}^{(i)}) \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) Q^j(s_{t}^{(i)}, a))\end{split}\]</div>
<p>DRL achieves the semiparametric efficiency bound with a consistent value predictor <span class="math notranslate nohighlight">\(Q\)</span>.
Therefore, to alleviate the potential bias introduced in <span class="math notranslate nohighlight">\(Q\)</span>, DRL uses the “cross-fitting” technique to estimate the value function.
Specifically, let <span class="math notranslate nohighlight">\(K\)</span> is the number of folds and <span class="math notranslate nohighlight">\(\mathcal{D}_j\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th split of logged data consisting of <span class="math notranslate nohighlight">\(n_k\)</span> samples.
Cross-fitting trains <span class="math notranslate nohighlight">\(\rho^j\)</span> and <span class="math notranslate nohighlight">\(Q^j\)</span> on the subset of data used for OPE, i.e., <span class="math notranslate nohighlight">\(\mathcal{D} \setminus \mathcal{D}_j\)</span>.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DoubleReinforcementLearning</span></code></p></li>
</ul>
</div></blockquote>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to obtain Q-hat via cross-fitting?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">To obtain <span class="math notranslate nohighlight">\(\hat{Q}\)</span> via cross-fitting, please specify <code class="docutils literal notranslate"><span class="pre">k_fold</span></code> of <code class="xref py py-class docutils literal notranslate"><span class="pre">obtain_whole_inputs</span></code> of <code class="xref py py-class docutils literal notranslate"><span class="pre">CreateOPEInput</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prep</span> <span class="o">=</span> <span class="n">CreateOPEInput</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">input_dict</span> <span class="o">=</span> <span class="n">prep</span><span class="o">.</span><span class="n">obtain_whole_inputs</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">evaluation_policies</span><span class="o">=</span><span class="n">evaluation_policies</span><span class="p">,</span>
    <span class="n">require_value_prediction</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># use model-based prediction</span>
    <span class="n">k_fold</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>                       <span class="c1"># use 3-fold cross-fitting</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">The default <code class="xref py py-class docutils literal notranslate"><span class="pre">k_fold=1</span></code> trains <span class="math notranslate nohighlight">\(\hat{Q}\)</span> and <span class="math notranslate nohighlight">\(\hat{w}\)</span> without cross-fitting.</p>
</div>
</details></div>
</section>
<section id="spectrum-of-off-policy-estimators-sope">
<span id="implementation-sope"></span><h3>Spectrum of Off-Policy Estimators (SOPE)<a class="headerlink" href="#spectrum-of-off-policy-estimators-sope" title="Permalink to this heading">#</a></h3>
<p>While state marginal or state-action marginal importance weight effectively alleviates the variance of per-decision importance weight, the estimation error of marginal importance weights
may introduce some bias in estimation. To alleviate this and control the bias-variance tradeoff more flexibly, SOPE uses the following interpolated importance weights <span id="id18">[<a class="reference internal" href="references.html#id38" title="Christina Yuan, Yash Chandak, Stephen Giguere, Philip S Thomas, and Scott Niekum. Sope: spectrum of off-policy estimators. Advances in Neural Information Processing Systems, 34:18958–18969, 2021.">14</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}w_{\mathrm{SOPE}}(s_t, a_t) &amp;=
\begin{cases}
    \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) &amp; \mathrm{if} \, t &lt; k \\
    \rho(s_{t-k}, a_{t-k}) \prod_{t'=t-k+1}^{t} w_t(s_{t'}, a_{t'}) &amp; \mathrm{otherwise}
\end{cases} \\
w_{\mathrm{SOPE}}(s_t, a_t) &amp;=
\begin{cases}
    \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) &amp; \mathrm{if} \, t &lt; k \\
    \rho(s_{t-k}) \prod_{t'=t-k}^{t} w_t(s_{t'}, a_{t'}) &amp; \mathrm{otherwise}
\end{cases}\end{split}\]</div>
<p>where SOPE uses per-decision importance weight <span class="math notranslate nohighlight">\(w_t(s_t, a_t) := \pi(a_t | s_t) / \pi_0(a_t | s_t)\)</span> for the <span class="math notranslate nohighlight">\(k\)</span> most recent timesteps.</p>
<p>For instance, State Action-Marginal Importance Sampling (SAMIS) and State Action-Marginal Doubly Robust (SAM-DR) are defined as follows.</p>
<div class="math notranslate nohighlight">
\[\hat{J}_{\mathrm{SOPE-SAM-IS}} (\pi; \mathcal{D})
:= \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{k-1} \gamma^t w_{0:t}^{(i)} r_t^{(i)}
+ \frac{1}{n} \sum_{i=1}^n \sum_{t=k}^{T-1} \gamma^t \rho(s_{t-k}^{(i)}, a_{t-k}^{(i)}) w_{t-k+1:t}^{(i)} r_t^{(i)},\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{J}_{\mathrm{SOPE-SAM-DR}} (\pi; \mathcal{D})
&amp;:= \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_0^{(i)}) \hat{Q}(s_0^{(i)}, a) \\
&amp; \quad \quad + \frac{1}{n} \sum_{i=1}^n \sum_{t=0}^{k-1} \gamma^t w_{0:t}^{(i)} \left(r_t^{(i)} + \gamma \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_{t+1}^{(i)}, a) - \hat{Q}(s_t^{(i)}, a_t^{(i)}) \right) \\
&amp; \quad \quad + \frac{1}{n} \sum_{i=1}^n \sum_{t=k}^{T-1} \gamma^t \rho(s_{t-k}^{(i)}, a_{t-k}^{(i)}) w_{t-k+1:t}^{(i)} \left(r_t^{(i)} + \gamma \sum_{a \in \mathcal{A}} \pi(a | s_t^{(i)}) \hat{Q}(s_{t+1}^{(i)}, a) - \hat{Q}(s_t^{(i)}, a_t^{(i)}) \right),\end{split}\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to change the spectrum of (marginal) OPE?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">SOPE is available by specifying <code class="xref py py-class docutils literal notranslate"><span class="pre">n_step_pdis</span></code> in the state marginal and state-action marginal estimators.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">OPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">SMIS</span><span class="p">(),</span> <span class="n">SMDR</span><span class="p">(),</span> <span class="n">SAMIS</span><span class="p">(),</span> <span class="n">SAMDR</span><span class="p">()],</span>  <span class="c1"># any marginal estimators</span>
    <span class="n">n_step_pdis</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># number of recent timesteps using per-decision importance sampling</span>
<span class="p">)</span>
<span class="n">estimation_dict</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_policy_value</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><code class="xref py py-class docutils literal notranslate"><span class="pre">n_step_pdis=0</span></code> is equivalent to the original marginal OPE estimators.</p>
</div>
</details></div>
</section>
<section id="high-confidence-off-policy-evaluation-hcope">
<span id="implementation-high-confidence-ope"></span><h3>High Confidence Off-Policy Evaluation (HCOPE)<a class="headerlink" href="#high-confidence-off-policy-evaluation-hcope" title="Permalink to this heading">#</a></h3>
<p>To alleviate the risk of optimistically overestimating the policy value, we are sometimes interested in the confidence intervals and the lower bound of the estimated policy value.
We implement four methods to estimate the confidence intervals <span id="id19">[<a class="reference internal" href="references.html#id31" title="Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learning, 2380–2388. PMLR, 2015.">21</a>, <a class="reference internal" href="references.html#id24" title="Josiah P Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: confidence intervals for off-policy evaluation. In Proceedings of the 31st AAAI Conference on Artificial Intelligence. 2017.">22</a>, <a class="reference internal" href="references.html#id30" title="Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29. 2015.">23</a>]</span>.</p>
<ul class="simple">
<li><p>Hoeffding <span id="id20">[<a class="reference internal" href="references.html#id30" title="Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29. 2015.">23</a>]</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \hat{J}_{\max} \displaystyle \sqrt{\frac{\log(1 / \alpha)}{2 n}}.\]</div>
<ul class="simple">
<li><p>Empirical Bernstein <span id="id21">[<a class="reference internal" href="references.html#id31" title="Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learning, 2380–2388. PMLR, 2015.">21</a>, <a class="reference internal" href="references.html#id30" title="Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29. 2015.">23</a>]</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \displaystyle \frac{7 \hat{J}_{\max} \log(2 / \alpha)}{3 (n - 1)} + \displaystyle \sqrt{\frac{2 \hat{\mathbb{V}}_{\mathcal{D}}(\hat{J}) \log(2 / \alpha)}{(n - 1)}}.\]</div>
<ul class="simple">
<li><p>Student T-test <span id="id22">[<a class="reference internal" href="references.html#id31" title="Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learning, 2380–2388. PMLR, 2015.">21</a>]</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \displaystyle \frac{T_{\mathrm{test}}(1 - \alpha, n-1)}{\sqrt{n} / \hat{\sigma}}.\]</div>
<ul class="simple">
<li><p>Bootstrapping <span id="id23">[<a class="reference internal" href="references.html#id31" title="Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learning, 2380–2388. PMLR, 2015.">21</a>, <a class="reference internal" href="references.html#id24" title="Josiah P Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: confidence intervals for off-policy evaluation. In Proceedings of the 31st AAAI Conference on Artificial Intelligence. 2017.">22</a>]</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \mathrm{Bootstrap}(1 - \alpha).\]</div>
<p>Note that all the above bound holds with probability <span class="math notranslate nohighlight">\(1 - \alpha\)</span>.
For notations, we denote <span class="math notranslate nohighlight">\(\hat{\mathbb{V}}_{\mathcal{D}}(\cdot)\)</span> to be the sample variance,
<span class="math notranslate nohighlight">\(T_{\mathrm{test}}(\cdot,\cdot)\)</span> to be T value,
and <span class="math notranslate nohighlight">\(\sigma\)</span> to be the standard deviation.</p>
<p>Among the above high confidence interval estimation, hoeffding and empirical bernstein derives lower bound without any distribution assumption of <span class="math notranslate nohighlight">\(p(\hat{J})\)</span>, which sometimes leads to quite conservative estimation.
On the other hand, T-test is based on the assumption that each sample of <span class="math notranslate nohighlight">\(p(\hat{J})\)</span> follows the normal distribution.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to use High-confidence OPE?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">The implementation is available by calling <code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_intervals</span></code> of each OPE estimator as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">OPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">DM</span><span class="p">(),</span> <span class="n">TIS</span><span class="p">(),</span> <span class="n">PDIS</span><span class="p">(),</span> <span class="n">DR</span><span class="p">()],</span>  <span class="c1"># any standard or marginal estimators</span>
<span class="p">)</span>
<span class="n">estimation_dict</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_intervals</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>
    <span class="n">ci</span><span class="o">=</span><span class="s2">&quot;hoeffding&quot;</span><span class="p">,</span>  <span class="c1"># one of {&quot;hoeffding&quot;, &quot;bernstein&quot;, &quot;ttest&quot;, &quot;bootstrap&quot;}</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>      <span class="c1"># confidence level</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details></div>
</section>
<section id="extension-to-the-continuous-action-space">
<span id="implementation-continuous-ope"></span><h3>Extension to the Continuous Action Space<a class="headerlink" href="#extension-to-the-continuous-action-space" title="Permalink to this heading">#</a></h3>
<p>When the action space is continuous, the naive importance weight <span class="math notranslate nohighlight">\(w_t = \pi(a_t|s_t) / \pi_0(a_t|s_t) = (\pi(a |s_t) / \pi_0(a_t|s_t)) \cdot \mathbb{I} \{a = a_t \}\)</span> rejects almost every actions,
as the indicator function <span class="math notranslate nohighlight">\(\mathbb{I}\{a = a_t\}\)</span> filters only the action observed in the logged data.</p>
<p>To address this issue, continuous-action OPE estimators apply kernel density estimation technique to smooth the importance weight <span id="id24">[<a class="reference internal" href="references.html#id35" title="Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, volume 84, 1243–1251. PMLR, 2018.">57</a>, <a class="reference internal" href="references.html#id36" title="Haanvid Lee, Jongmin Lee, Yunseon Choi, Wonseok Jeon, Byung-Jun Lee, Yung-Kyun Noh, and Kee-Eung Kim. Local metric learning for off-policy evaluation in contextual bandits with continuous actions. In Advances in Neural Information Processing Systems, xxxx–xxxx. 2022.">58</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[\overline{w}_t = \int_{a \in \mathcal{A}} \frac{\pi(a | s_t)}{\pi_0(a_t | s_t)} \cdot \frac{1}{h} K \left( \frac{a - a_t}{h} \right) da,\]</div>
<p>where <span class="math notranslate nohighlight">\(K(\cdot)\)</span> denotes a kernel function and <span class="math notranslate nohighlight">\(h\)</span> is the bandwidth hyperparameter.
We can use any function as <span class="math notranslate nohighlight">\(K(\cdot)\)</span> that meets the following qualities:</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\int xK(x) dx = 0\)</span>,</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p><span class="math notranslate nohighlight">\(\int K(x) dx = 1\)</span>,</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p><span class="math notranslate nohighlight">\(\lim _{x \rightarrow-\infty} K(x)=\lim _{x \rightarrow+\infty} K(x)=0\)</span>,</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="4">
<li><p><span class="math notranslate nohighlight">\(K(x) \geq 0, \forall x\)</span>.</p></li>
</ol>
</li>
</ul>
<p>We provide the following kernel functions in SCOPE-RL.</p>
<ul class="simple">
<li><p>Gaussian kernel: <span class="math notranslate nohighlight">\(K(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^{2}}{2}}\)</span></p></li>
<li><p>Epanechnikov kernel: <span class="math notranslate nohighlight">\(K(x) = \frac{3}{4} (1 - x^2) \, (|x| \leq 1)\)</span></p></li>
<li><p>Triangular kernel: <span class="math notranslate nohighlight">\(K(x) = 1 - |x| \, (|x| \leq 1)\)</span></p></li>
<li><p>Cosine kernel: <span class="math notranslate nohighlight">\(K(x) = \frac{\pi}{4} \mathrm{cos} \left( \frac{\pi}{2} x \right) \, (|x| \leq 1)\)</span></p></li>
<li><p>Uniform kernel: <span class="math notranslate nohighlight">\(K(x) = \frac{1}{2} \, (|x| \leq 1)\)</span></p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to control the bias-variance tradeoff with a kernel?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">The bandwidth parameter <span class="math notranslate nohighlight">\(h\)</span> controls the bias-variance tradeoff.
Specifically, a large value of <span class="math notranslate nohighlight">\(h\)</span> leads to a low-variance but high-bias estimation,
while a small value of <span class="math notranslate nohighlight">\(h\)</span> leads to a high-variance but low-bias estimation.</p>
<p class="sd-card-text">The bandwidth parameter corresponds to <code class="docutils literal notranslate"><span class="pre">bandwidth</span></code> in the <code class="xref py py-class docutils literal notranslate"><span class="pre">OffPolicyEvaluation</span></code> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">OPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">DM</span><span class="p">(),</span> <span class="n">TIS</span><span class="p">(),</span> <span class="n">PDIS</span><span class="p">(),</span> <span class="n">DR</span><span class="p">()],</span>
    <span class="n">bandwidth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># bandwidth hyperparameter</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">For multi-dimension actions, we define the kernel with dot product among actions as <span class="math notranslate nohighlight">\(K(a, a') := K(a^T a')\)</span>.
To control the scale of each dimension, <code class="docutils literal notranslate"><span class="pre">action_scaler</span></code>, which is speficied in <code class="xref py py-class docutils literal notranslate"><span class="pre">OffPolicyEvaluation</span></code>, is also useful.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">d3rlpy.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxActionScaler</span>
<span class="n">ope</span> <span class="o">=</span> <span class="n">OPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">DM</span><span class="p">(),</span> <span class="n">TIS</span><span class="p">(),</span> <span class="n">PDIS</span><span class="p">(),</span> <span class="n">DR</span><span class="p">()],</span>
    <span class="n">bandwidth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># bandwidth hyperparameter</span>
    <span class="n">action_scaler</span><span class="o">=</span><span class="n">MinMaxActionScaler</span><span class="p">(</span>
        <span class="n">minimum</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">low</span><span class="p">,</span>
        <span class="n">maximum</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">high</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p class="sd-card-text"><a class="reference external" href="https://d3rlpy.readthedocs.io/en/latest/references/generated/d3rlpy.preprocessing.MinMaxActionScaler.html#d3rlpy.preprocessing.MinMaxActionScaler">(external) d3rlpy’s documentation about action_scaler</a></p>
</div>
</div>
</details></div>
</section>
</section>
<section id="cumulative-distribution-off-policy-evaluation-cd-ope">
<span id="implementation-cumulative-distribution-ope"></span><h2>Cumulative Distribution Off-Policy Evaluation (CD-OPE)<a class="headerlink" href="#cumulative-distribution-off-policy-evaluation-cd-ope" title="Permalink to this heading">#</a></h2>
<p>While the basic OPE aims to estimate the average policy performance, we are often also interested in the performance distribution of the evaluation policy.
Cumulative distribution OPE enables flexible estimation of various risk functions such as variance and conditional value at risk (CVaR) using the cumulative distribution function (CDF) <span id="id25">[<a class="reference internal" href="references.html#id9" title="Audrey Huang, Liu Leqi, Zachary Lipton, and Kamyar Azizzadenesheli. Off-policy risk assessment in contextual bandits. In Advances in Neural Information Processing Systems, volume 34, 23714–23726. 2021.">8</a>, <a class="reference internal" href="references.html#id10" title="Audrey Huang, Liu Leqi, Zachary Lipton, and Kamyar Azizzadenesheli. Off-policy risk assessment for markov decision processes. In roceedings of the 25th International Conference on Artificial Intelligence and Statistics, 5022–5050. 2022.">9</a>, <a class="reference internal" href="references.html#id8" title="Yash Chandak, Scott Niekum, Bruno da Silva, Erik Learned-Miller, Emma Brunskill, and Philip S Thomas. Universal off-policy evaluation. In Advances in Neural Information Processing Systems, volume 34, 27475–27490. 2021.">10</a>]</span>.</p>
<p>(Cumulative Distribution Function)</p>
<div class="math notranslate nohighlight">
\[F(m, \pi) := \mathbb{E} \left[ \mathbb{I} \left \{ \sum_{t=0}^{T-1} \gamma^t r_t \leq m \right \} \mid \pi \right]\]</div>
<p>(Risk Functions derived by CDF)</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mu(F) := \int_{G} G \, \mathrm{d}F(G)\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\sigma^2(F) := \int_{G} (G - \mu(F))^2 \, \mathrm{d}F(G)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>-quartile: <span class="math notranslate nohighlight">\(Q^{\alpha}(F) := \min \{ G \mid F(G) \leq \alpha \}\)</span></p></li>
<li><p>Conditional Value at Risk (CVaR): <span class="math notranslate nohighlight">\(\int_{G} G \, \mathbb{I}\{ G \leq Q^{\alpha}(F) \} \, \mathrm{d}F(G)\)</span></p></li>
</ul>
<p>where we let <span class="math notranslate nohighlight">\(G := \sum_{t=0}^{T-1} \gamma^t r_t\)</span> to represent the random variable of trajectory-wise reward
and <span class="math notranslate nohighlight">\(dF(G) := \mathrm{lim}_{\Delta \rightarrow 0} F(G) - F(G- \Delta)\)</span>.</p>
<p>To estimate both CDF and various risk functions, we provide the following <code class="xref py py-class docutils literal notranslate"><span class="pre">CumulativeDistributionOffPolicyEvaluation</span></code> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize the OPE class</span>
<span class="kn">from</span> <span class="nn">scope_rl.ope</span> <span class="kn">import</span> <span class="n">CumulativeDistributionOPE</span>
<span class="n">cd_ope</span> <span class="o">=</span> <span class="n">CumulativeDistributionOPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">CD_DM</span><span class="p">(),</span> <span class="n">CD_IS</span><span class="p">(),</span> <span class="n">CD_DR</span><span class="p">()],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>It estimates the cumulative distribution of the trajectory-wise reward and various risk functions as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cdf_dict</span> <span class="o">=</span> <span class="n">cd_ope</span><span class="o">.</span><span class="n">estimate_cumulative_distribution_function</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>
<span class="n">variance_dict</span> <span class="o">=</span> <span class="n">cd_ope</span><span class="o">.</span><span class="n">estimate_variance</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip" id="tip-cumulative-distribution-ope">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to conduct Cumulative Distribution OPE with multiple logged datasets?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Conducting Cumulative Distribution OPE with multiple logged datasets requires no additional efforts.</p>
<p class="sd-card-text">First, the same command with the single logged dataset case also works with multiple logged datasets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span> <span class="o">=</span> <span class="n">CumulativeDistributionOPE</span><span class="p">(</span>
    <span class="n">logged_dataset</span><span class="o">=</span><span class="n">logged_dataset</span><span class="p">,</span>  <span class="c1"># MultipleLoggedDataset</span>
    <span class="n">ope_estimators</span><span class="o">=</span><span class="p">[</span><span class="n">CD_DM</span><span class="p">(),</span> <span class="n">CD_IS</span><span class="p">(),</span> <span class="n">CD_DR</span><span class="p">()],</span>
<span class="p">)</span>
<span class="n">multiple_cdf_dict</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_cumulative_distribution_function</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>  <span class="c1"># MultipleInputDict</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">The returned value is the dictionary containing the ope result.</p>
<p class="sd-card-text">In addition, we can specify which logged dataset and input_dict to use by setting <code class="docutils literal notranslate"><span class="pre">behavior_policy_name</span></code> and <code class="docutils literal notranslate"><span class="pre">dataset_id</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">multiple_ope_dict</span> <span class="o">=</span> <span class="n">ope</span><span class="o">.</span><span class="n">estimate_cumulative_distribution_function</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>
    <span class="n">behavior_policy_name</span><span class="o">=</span><span class="n">behavior_policy</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>  <span class="c1">#</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># specify which logged dataset and input_dict to use</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">The basic visualization function also work by specifying the dataset id.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span><span class="o">.</span><span class="n">visualize_cumulative_distribution_function</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>
    <span class="n">behavior_policy_name</span><span class="o">=</span><span class="n">behavior_policy</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>  <span class="c1">#</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1">#</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_cumulative_distribution_function.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">cumulative distribution function estimated with the specified dataset</p>
</div>
</div>
<p class="sd-card-text">Moreover, we provide additional visualization function for the multiple logged dataset case.</p>
<p class="sd-card-text">The following visualizes confidence intervals of cumulative distribution function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span><span class="o">.</span><span class="n">visualize_cumulative_distribution_function_with_multiple_estimates</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>      <span class="c1"># MultipleInputDict</span>
    <span class="n">behavior_policy_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>                   <span class="c1"># compare estimators with multiple behavior policies</span>
    <span class="c1"># behavior_policy_name=behavior_policy.name  # compare estimators with a single behavior policy</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_cumulative_distribution_function_multiple.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">cumulative distribution function estimated with the multiple datasets</p>
</div>
</div>
<p class="sd-card-text">On contrary, the following visualizes the distribution of multiple estimates of ponit-wise policy performance
(e.g., policy value, variance, conditional value at risk, lower quartile).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ope</span><span class="o">.</span><span class="n">visualize_policy_value_with_multiple_estimates</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>      <span class="c1"># MultipleInputDict</span>
    <span class="n">plot_type</span><span class="o">=</span><span class="s2">&quot;ci&quot;</span><span class="p">,</span>  <span class="c1"># one of {&quot;ci&quot;, &quot;violin&quot;, &quot;scatter&quot;}, default=&quot;ci&quot;</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">When the <code class="docutils literal notranslate"><span class="pre">plot_type</span></code> is “ci”, the plot is somewhat similar to the basic visualization.
(The star indicates the ground-truth policy value and the confidence intervals are derived by multiple estimates across datasets.)</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_cumulative_policy_value_basic_multiple.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">policy value estimated with the multiple datasets</p>
</div>
</div>
<p class="sd-card-text">When the <code class="docutils literal notranslate"><span class="pre">plot_type</span></code> is “violin”, the plot visualizes the distribution of multiple estimates.
This is particularly useful to see how the estimation result can vary depending on different datasets or random seeds.</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_cumulative_policy_value_basic_multiple_violin.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">policy value estimated with the multiple datasets (violin)</p>
</div>
</div>
<p class="sd-card-text">Finally, when the <code class="docutils literal notranslate"><span class="pre">plot_type</span></code> is “scatter”, the plot visualizes each estimation with its color specifying the behavior policy.
This function is particularly useful to see how the choice of behavior policy (e.g., their stochasticity) affects the estimation result.</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ope_cumulative_policy_value_basic_multiple_scatter.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">policy value estimated with the multiple datasets (scatter)</p>
</div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tips-synthetic-dataset"><span class="std std-ref">How to obtain MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tip-opl"><span class="std std-ref">How to handle OPL with MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#tip-create-input-dict"><span class="std std-ref">How to create input_dict for MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="examples/multiple.html"><span class="doc">Examples with MultipleLoggedDataset</span></a></p></li>
</ul>
</div>
</div>
</details></div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="quickstart.html"><span class="doc">Quickstart</span></a> and <a class="reference internal" href="examples/cumulative_dist_ope.html"><span class="doc">related example codes</span></a></p></li>
</ul>
</div>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">CumulativeDistributionOffPolicyEvaluation</span></code> implements the following functions.</p>
<p>(Cumulative Distribution Function)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_cumulative_distribution_function</span></code></p></li>
</ul>
<p>(Risk Functions and Statistics)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_mean</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_variance</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_conditional_value_at_risk</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">estimate_interquartile_range</span></code></p></li>
</ul>
<p>(Visualization)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_conditional_value_at_risk</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_interquartile_range</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_cumulative_distribution_function</span></code></p></li>
</ul>
<p>(Visualization with multiple estimates on multiple logged datasets)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_with_multiple_estimates</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_variance_with_multiple_estimates</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_cumulative_distribution_function_with_multiple_estimates</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_lower_quartile_with_multiple_estimates</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_cumulative_distribution_function_with_multiple_estimates</span></code></p></li>
</ul>
<p>(Others)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">obtain_reward_scale</span></code></p></li>
</ul>
<p>Below, we describe the implemented cumulative distribution OPE estimators.</p>
<table class="table">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#implementation-cd-dm"><span class="std std-ref">Direct Method (DM)</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#implementation-cd-tis"><span class="std std-ref">Trajectory-wise Importance Sampling (TIS)</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#implementation-cd-tdr"><span class="std std-ref">Trajectory-wise Doubly Robust (DR)</span></a></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Self-Normalized estimators</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Extension to the continuous action space</p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to define my own cumulative distribution OPE estimator?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">To define your own OPE estimator, use <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseCumulativeDistributionOPEEstimator</span></code>.</p>
<p class="sd-card-text">Basically, the common inputs for each functions are <code class="docutils literal notranslate"><span class="pre">reward_scale</span></code> (np.ndarray indicating x-axis of cumulative distribution function)
and the following keys from <code class="docutils literal notranslate"><span class="pre">logged_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">input_dict</span></code>.</p>
<p class="sd-card-text">(logged_dataset)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">key</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">size</span><span class="p">,</span>
    <span class="n">step_per_trajectory</span><span class="p">,</span>
    <span class="n">action</span><span class="p">,</span>
    <span class="n">reward</span><span class="p">,</span>
    <span class="n">pscore</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text">(input_dict)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">key</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">evaluation_policy_action</span><span class="p">,</span>
    <span class="n">evaluation_policy_action_dist</span><span class="p">,</span>
    <span class="n">state_action_value_prediction</span><span class="p">,</span>
    <span class="n">initial_state_value_prediction</span><span class="p">,</span>
    <span class="n">state_action_marginal_importance_weight</span><span class="p">,</span>
    <span class="n">state_marginal_importance_weight</span><span class="p">,</span>
    <span class="n">on_policy_policy_value</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text"><code class="docutils literal notranslate"><span class="pre">action_scaler</span></code> and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> are also added in the continuous-action case.</p>
<p class="sd-card-text">If you want to add other arguments, please add them in the initialization arguments for API consistency.</p>
<p class="sd-card-text">Finally, contribution to SCOPE-RL with a new OPE estimator is more than welcome! Please read <a class="reference external" href="https://github.com/hakuhodo-technologies/scope-rl/blob/main/CONTRIBUTING.md">the guidelines for contribution (CONTRIBUTING.md)</a>.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p class="sd-card-text"><a class="reference internal" href="_autosummary/scope_rl.ope.estimators_base.html"><span class="doc">API reference of BaseOffPolicyEstimator</span></a> explains the abstract methods.</p>
</div>
</div>
</details></div>
<section id="implementation-cd-dm">
<span id="id27"></span><h3>Direct Method (DM)<a class="headerlink" href="#implementation-cd-dm" title="Permalink to this heading">#</a></h3>
<p>DM adopts a model-based approach to estimate the cumulative distribution function.</p>
<div class="math notranslate nohighlight">
\[\hat{F}_{\mathrm{DM}}(m, \pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n \sum_{a \in \mathcal{A}} \pi(a | s_0^{(i)}) \hat{G}(m; s_0^{(i)}, a)\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{F}(\cdot)\)</span> is the estimated cumulative distribution function and <span class="math notranslate nohighlight">\(\hat{G}(\cdot)\)</span> is an estimator for <span class="math notranslate nohighlight">\(\mathbb{E} \left[ \mathbb{I} \left \{\sum_{t=0}^{T-1} \gamma^t r_t \leq m \right \} \mid s,a \right]\)</span>.</p>
<p>DM is vulnerable to the approximation error, but has low variance.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CumulativeDistributionDM</span></code></p></li>
</ul>
</div></blockquote>
</section>
<section id="implementation-cd-tis">
<span id="id28"></span><h3>Trajectory-wise Importance Sampling (TIS)<a class="headerlink" href="#implementation-cd-tis" title="Permalink to this heading">#</a></h3>
<p>TIS corrects the distribution shift by applying importance sampling technique on the cumulative distribution estimation.</p>
<div class="math notranslate nohighlight">
\[\hat{F}_{\mathrm{TIS}}(m, \pi; \mathcal{D}) := \frac{1}{n} \sum_{i=1}^n w_{0:T-1}^{(i)} \mathbb{I} \left \{\sum_{t=0}^{T-1} \gamma^t r_t^{(i)} \leq m \right \}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_{0:T-1} := \prod_{t=0}^{T-1} (\pi(a_t | s_t) / \pi_0(a_t | s_t))\)</span> is the trajectory-wise importance weight.
TIS is unbiased but can suffer from high variance.
As a consequence, <span class="math notranslate nohighlight">\(\hat{F}_{\mathrm{TIS}}(\cdot)\)</span> sometimes becomes more than 1.0 when the variance is high.
Therefore, we correct CDF as follows <span id="id29">[<a class="reference internal" href="references.html#id9" title="Audrey Huang, Liu Leqi, Zachary Lipton, and Kamyar Azizzadenesheli. Off-policy risk assessment in contextual bandits. In Advances in Neural Information Processing Systems, volume 34, 23714–23726. 2021.">8</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[\hat{F}^{\ast}_{\mathrm{TIS}}(m, \pi; \mathcal{D}) := \min(\max_{m' \leq m} \hat{F}_{\mathrm{TIS}}(m', \pi; \mathcal{D}), 1)\]</div>
<p>.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CumulativeDistributionTIS</span></code></p></li>
</ul>
</div></blockquote>
</section>
<section id="trajectory-wise-doubly-robust-tdr">
<span id="implementation-cd-tdr"></span><h3>Trajectory-wise Doubly Robust (TDR)<a class="headerlink" href="#trajectory-wise-doubly-robust-tdr" title="Permalink to this heading">#</a></h3>
<p>TDR combines TIS and DM to reduce the variance while being unbiased.</p>
<div class="math notranslate nohighlight">
\[\hat{F}_{\mathrm{TDR}}(m, \pi; \mathcal{D})
:= \frac{1}{n} \sum_{i=1}^n w_{0:T-1}^{(i)} \left( \mathbb{I} \left \{\sum_{t=0}^{T-1} \gamma^t r_t^{(i)} \leq m \right \} - \hat{G}(m; s_0^{(i)}, a_0^{(i)}) \right)
+ \hat{F}_{\mathrm{DM}}(m, \pi; \mathcal{D})\]</div>
<p>TDR reduces the variance of TIS while being unbiased, leveraging the model-based estimate (i.e., DM) as a control variate.
Since <span class="math notranslate nohighlight">\(\hat{F}_{\mathrm{TDR}}(\cdot)\)</span> may be less than zero or more than one, we should apply the following transformation to bound <span class="math notranslate nohighlight">\(\hat{F}_{\mathrm{TDR}}(\cdot) \in [0, 1]\)</span> <span id="id30">[<a class="reference internal" href="references.html#id9" title="Audrey Huang, Liu Leqi, Zachary Lipton, and Kamyar Azizzadenesheli. Off-policy risk assessment in contextual bandits. In Advances in Neural Information Processing Systems, volume 34, 23714–23726. 2021.">8</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[\hat{F}^{\ast}_{\mathrm{TDR}}(m, \pi; \mathcal{D}) := \mathrm{clip}(\max_{m' \leq m} \hat{F}_{\mathrm{TDR}}(m', \pi; \mathcal{D}), 0, 1).\]</div>
<p>Note that this estimator is not equivalent to the (recursive) DR estimator defined by <span id="id31">[<a class="reference internal" href="references.html#id10" title="Audrey Huang, Liu Leqi, Zachary Lipton, and Kamyar Azizzadenesheli. Off-policy risk assessment for markov decision processes. In roceedings of the 25th International Conference on Artificial Intelligence and Statistics, 5022–5050. 2022.">9</a>]</span>. We are planning to implement the recursive version in a future update of the software.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CumulativeDistributionTDR</span></code></p></li>
</ul>
</div></blockquote>
<p>Finally, we also provide the self-normalized estimators for TIS and TDR.
They use the self-normalized importance weight <span class="math notranslate nohighlight">\(\tilde{w}_{\ast} := w_{\ast} / (\sum_{i=1}^{n} w_{\ast})\)</span> for the variance reduction purpose.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CumulativeDistributionSNTIS</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CumulativeDistributionSNDR</span></code></p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="evaluation-metrics-of-ope-ops">
<span id="implementation-eval-ope-ops"></span><h2>Evaluation Metrics of OPE/OPS<a class="headerlink" href="#evaluation-metrics-of-ope-ops" title="Permalink to this heading">#</a></h2>
<p>Finally, we describe the metrics to evaluate the quality of OPE estimators and its OPS results.</p>
<ul class="simple">
<li><dl class="simple">
<dt>Mean Squared Error (MSE) <span id="id32">[<a class="reference internal" href="references.html#id5" title="Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. arXiv preprint arXiv:2007.09055, 2020.">24</a>, <a class="reference internal" href="references.html#id6" title="Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy evaluation for reinforcement learning. Advances in Neural Information Processing Systems, 2019.">25</a>, <a class="reference internal" href="references.html#id4" title="Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, and Thomas Paine. Benchmarks for deep off-policy evaluation. In Proceedings of the 9th International Conference on Learning Representations. 2021.">26</a>]</span>:</dt><dd><p>This metrics measures the estimation accuracy as <span class="math notranslate nohighlight">\(\sum_{\pi \in \Pi} (\hat{J}(\pi; \mathcal{D}) - J(\pi))^2 / |\Pi|\)</span>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Regret&#64;k <span id="id33">[<a class="reference internal" href="references.html#id5" title="Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. arXiv preprint arXiv:2007.09055, 2020.">24</a>, <a class="reference internal" href="references.html#id4" title="Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, and Thomas Paine. Benchmarks for deep off-policy evaluation. In Proceedings of the 9th International Conference on Learning Representations. 2021.">26</a>]</span>:</dt><dd><p>This metrics measures how well the selected policy(ies) performs. In particular, Regret&#64;1 indicates the expected performance difference between the (oracle) best policy and the selected policy as <span class="math notranslate nohighlight">\(J(\pi^{\ast}) - J(\hat{\pi}^{\ast})\)</span>, where <span class="math notranslate nohighlight">\(\pi^{\ast} := {\arg\max}_{\pi \in \Pi} J(\pi)\)</span> and <span class="math notranslate nohighlight">\(\hat{\pi}^{\ast} := {\arg\max}_{\pi \in \Pi} \hat{J}(\pi; \mathcal{D})\)</span>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Spearman’s Rank Correlation Coefficient <span id="id34">[<a class="reference internal" href="references.html#id5" title="Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. arXiv preprint arXiv:2007.09055, 2020.">24</a>, <a class="reference internal" href="references.html#id4" title="Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, and Thomas Paine. Benchmarks for deep off-policy evaluation. In Proceedings of the 9th International Conference on Learning Representations. 2021.">26</a>]</span>:</dt><dd><p>This metrics measures how well the raking of the candidate estimators are preserved in the OPE result.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Type I and Type II Error Rate:</dt><dd><p>This metrics measures how well an OPE estimator validates whether the policy performance surpasses the given safety threshold or not.</p>
</dd>
</dl>
</li>
</ul>
<p>To ease the comparison of candidate (evaluation) policies and the OPE estimators, we provide the <code class="xref py py-class docutils literal notranslate"><span class="pre">OffPolicySelection</span></code> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize the OPS class</span>
<span class="kn">from</span> <span class="nn">scope_rl.ope</span> <span class="kn">import</span> <span class="n">OffPolicySelection</span>
<span class="n">ops</span> <span class="o">=</span> <span class="n">OffPolicySelection</span><span class="p">(</span>
    <span class="n">ope</span><span class="o">=</span><span class="n">ope</span><span class="p">,</span>
    <span class="n">cumulative_distribution_ope</span><span class="o">=</span><span class="n">cd_ope</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">OffPolicySelection</span></code> class returns both the OPE results and the OPS metrics as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ranking_df</span><span class="p">,</span> <span class="n">metric_df</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select_by_policy_value</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>
    <span class="n">return_metrics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">return_by_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Moreover, the OPS class enables us to validate the best/worst/mean performance of top k deployment and how well the safety requirement is satisfied.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ops</span><span class="o">.</span><span class="n">visualize_topk_policy_value_selected_by_standard_ope</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
    <span class="n">safety_criteria</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Finally, the OPS class also implements the modules to compare the OPE result and the true policy metric as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ops</span><span class="o">.</span><span class="n">visualize_policy_value_for_validation</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
    <span class="n">n_cols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">share_axes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
How to conduct OPS with multiple logged datasets?<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Conducting OPS with multiple logged datasets requires no additional efforts.</p>
<p class="sd-card-text">First, the same command with the single logged dataset case also works with multiple logged datasets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ops</span> <span class="o">=</span> <span class="n">OffPolicySelection</span><span class="p">(</span>
    <span class="n">ope</span><span class="o">=</span><span class="n">ope</span><span class="p">,</span>                             <span class="c1"># initialized with MultipleLoggedDataset</span>
    <span class="n">cumulative_distribution_ope</span><span class="o">=</span><span class="n">cd_ope</span><span class="p">,</span>  <span class="c1"># initialized with MultipleLoggedDataset</span>
<span class="p">)</span>
<span class="n">ranking_df</span><span class="p">,</span> <span class="n">metric_df</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select_by_policy_value</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="p">,</span>  <span class="c1"># MultipleInputDict</span>
    <span class="n">return_metrics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">return_by_dataframe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">The returned value is dictionary containing the ops result.</p>
<p class="sd-card-text">Next, visualization functions for OPS demonstrate the aggregated ops result by default.
For example, the average topk performance and its confidence intervals is shown for topk visualization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ops</span><span class="o">.</span><span class="n">visualize_topk_policy_value_selected_by_standard_ope</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
    <span class="n">safety_criteria</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ops_topk_policy_value_multiple.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">top-k deployment result with multiple logged datasets</p>
</div>
</div>
<p class="sd-card-text">In the validation visualization, colors indicate the behavior policies.
This function is particularly useful to see how the choice of behavior policy (e.g., their stochasticity) affects the estimation result.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ops</span><span class="o">.</span><span class="n">visualize_policy_value_for_validation</span><span class="p">(</span>
    <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
    <span class="n">n_cols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">share_axes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm sd-text-center docutils">
<img alt="card-img-top" class="sd-card-img-top" src="../_images/ops_validation_policy_value_multiple.png" />
<div class="sd-card-body docutils">
<p class="sd-card-text">validation results on multiple logged datasets</p>
</div>
</div>
<p class="sd-card-text">Note that when the <code class="docutils literal notranslate"><span class="pre">behavior_policy_name</span></code> and <code class="docutils literal notranslate"><span class="pre">dataset_id</span></code> is specified, the methods show the result on the specified dataset.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tips-synthetic-dataset"><span class="std std-ref">How to obtain MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="learning_implementation.html#tip-opl"><span class="std std-ref">How to handle OPL with MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#tip-create-input-dict"><span class="std std-ref">How to create input_dict for MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#tip-ope"><span class="std std-ref">How to conduct OPE with MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="#tip-cumulative-distribution-ope"><span class="std std-ref">How to conduct Cumulative Distribution OPE with MultipleLoggedDataset?</span></a></p></li>
<li><p class="sd-card-text"><a class="reference internal" href="examples/multiple.html"><span class="doc">Examples with MultipleLoggedDataset</span></a></p></li>
</ul>
</div>
</div>
</details></div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="quickstart.html"><span class="doc">Quickstart</span></a> and <a class="reference internal" href="examples/assessments.html"><span class="doc">related example codes</span></a></p></li>
</ul>
</div>
<p>The OPS class implements the following functions.</p>
<p>(OPS)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">obtain_oracle_selection_result</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">select_by_policy_value</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">select_by_policy_value_via_cumulative_distribution_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">select_by_policy_value_lower_bound</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">select_by_lower_quartile</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">select_by_conditional_value_at_risk</span></code></p></li>
</ul>
<p>(Visualization)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_for_selection</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_cumulative_distribution_function_for_selection</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_for_selection</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_of_cumulative_distribution_ope_for_selection</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_conditional_value_at_risk_for_selection</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_interquartile_range_for_selection</span></code></p></li>
</ul>
<p>(Visualization with multiple estimates on multiple logged datasets)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_with_multiple_estimates_standard_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_with_multiple_estimates_cumulative_distribution_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_variance_with_multiple_estimates</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_cumulative_distribution_function_with_multiple_estimates</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_lower_quartile_with_multiple_estimates</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_cumulative_distribution_function_with_multiple_estimates</span></code></p></li>
</ul>
<p>(Visualization of top k performance)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_topk_policy_value_selected_by_standard_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_topk_policy_value_selected_by_cumulative_distribution_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_topk_policy_value_selected_by_lower_bound</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_topk_conditional_value_at_risk_selected_by_standard_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_topk_lower_quartile_selected_by_standard_ope</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_topk_lower_quartile_selected_by_cumulative_distribution_ope</span></code></p></li>
</ul>
<p>(Visualization for validation)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_for_validation</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_of_cumulative_distribution_ope_for_validation</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_policy_value_lower_bound_for_validation</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_variance_for_validation</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_lower_quartile_for_validation</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">visualize_conditional_value_at_risk_for_validation</span></code></p></li>
</ul>
<div class="white-space-20px"></div><div class="sd-container-fluid sd-sphinx-override sd-m-0 docutils">
<div class="sd-row docutils">
<div class="sd-col sd-d-flex-column sd-col-3 sd-col-xs-3 sd-col-sm-3 sd-col-md-3 sd-col-lg-3 sd-m-0 sd-p-0 docutils">
<div class="sd-container-fluid sd-sphinx-override sd-m-0 docutils">
<div class="sd-row docutils">
<div class="sd-col sd-d-flex-row sd-m-0 sd-p-0 docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-none sd-card-hover docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">&lt;&lt;&lt; Prev
<strong>Problem Formulation</strong></p>
</div>
<span class="sd-stretched-link"></span></div>
</div>
<div class="sd-col sd-d-flex-row sd-m-0 sd-p-0 docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-none sd-card-hover docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">&lt;&lt;&lt; Prev
<strong>Offline RL</strong></p>
</div>
<a class="sd-stretched-link reference internal" href="learning_implementation.html"><span class="doc"></span></a></div>
</div>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-column sd-col-6 sd-col-xs-6 sd-col-sm-6 sd-col-md-6 sd-col-lg-6 sd-m-0 sd-p-0 docutils">
</div>
<div class="sd-col sd-d-flex-column sd-col-3 sd-col-xs-3 sd-col-sm-3 sd-col-md-3 sd-col-lg-3 sd-m-0 sd-p-0 docutils">
<div class="sd-container-fluid sd-sphinx-override sd-m-0 docutils">
<div class="sd-row docutils">
<div class="sd-col sd-d-flex-row sd-m-0 sd-p-0 docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-none sd-card-hover docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">Next &gt;&gt;&gt;
<strong>Visualization tools</strong></p>
</div>
<a class="sd-stretched-link reference internal" href="visualization.html"><span class="doc"></span></a></div>
</div>
<div class="sd-col sd-d-flex-row sd-m-0 sd-p-0 docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-none sd-card-hover docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">Next &gt;&gt;&gt;
<strong>Package Reference</strong></p>
</div>
<a class="sd-stretched-link reference internal" href="scope_rl_api.html"><span class="doc"></span></a></div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>


                </article>
              
              
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-ope-input">Create OPE Input</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-off-policy-evaluation-ope">Basic Off-Policy Evaluation (OPE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#direct-method-dm">Direct Method (DM)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trajectory-wise-importance-sampling-tis">Trajectory-wise Importance Sampling (TIS)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#per-decision-importance-sampling-pdis">Per-Decision Importance Sampling (PDIS)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#doubly-robust-dr">Doubly Robust (DR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-normalized-estimators">Self-Normalized estimators</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginalized-importance-sampling-estimators">Marginalized Importance Sampling Estimators</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#double-reinforcement-learning-drl">Double Reinforcement Learning (DRL)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectrum-of-off-policy-estimators-sope">Spectrum of Off-Policy Estimators (SOPE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#high-confidence-off-policy-evaluation-hcope">High Confidence Off-Policy Evaluation (HCOPE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-to-the-continuous-action-space">Extension to the Continuous Action Space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-off-policy-evaluation-cd-ope">Cumulative Distribution Off-Policy Evaluation (CD-OPE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-cd-dm">Direct Method (DM)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-cd-tis">Trajectory-wise Importance Sampling (TIS)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trajectory-wise-doubly-robust-tdr">Trajectory-wise Doubly Robust (TDR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics-of-ope-ops">Evaluation Metrics of OPE/OPS</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner"></div>
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2023, Haruka Kiyohara, Ren Kishimoto, HAKUHODO Technologies Inc., Hanjuku-kaso Co., Ltd.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>
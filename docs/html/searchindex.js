Search.setIndex({"docnames": ["documentation/_autosummary/basicgym/env/basicgym.envs.synthetic", "documentation/_autosummary/basicgym/env/basicgym.envs.synthetic.BasicEnv", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.base", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.base.BaseRewardFunction", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.base.BaseStateTransitionFunction", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.function", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.function.RewardFunction", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.function.StateTransitionFunction", "documentation/_autosummary/dataset/scope_rl.dataset.base", "documentation/_autosummary/dataset/scope_rl.dataset.base.BaseDataset", "documentation/_autosummary/dataset/scope_rl.dataset.synthetic", "documentation/_autosummary/dataset/scope_rl.dataset.synthetic.SyntheticDataset", "documentation/_autosummary/recgym/env/recgym.envs.rec", "documentation/_autosummary/recgym/env/recgym.envs.rec.RECEnv", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.base", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.base.BaseUserModel", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.function", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.function.UserModel", "documentation/_autosummary/rtbgym/env/rtbgym.envs.rtb", "documentation/_autosummary/rtbgym/env/rtbgym.envs.rtb.RTBEnv", "documentation/_autosummary/rtbgym/env/rtbgym.envs.wrapper_rtb", "documentation/_autosummary/rtbgym/env/rtbgym.envs.wrapper_rtb.CustomizedRTBEnv", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseClickAndConversionRate", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseSimulator", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseWinningPriceDistribution", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.bidder", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.bidder.Bidder", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.ClickThroughRate", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.ConversionRate", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.WinningPriceDistribution", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.rtb_synthetic", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator", "documentation/_autosummary/rtbgym/utils/rtbgym.utils", "documentation/_autosummary/rtbgym/utils/rtbgym.utils.NormalDistribution", "documentation/_autosummary/rtbgym/utils/rtbgym.utils.check_array", "documentation/_autosummary/rtbgym/utils/rtbgym.utils.sigmoid", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.DirectMethod", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.DoublyRobust", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalDM", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalDR", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalIS", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.DirectMethod", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.DoublyRobust", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalDM", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalDR", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalIS", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS", "documentation/_autosummary/scope_rl.ope.estimators_base", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseMarginalOPEEstimator", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseOffPolicyEstimator", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator", "documentation/_autosummary/scope_rl.ope.input", "documentation/_autosummary/scope_rl.ope.input.CreateOPEInput", "documentation/_autosummary/scope_rl.ope.online", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_conditional_value_at_risk", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_cumulative_distribution_function", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_interquartile_range", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_policy_value", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_policy_value_interval", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_statistics", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_variance", "documentation/_autosummary/scope_rl.ope.online.rollout_policy_online", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_interquartile_range", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_policy_value", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_policy_value_with_variance", "documentation/_autosummary/scope_rl.ope.ope", "documentation/_autosummary/scope_rl.ope.ope.CumulativeDistributionOPE", "documentation/_autosummary/scope_rl.ope.ope.OffPolicyEvaluation", "documentation/_autosummary/scope_rl.ope.ops", "documentation/_autosummary/scope_rl.ope.ops.OffPolicySelection", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.base", "documentation/_autosummary/scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.ContinuousQFunction", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.DiscreteQFunction", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.StateWeightFunction", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.VFunction", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning", "documentation/_autosummary/scope_rl.policy.head", "documentation/_autosummary/scope_rl.policy.head.BaseHead", "documentation/_autosummary/scope_rl.policy.head.ContinuousEvalHead", "documentation/_autosummary/scope_rl.policy.head.EpsilonGreedyHead", "documentation/_autosummary/scope_rl.policy.head.GaussianHead", "documentation/_autosummary/scope_rl.policy.head.OnlineHead", "documentation/_autosummary/scope_rl.policy.head.SoftmaxHead", "documentation/_autosummary/scope_rl.policy.head.TruncatedGaussianHead", "documentation/_autosummary/scope_rl.utils", "documentation/_autosummary/scope_rl.utils.MinMaxActionScaler", "documentation/_autosummary/scope_rl.utils.MinMaxScaler", "documentation/_autosummary/scope_rl.utils.MultipleInputDict", "documentation/_autosummary/scope_rl.utils.MultipleLoggedDataset", "documentation/_autosummary/scope_rl.utils.NewGymAPIWrapper", "documentation/_autosummary/scope_rl.utils.OldGymAPIWrapper", "documentation/_autosummary/scope_rl.utils.check_array", "documentation/_autosummary/scope_rl.utils.check_input_dict", "documentation/_autosummary/scope_rl.utils.check_logged_dataset", "documentation/_autosummary/scope_rl.utils.cosine_kernel", "documentation/_autosummary/scope_rl.utils.defaultdict_to_dict", "documentation/_autosummary/scope_rl.utils.epanechnikov_kernel", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_bootstrap", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_hoeffding", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_t_test", "documentation/_autosummary/scope_rl.utils.gaussian_kernel", "documentation/_autosummary/scope_rl.utils.l2_distance", "documentation/_autosummary/scope_rl.utils.triangular_kernel", "documentation/_autosummary/scope_rl.utils.uniform_kernel", "documentation/distinctive_features", "documentation/evaluation_implementation", "documentation/examples/assessments", "documentation/examples/basic_ope", "documentation/examples/cumulative_dist_ope", "documentation/examples/custom_estimators", "documentation/examples/index", "documentation/examples/multiple", "documentation/examples/ops", "documentation/examples/real_world", "documentation/frequently_asked_questions", "documentation/index", "documentation/installation", "documentation/learning_implementation", "documentation/news", "documentation/online_offline_rl", "documentation/ope_ops", "documentation/quickstart", "documentation/references", "documentation/release_notes", "documentation/scope_rl_api", "documentation/sharpe_ratio", "documentation/subpackages/basicgym_about", "documentation/subpackages/basicgym_api", "documentation/subpackages/index", "documentation/subpackages/recgym_about", "documentation/subpackages/recgym_api", "documentation/subpackages/rtbgym_about", "documentation/subpackages/rtbgym_api", "documentation/visualization", "index"], "filenames": ["documentation/_autosummary/basicgym/env/basicgym.envs.synthetic.rst", "documentation/_autosummary/basicgym/env/basicgym.envs.synthetic.BasicEnv.rst", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.base.rst", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.base.BaseRewardFunction.rst", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.base.BaseStateTransitionFunction.rst", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.function.rst", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.function.RewardFunction.rst", "documentation/_autosummary/basicgym/simulation/basicgym.envs.simulator.function.StateTransitionFunction.rst", "documentation/_autosummary/dataset/scope_rl.dataset.base.rst", "documentation/_autosummary/dataset/scope_rl.dataset.base.BaseDataset.rst", "documentation/_autosummary/dataset/scope_rl.dataset.synthetic.rst", "documentation/_autosummary/dataset/scope_rl.dataset.synthetic.SyntheticDataset.rst", "documentation/_autosummary/recgym/env/recgym.envs.rec.rst", "documentation/_autosummary/recgym/env/recgym.envs.rec.RECEnv.rst", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.base.rst", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.base.BaseUserModel.rst", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.function.rst", "documentation/_autosummary/recgym/simulation/recgym.envs.simulator.function.UserModel.rst", "documentation/_autosummary/rtbgym/env/rtbgym.envs.rtb.rst", "documentation/_autosummary/rtbgym/env/rtbgym.envs.rtb.RTBEnv.rst", "documentation/_autosummary/rtbgym/env/rtbgym.envs.wrapper_rtb.rst", "documentation/_autosummary/rtbgym/env/rtbgym.envs.wrapper_rtb.CustomizedRTBEnv.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseClickAndConversionRate.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseSimulator.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.base.BaseWinningPriceDistribution.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.bidder.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.bidder.Bidder.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.ClickThroughRate.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.ConversionRate.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.function.WinningPriceDistribution.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.rtb_synthetic.rst", "documentation/_autosummary/rtbgym/simulation/rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.rst", "documentation/_autosummary/rtbgym/utils/rtbgym.utils.rst", "documentation/_autosummary/rtbgym/utils/rtbgym.utils.NormalDistribution.rst", "documentation/_autosummary/rtbgym/utils/rtbgym.utils.check_array.rst", "documentation/_autosummary/rtbgym/utils/rtbgym.utils.sigmoid.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.DirectMethod.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.DoublyRobust.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS.rst", "documentation/_autosummary/scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling.rst", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.rst", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.rst", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.rst", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.rst", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.rst", "documentation/_autosummary/scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalDM.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalDR.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalIS.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR.rst", "documentation/_autosummary/scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.DirectMethod.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.DoublyRobust.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling.rst", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.rst", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.rst", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.rst", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.rst", "documentation/_autosummary/scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalDM.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalDR.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalIS.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR.rst", "documentation/_autosummary/scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS.rst", "documentation/_autosummary/scope_rl.ope.estimators_base.rst", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.rst", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseMarginalOPEEstimator.rst", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseOffPolicyEstimator.rst", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator.rst", "documentation/_autosummary/scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator.rst", "documentation/_autosummary/scope_rl.ope.input.rst", "documentation/_autosummary/scope_rl.ope.input.CreateOPEInput.rst", "documentation/_autosummary/scope_rl.ope.online.rst", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_conditional_value_at_risk.rst", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_cumulative_distribution_function.rst", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_interquartile_range.rst", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_policy_value.rst", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_policy_value_interval.rst", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_statistics.rst", "documentation/_autosummary/scope_rl.ope.online.calc_on_policy_variance.rst", "documentation/_autosummary/scope_rl.ope.online.rollout_policy_online.rst", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk.rst", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function.rst", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_interquartile_range.rst", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_policy_value.rst", "documentation/_autosummary/scope_rl.ope.online.visualize_on_policy_policy_value_with_variance.rst", "documentation/_autosummary/scope_rl.ope.ope.rst", "documentation/_autosummary/scope_rl.ope.ope.CumulativeDistributionOPE.rst", "documentation/_autosummary/scope_rl.ope.ope.OffPolicyEvaluation.rst", "documentation/_autosummary/scope_rl.ope.ops.rst", "documentation/_autosummary/scope_rl.ope.ops.OffPolicySelection.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.base.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.ContinuousQFunction.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.DiscreteQFunction.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.StateWeightFunction.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.function.VFunction.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.rst", "documentation/_autosummary/scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.rst", "documentation/_autosummary/scope_rl.policy.head.rst", "documentation/_autosummary/scope_rl.policy.head.BaseHead.rst", "documentation/_autosummary/scope_rl.policy.head.ContinuousEvalHead.rst", "documentation/_autosummary/scope_rl.policy.head.EpsilonGreedyHead.rst", "documentation/_autosummary/scope_rl.policy.head.GaussianHead.rst", "documentation/_autosummary/scope_rl.policy.head.OnlineHead.rst", "documentation/_autosummary/scope_rl.policy.head.SoftmaxHead.rst", "documentation/_autosummary/scope_rl.policy.head.TruncatedGaussianHead.rst", "documentation/_autosummary/scope_rl.utils.rst", "documentation/_autosummary/scope_rl.utils.MinMaxActionScaler.rst", "documentation/_autosummary/scope_rl.utils.MinMaxScaler.rst", "documentation/_autosummary/scope_rl.utils.MultipleInputDict.rst", "documentation/_autosummary/scope_rl.utils.MultipleLoggedDataset.rst", "documentation/_autosummary/scope_rl.utils.NewGymAPIWrapper.rst", "documentation/_autosummary/scope_rl.utils.OldGymAPIWrapper.rst", "documentation/_autosummary/scope_rl.utils.check_array.rst", "documentation/_autosummary/scope_rl.utils.check_input_dict.rst", "documentation/_autosummary/scope_rl.utils.check_logged_dataset.rst", "documentation/_autosummary/scope_rl.utils.cosine_kernel.rst", "documentation/_autosummary/scope_rl.utils.defaultdict_to_dict.rst", "documentation/_autosummary/scope_rl.utils.epanechnikov_kernel.rst", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_bootstrap.rst", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein.rst", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_hoeffding.rst", "documentation/_autosummary/scope_rl.utils.estimate_confidence_interval_by_t_test.rst", "documentation/_autosummary/scope_rl.utils.gaussian_kernel.rst", "documentation/_autosummary/scope_rl.utils.l2_distance.rst", "documentation/_autosummary/scope_rl.utils.triangular_kernel.rst", "documentation/_autosummary/scope_rl.utils.uniform_kernel.rst", "documentation/distinctive_features.rst", "documentation/evaluation_implementation.rst", "documentation/examples/assessments.rst", "documentation/examples/basic_ope.rst", "documentation/examples/cumulative_dist_ope.rst", "documentation/examples/custom_estimators.rst", "documentation/examples/index.rst", "documentation/examples/multiple.rst", "documentation/examples/ops.rst", "documentation/examples/real_world.rst", "documentation/frequently_asked_questions.rst", "documentation/index.rst", "documentation/installation.rst", "documentation/learning_implementation.rst", "documentation/news.rst", "documentation/online_offline_rl.rst", "documentation/ope_ops.rst", "documentation/quickstart.rst", "documentation/references.rst", "documentation/release_notes.rst", "documentation/scope_rl_api.rst", "documentation/sharpe_ratio.rst", "documentation/subpackages/basicgym_about.rst", "documentation/subpackages/basicgym_api.rst", "documentation/subpackages/index.rst", "documentation/subpackages/recgym_about.rst", "documentation/subpackages/recgym_api.rst", "documentation/subpackages/rtbgym_about.rst", "documentation/subpackages/rtbgym_api.rst", "documentation/visualization.rst", "index.rst"], "titles": ["basicgym.envs.synthetic", "basicgym.envs.synthetic.BasicEnv", "basicgym.envs.simulator.base", "basicgym.envs.simulator.base.BaseRewardFunction", "basicgym.envs.simulator.base.BaseStateTransitionFunction", "basicgym.envs.simulator.function", "basicgym.envs.simulator.function.RewardFunction", "basicgym.envs.simulator.function.StateTransitionFunction", "scope_rl.dataset.base", "scope_rl.dataset.base.BaseDataset", "scope_rl.dataset.synthetic", "scope_rl.dataset.synthetic.SyntheticDataset", "recgym.envs.rec", "recgym.envs.rec.RECEnv", "recgym.envs.simulator.base", "recgym.envs.simulator.base.BaseUserModel", "recgym.envs.simulator.function", "recgym.envs.simulator.function.UserModel", "rtbgym.envs.rtb", "rtbgym.envs.rtb.RTBEnv", "rtbgym.envs.wrapper_rtb", "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv", "rtbgym.envs.simulator.base", "rtbgym.envs.simulator.base.BaseClickAndConversionRate", "rtbgym.envs.simulator.base.BaseSimulator", "rtbgym.envs.simulator.base.BaseWinningPriceDistribution", "rtbgym.envs.simulator.bidder", "rtbgym.envs.simulator.bidder.Bidder", "rtbgym.envs.simulator.function", "rtbgym.envs.simulator.function.ClickThroughRate", "rtbgym.envs.simulator.function.ConversionRate", "rtbgym.envs.simulator.function.WinningPriceDistribution", "rtbgym.envs.simulator.rtb_synthetic", "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator", "rtbgym.utils", "rtbgym.utils.NormalDistribution", "rtbgym.utils.check_array", "rtbgym.utils.sigmoid", "scope_rl.ope.continuous.basic_estimators", "scope_rl.ope.continuous.basic_estimators.DirectMethod", "scope_rl.ope.continuous.basic_estimators.DoublyRobust", "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling", "scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR", "scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS", "scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS", "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling", "scope_rl.ope.continuous.cumulative_distribution_estimators", "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM", "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR", "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS", "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR", "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS", "scope_rl.ope.continuous.marginal_estimators", "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning", "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR", "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS", "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR", "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS", "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM", "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR", "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS", "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR", "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS", "scope_rl.ope.discrete.basic_estimators", "scope_rl.ope.discrete.basic_estimators.DirectMethod", "scope_rl.ope.discrete.basic_estimators.DoublyRobust", "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling", "scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR", "scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS", "scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS", "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling", "scope_rl.ope.discrete.cumulative_distribution_estimators", "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM", "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR", "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS", "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR", "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS", "scope_rl.ope.discrete.marginal_estimators", "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning", "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR", "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS", "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR", "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS", "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM", "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR", "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS", "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR", "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS", "scope_rl.ope.estimators_base", "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator", "scope_rl.ope.estimators_base.BaseMarginalOPEEstimator", "scope_rl.ope.estimators_base.BaseOffPolicyEstimator", "scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator", "scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator", "scope_rl.ope.input", "scope_rl.ope.input.CreateOPEInput", "scope_rl.ope.online", "scope_rl.ope.online.calc_on_policy_conditional_value_at_risk", "scope_rl.ope.online.calc_on_policy_cumulative_distribution_function", "scope_rl.ope.online.calc_on_policy_interquartile_range", "scope_rl.ope.online.calc_on_policy_policy_value", "scope_rl.ope.online.calc_on_policy_policy_value_interval", "scope_rl.ope.online.calc_on_policy_statistics", "scope_rl.ope.online.calc_on_policy_variance", "scope_rl.ope.online.rollout_policy_online", "scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk", "scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function", "scope_rl.ope.online.visualize_on_policy_interquartile_range", "scope_rl.ope.online.visualize_on_policy_policy_value", "scope_rl.ope.online.visualize_on_policy_policy_value_with_variance", "scope_rl.ope.ope", "scope_rl.ope.ope.CumulativeDistributionOPE", "scope_rl.ope.ope.OffPolicyEvaluation", "scope_rl.ope.ops", "scope_rl.ope.ops.OffPolicySelection", "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous", "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning", "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning", "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete", "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning", "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning", "scope_rl.ope.weight_value_learning.base", "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner", "scope_rl.ope.weight_value_learning.function", "scope_rl.ope.weight_value_learning.function.ContinuousQFunction", "scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction", "scope_rl.ope.weight_value_learning.function.DiscreteQFunction", "scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction", "scope_rl.ope.weight_value_learning.function.StateWeightFunction", "scope_rl.ope.weight_value_learning.function.VFunction", "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous", "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning", "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning", "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete", "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning", "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning", "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous", "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning", "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning", "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete", "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning", "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning", "scope_rl.policy.head", "scope_rl.policy.head.BaseHead", "scope_rl.policy.head.ContinuousEvalHead", "scope_rl.policy.head.EpsilonGreedyHead", "scope_rl.policy.head.GaussianHead", "scope_rl.policy.head.OnlineHead", "scope_rl.policy.head.SoftmaxHead", "scope_rl.policy.head.TruncatedGaussianHead", "scope_rl.utils", "scope_rl.utils.MinMaxActionScaler", "scope_rl.utils.MinMaxScaler", "scope_rl.utils.MultipleInputDict", "scope_rl.utils.MultipleLoggedDataset", "scope_rl.utils.NewGymAPIWrapper", "scope_rl.utils.OldGymAPIWrapper", "scope_rl.utils.check_array", "scope_rl.utils.check_input_dict", "scope_rl.utils.check_logged_dataset", "scope_rl.utils.cosine_kernel", "scope_rl.utils.defaultdict_to_dict", "scope_rl.utils.epanechnikov_kernel", "scope_rl.utils.estimate_confidence_interval_by_bootstrap", "scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein", "scope_rl.utils.estimate_confidence_interval_by_hoeffding", "scope_rl.utils.estimate_confidence_interval_by_t_test", "scope_rl.utils.gaussian_kernel", "scope_rl.utils.l2_distance", "scope_rl.utils.triangular_kernel", "scope_rl.utils.uniform_kernel", "Why SCOPE-RL?", "Supported Implementation", "Example Codes for Assessing OPE Estimators", "Example Codes for Basic Off-Policy Evaluation", "Example Codes for Cumulative Distribution OPE", "Example Codes for Custom OPE Estimators", "Example Codes", "Example Codes with Multiple Logged Dataset and Behavior Policies", "Example Codes for Off-Policy Selection", "Guidelines for Preparing Real-World Datasets", "FAQs", "SCOPE-RL", "Installation", "Supported Implementation", "News", "Overview", "Overview", "Quickstart", "References", "Release Notes", "SCOPR-RL Package Reference", "Risk-Return Assessments of OPE via SharpRatio&#64;k", "BasicGym", "BasicGym Package Reference", "Sub-packages", "RECGym", "RECGym Package Reference", "RTBGym", "RTBGym Package Reference", "Visualization Tools", "Welcome!"], "terms": {"basic": [0, 1, 88, 91, 113, 114, 171, 173, 175, 178, 184, 186, 187, 192, 195, 201], "reinforc": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 32, 33, 38, 40, 42, 43, 44, 52, 53, 54, 56, 59, 61, 63, 65, 67, 68, 69, 77, 78, 79, 81, 83, 84, 86, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 113, 114, 142, 144, 145, 146, 147, 148, 149, 171, 183, 187, 189, 192, 193, 196, 198, 201], "learn": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 32, 33, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 171, 177, 183, 187, 189, 192, 193, 196, 198, 200, 201], "rl": [0, 1, 2, 3, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 142, 144, 145, 146, 147, 148, 149, 150, 153, 154, 172, 173, 174, 175, 176, 178, 179, 180, 183, 184, 185, 186, 187, 188, 189, 192, 193, 195, 196, 198, 200, 201], "environ": [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 142, 143, 150, 151, 152, 171, 173, 174, 175, 178, 179, 181, 182, 184, 187, 188, 189, 192, 195, 201], "class": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 171, 172, 173, 174, 175, 176, 178, 179, 184, 188, 193, 196, 200, 201], "basicenv": 0, "step_per_episod": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 94, 95, 110, 112, 113, 114, 174, 175, 178, 184, 188, 193, 196, 198], "10": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 32, 33, 171, 172, 173, 175, 178, 182, 184, 188, 189, 192, 193, 196, 198, 201], "state_dim": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 176, 180, 184, 193], "5": [0, 1, 10, 11, 12, 13, 18, 19, 28, 31, 32, 33, 113, 114, 171, 172, 173, 174, 176, 178, 182, 184, 186, 188, 189, 192, 193, 196, 198, 200], "action_typ": [0, 1, 8, 9, 10, 11, 20, 21, 94, 95, 110, 111, 112, 150, 153, 154, 176, 178, 180, 184, 193, 198], "continu": [0, 1, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 115, 116, 117, 123, 124, 125, 128, 129, 130, 131, 132, 136, 137, 138, 142, 146, 149, 150, 153, 154, 171, 175, 176, 177, 180, 184, 186, 187, 189, 192, 193, 196, 198], "n_action": [0, 1, 8, 9, 10, 11, 20, 21, 46, 47, 48, 50, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 118, 119, 120, 123, 126, 127, 133, 134, 135, 139, 140, 141, 142, 145, 148, 176, 180, 184, 188, 193, 196, 198], "action_dim": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 110, 111, 112, 115, 116, 117, 123, 124, 125, 130, 131, 132, 136, 137, 138, 142, 146, 149, 176, 180, 184, 193], "3": [0, 1, 10, 11, 94, 95, 110, 111, 112, 113, 114, 150, 164, 171, 172, 173, 174, 175, 179, 182, 184, 188, 189, 192, 193, 198], "action_context": [0, 1, 193], "none": [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 151, 152, 157, 163, 172, 176, 180, 184, 193, 196, 198], "reward_typ": [0, 1, 5, 6, 12, 13, 16, 17, 193, 196], "reward_std": [0, 1, 5, 6, 12, 13, 16, 17, 193, 196], "0": [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 151, 155, 156, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 184, 185, 186, 187, 188, 192, 193, 196, 198, 201], "obs_std": [0, 1, 12, 13, 193, 196], "statetransitionfunct": [0, 1, 5, 193], "simul": [0, 1, 12, 13, 18, 19, 182, 189, 193, 195, 196, 198], "function": [0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 15, 18, 19, 32, 33, 34, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 106, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 148, 150, 172, 173, 174, 176, 177, 178, 182, 184, 186, 188, 189, 193, 196, 198, 200, 201], "rewardfunct": [0, 1, 5, 193], "random_st": [0, 1, 5, 6, 7, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 163, 172, 174, 175, 176, 178, 179, 184, 188, 193, 196, 198, 200, 201], "sourc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 182, 183, 189, 196, 198], "agent": [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 176, 180, 181, 193, 196, 198], "interact": [0, 1, 12, 13, 18, 19, 20, 21, 142, 147, 171, 172, 176, 180, 184, 186, 188, 193, 195, 196, 198], "base": [0, 1, 5, 6, 7, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 28, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 166, 171, 172, 173, 178, 179, 180, 182, 184, 186, 187, 188, 189, 192, 193, 196, 198], "gym": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 150, 151, 152, 155, 156, 171, 173, 174, 175, 178, 179, 181, 182, 184, 188, 189, 192, 193, 196, 198], "import": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 171, 173, 176, 178, 179, 180, 181, 182, 184, 188, 189, 193, 196, 198], "syntheticgym": [0, 1], "work": [0, 1, 10, 11, 12, 13, 18, 19, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 113, 114, 171, 172, 178, 181, 182, 183, 184, 189, 192, 193, 196, 198, 201], "openai": [0, 1, 12, 13, 18, 19, 20, 21, 171, 182, 184, 188, 189, 193, 196, 198], "gymnasium": [0, 1, 12, 13, 18, 19, 20, 21, 171, 181, 182, 184, 188, 193, 196, 198], "like": [0, 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 97, 105, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 160, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 182, 184, 188, 192], "interfac": [0, 1, 12, 13, 18, 19, 110, 111, 112, 181, 182, 188, 193, 196, 198], "see": [0, 1, 12, 13, 18, 19, 115, 116, 117, 118, 119, 120, 172, 174, 188, 192, 201], "exampl": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 171, 172, 182, 184, 186, 187, 188, 192, 193, 196, 198, 200, 201], "below": [0, 1, 12, 13, 18, 19, 113, 114, 171, 172, 182, 183, 186, 192, 193, 196, 198, 201], "usag": [0, 1, 12, 13, 18, 19, 173, 174, 175, 176, 178, 179, 180, 193, 196, 198], "markov": [0, 1, 12, 13, 18, 19, 20, 21, 52, 53, 77, 78, 110, 111, 186, 187, 189, 193, 196, 198], "decis": [0, 1, 12, 13, 18, 19, 20, 21, 38, 40, 41, 42, 43, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 110, 111, 112, 171, 174, 182, 186, 187, 189, 193, 196, 198], "process": [0, 1, 12, 13, 18, 19, 20, 21, 52, 53, 77, 78, 110, 111, 150, 151, 152, 171, 186, 187, 189, 193, 196, 198], "cmdp": [0, 1, 18, 19, 20, 21, 198], "definit": [0, 1, 12, 13, 18, 19, 20, 21, 198], "ar": [0, 1, 12, 13, 18, 19, 20, 21, 28, 31, 32, 33, 38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 142, 143, 144, 145, 146, 147, 148, 149, 171, 172, 173, 174, 175, 176, 178, 179, 180, 182, 184, 186, 187, 188, 192, 193, 196, 198, 200, 201], "given": [0, 1, 12, 13, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 31, 32, 33, 38, 39, 46, 47, 48, 49, 50, 51, 52, 58, 63, 64, 71, 72, 73, 74, 75, 76, 77, 83, 88, 89, 94, 95, 96, 97, 98, 99, 100, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 142, 143, 145, 146, 148, 149, 150, 160, 162, 167, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 186, 187, 188, 192, 193, 196, 198], "follow": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 164, 165, 166, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 192, 193, 196, 198, 200], "timestep": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 186, 193, 196, 198], "int": [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 153, 154, 157, 163, 172, 176, 180, 193, 196, 198], "state": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 163, 171, 172, 174, 175, 176, 180, 184, 186, 187, 188, 193, 196, 198], "arrai": [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 97, 105, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 151, 157, 160, 162, 163, 164, 165, 167, 168, 169, 170, 175], "shape": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 104, 105, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 160, 162, 167, 168, 169, 170, 180, 198], "action": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 153, 154, 171, 175, 176, 177, 180, 181, 184, 186, 187, 188, 189, 193, 196, 198], "float": [0, 1, 2, 3, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 148, 150, 157, 160, 162, 163, 164, 165, 166, 167, 169, 170, 176, 180, 193, 196, 198], "reward": [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 171, 172, 173, 175, 176, 178, 180, 181, 182, 184, 186, 187, 188, 193, 196, 198], "bool": [0, 1, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 125, 127, 128, 150, 151, 152, 153, 154], "discount_r": [0, 1, 18, 19, 20, 21], "paramet": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 157, 158, 159, 162, 163, 164, 165, 166, 168, 169, 170, 172, 174, 186, 198], "default": [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 153, 154, 157, 160, 162, 163, 164, 165, 166, 167, 169, 170, 172, 173, 174, 175, 179], "number": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 126, 127, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 148, 150, 153, 163, 172, 175, 176, 178, 180, 193, 196, 198], "an": [0, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 171, 172, 173, 174, 175, 176, 178, 179, 180, 182, 184, 186, 188, 189, 192, 193, 196, 198], "episod": [0, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 176, 180, 184, 186, 193, 196, 198], "dimens": [0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 28, 29, 30, 31, 32, 33, 34, 36, 123, 124, 125, 126, 127, 128, 129, 150, 157, 172, 174, 176, 180, 193, 196, 198], "discret": [0, 1, 8, 9, 10, 11, 20, 21, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 113, 114, 118, 119, 120, 123, 126, 127, 128, 129, 133, 134, 135, 139, 140, 141, 142, 145, 148, 150, 153, 154, 171, 172, 174, 175, 176, 178, 180, 184, 186, 187, 188, 192, 193, 196, 198], "type": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 151, 152, 153, 154, 157, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 176, 180, 182, 193, 196, 198], "space": [0, 1, 8, 9, 10, 11, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 148, 149, 150, 153, 154, 177, 182, 186, 187, 193, 196, 198], "context": [0, 1, 5, 6, 7, 26, 27, 28, 29, 30, 32, 33, 193], "case": [0, 1, 28, 29, 30, 32, 33, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 71, 77, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 172, 173, 174, 175, 176, 178, 179, 180, 184, 192, 193, 198], "featur": [0, 1, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 171, 182, 187, 188, 193, 196, 198, 201], "vector": [0, 1, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 142, 149, 150, 151, 193, 196, 198], "character": [0, 1, 12, 13, 14, 15, 16, 17, 18, 19, 32, 33, 193, 196, 198], "each": [0, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 38, 41, 63, 66, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 150, 151, 152, 153, 171, 172, 173, 174, 176, 178, 180, 184, 186, 187, 192, 193, 196, 198], "applic": [0, 1, 5, 6, 10, 11, 12, 13, 16, 17, 94, 95, 110, 111, 112, 113, 114, 142, 145, 146, 148, 149, 171, 172, 179, 180, 186, 188, 193, 196, 198, 200], "onli": [0, 1, 5, 6, 10, 11, 12, 13, 16, 17, 20, 21, 26, 27, 94, 95, 110, 111, 112, 113, 114, 142, 147, 171, 172, 174, 175, 179, 182, 186, 188, 193, 196, 198, 200], "when": [0, 1, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 32, 33, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 70, 71, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 142, 146, 172, 173, 174, 175, 178, 179, 180, 184, 186, 187, 192, 193, 196, 198, 200], "i": [0, 1, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 153, 154, 164, 165, 166, 171, 172, 173, 174, 176, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 192, 193, 196, 198, 200], "binari": [0, 1, 5, 6, 12, 13, 14, 15, 16, 17, 22, 24, 32, 33, 193, 196], "nois": [0, 1, 5, 6, 12, 13, 16, 17, 193, 196], "level": [0, 1, 5, 6, 12, 13, 16, 17, 18, 19, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 100, 101, 107, 108, 109, 110, 111, 112, 113, 114, 150, 163, 164, 165, 166, 172, 174, 193, 198], "observ": [0, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 152, 171, 172, 174, 175, 176, 180, 184, 186, 187, 188, 192, 193, 196, 198, 200], "basestatetransitionfunct": [0, 1, 2, 5, 7, 193], "transit": [0, 1, 2, 4, 5, 7, 14, 15, 16, 17, 150, 151, 152, 172, 186, 187, 193, 196, 198], "both": [0, 1, 12, 13, 18, 19, 20, 21, 28, 29, 30, 32, 33, 110, 111, 112, 115, 116, 117, 118, 119, 120, 123, 128, 129, 171, 172, 178, 182, 184, 192], "instanc": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 32, 33, 94, 95, 113, 114, 142, 147, 172, 173, 174, 175, 178, 184, 192, 198], "accept": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 32, 33, 94, 95, 150, 153, 154, 184], "baserewardfunct": [0, 1, 2, 5, 6, 193], "expect": [0, 1, 2, 3, 5, 6, 18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 34, 36, 88, 90, 91, 92, 93, 110, 111, 112, 113, 114, 123, 126, 150, 157, 164, 165, 171, 172, 182, 186, 187, 188, 193, 196, 198, 200], "immedi": [0, 1, 2, 3, 5, 6, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 117, 118, 120, 136, 138, 139, 141, 172, 186, 187, 193, 196, 198], "random": [0, 1, 5, 6, 7, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 163, 172, 178, 186, 187, 188, 192, 193, 196, 198], "setup": [0, 1, 12, 13, 18, 19, 20, 21, 172, 183, 186, 187, 188, 189], "necessari": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 193, 196, 198], "modul": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 123, 124, 125, 126, 127, 128, 129, 171, 172, 182, 184, 188], "from": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 28, 29, 30, 32, 33, 34, 35, 38, 40, 41, 45, 46, 50, 51, 52, 58, 63, 65, 66, 70, 71, 75, 76, 77, 83, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 142, 146, 149, 150, 151, 152, 171, 172, 173, 174, 175, 176, 178, 181, 182, 183, 184, 186, 187, 188, 189, 192, 193, 196, 198, 200], "syntheticenv": [0, 1], "scope_rl": [0, 1, 12, 13, 18, 19, 172, 173, 174, 175, 176, 178, 179, 181, 184, 188, 193, 198], "polici": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 153, 154, 171, 176, 180, 183, 189, 193, 196, 198, 200, 201], "onlinehead": [0, 1, 12, 13, 18, 19, 20, 21, 142, 182, 193, 198], "op": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 150, 153, 180, 184, 186, 187, 189, 193, 196, 198, 200, 201], "onlin": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 142, 143, 147, 171, 187, 188, 189, 192, 195], "calc_on_policy_policy_valu": [0, 1, 12, 13, 18, 19, 20, 21, 96, 184], "other": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 94, 95, 110, 111, 112, 113, 114, 171, 172, 174, 176, 180, 182, 184, 186, 187, 192, 193, 196, 198, 200], "librari": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 77, 83, 94, 95, 110, 111, 112, 113, 114, 171, 182, 189, 193, 196, 198], "d3rlpy": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 83, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 171, 172, 177, 181, 182, 184, 188, 189, 193, 196, 198, 201], "algo": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 142, 143, 144, 145, 146, 147, 148, 149, 184, 188, 193, 196, 198], "randompolici": [0, 1, 18, 19, 193, 196, 198], "preprocess": [0, 1, 18, 19, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 151, 152, 172, 182, 198], "minmaxactionscal": [0, 1, 18, 19, 150, 172, 174, 175, 193, 198], "initi": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 38, 39, 46, 47, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 172, 173, 174, 175, 176, 178, 179, 180, 184, 186, 187, 188, 196, 198, 200, 201], "12345": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 28, 31, 32, 33, 38, 40, 41, 42, 43, 44, 45, 94, 95, 110, 111, 112, 113, 114, 179, 188, 193, 196, 198], "command": [0, 1, 10, 11, 12, 13, 18, 19, 94, 95, 172, 173, 184], "also": [0, 1, 10, 11, 12, 13, 18, 19, 63, 67, 68, 69, 71, 73, 74, 88, 89, 90, 91, 92, 93, 110, 111, 113, 114, 142, 143, 144, 145, 146, 147, 148, 149, 171, 172, 173, 174, 175, 177, 178, 179, 184, 186, 187, 188, 192, 198, 200], "make": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 142, 147, 171, 172, 174, 175, 181, 184, 188, 192, 193, 196, 198], "v0": [0, 1, 10, 11, 12, 13, 18, 19, 94, 95, 110, 111, 112, 113, 114, 185, 188, 193, 196, 198], "defin": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 112, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 176, 184, 186, 188, 192, 193, 196, 198], "e": [0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 148, 150, 164, 165, 166, 171, 172, 174, 176, 178, 179, 180, 182, 184, 186, 187, 188, 192, 193, 195, 196, 198], "action_scal": [0, 1, 18, 19, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 110, 111, 112, 115, 116, 117, 130, 131, 132, 136, 137, 138, 172, 174, 175, 193, 198], "minimum": [0, 1, 10, 11, 18, 19, 20, 21, 28, 31, 32, 33, 34, 36, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 142, 149, 150, 151, 152, 157, 172, 174, 175, 193, 198], "1": [0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 148, 150, 151, 157, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 180, 184, 185, 186, 187, 188, 189, 192, 193, 196, 198, 200], "maximum": [0, 1, 8, 9, 10, 11, 18, 19, 20, 21, 34, 36, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 142, 149, 150, 151, 152, 157, 172, 174, 175, 186, 193, 198], "name": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 142, 144, 145, 146, 147, 148, 149, 150, 153, 154, 157, 172, 174, 175, 176, 178, 180, 184, 188, 193, 196, 200], "build_with_env": [0, 1, 12, 13, 18, 19, 20, 21, 193], "rang": [0, 1, 12, 13, 18, 19, 20, 21, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 96, 99, 102, 107, 110, 111, 113, 114, 150, 151, 171, 176, 182, 187, 200], "1000": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 188], "ob": [0, 1, 12, 13, 18, 19, 20, 21, 181, 193, 196, 198], "info": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 174, 175, 176, 180, 181, 184, 193, 196, 198], "reset": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 150, 155, 156, 181, 193, 196, 198], "done": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 171, 174, 176, 180, 181, 184, 188, 193, 196, 198, 200], "fals": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 125, 127, 128, 142, 143, 150, 151, 152, 153, 154, 174, 175, 176, 181, 193, 196, 198], "while": [0, 1, 12, 13, 18, 19, 20, 21, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 66, 67, 68, 69, 71, 73, 74, 171, 172, 180, 181, 184, 186, 188, 192, 193, 196, 198], "predict_onlin": [0, 1, 18, 19, 20, 21, 142, 143, 184, 193, 196, 198], "truncat": [0, 1, 12, 13, 18, 19, 20, 21, 142, 149, 181, 182, 193, 196, 198], "step": [0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 38, 39, 40, 41, 42, 43, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 110, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 155, 156, 172, 174, 175, 176, 178, 180, 181, 182, 188, 193, 196, 198], "evalu": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 150, 153, 173, 176, 179, 180, 183, 186, 189, 193, 196, 198, 200, 201], "calcul": [0, 1, 12, 13, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 102, 103, 110, 111, 113, 114, 142, 143, 145, 146, 148, 149, 174, 175, 178, 184, 186, 198], "valu": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 148, 149, 150, 151, 152, 157, 163, 164, 165, 166, 171, 172, 173, 176, 178, 179, 180, 186, 188, 189, 192, 193, 198, 200, 201], "on_policy_perform": [0, 1, 12, 13, 18, 19, 20, 21], "n_trajectori": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 174, 175, 176, 178, 180, 184, 188], "100": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 28, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 123, 124, 125, 126, 127, 128, 129, 150, 163, 172, 174, 175, 176, 178, 184, 188, 196], "output": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 150, 155, 156, 184], "27": [0, 1, 171, 189, 192], "59": [0, 1, 189], "refer": [0, 1, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 171, 172, 173, 174, 175, 177, 178, 179, 184, 186, 187, 188, 192, 193, 196, 198, 200], "greg": [0, 1, 12, 13, 18, 19, 20, 21, 189], "brockman": [0, 1, 12, 13, 18, 19, 20, 21, 189], "vicki": [0, 1, 12, 13, 18, 19, 20, 21, 189], "cheung": [0, 1, 12, 13, 18, 19, 20, 21, 189], "ludwig": [0, 1, 12, 13, 18, 19, 20, 21, 189], "pettersson": [0, 1, 12, 13, 18, 19, 20, 21, 189], "jona": [0, 1, 12, 13, 18, 19, 20, 21, 189], "schneider": [0, 1, 12, 13, 18, 19, 20, 21, 189], "john": [0, 1, 12, 13, 18, 19, 20, 21, 77, 83, 189], "schulman": [0, 1, 12, 13, 18, 19, 20, 21, 189], "jie": [0, 1, 12, 13, 18, 19, 20, 21, 189], "tang": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 113, 114, 189], "wojciech": [0, 1, 12, 13, 18, 19, 20, 21, 189], "zaremba": [0, 1, 12, 13, 18, 19, 20, 21, 189], "2016": [0, 1, 12, 13, 18, 19, 20, 21, 38, 40, 42, 52, 54, 56, 59, 61, 63, 65, 67, 77, 79, 81, 84, 86, 189], "method": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 171, 174, 175, 176, 179, 180, 186, 188, 192, 200], "The": [0, 1, 12, 13, 14, 15, 16, 17, 18, 19, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 148, 150, 163, 164, 165, 166, 171, 172, 173, 174, 175, 178, 180, 182, 184, 188, 189, 192, 193, 196, 198], "procedur": [0, 1, 12, 13, 18, 19, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 100, 101, 108, 110, 112, 113, 114, 150, 163, 171, 172, 174, 175, 182, 184, 188, 193, 196, 198], "sampl": [0, 1, 2, 3, 6, 10, 11, 12, 13, 18, 19, 22, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 38, 41, 43, 44, 45, 46, 49, 51, 52, 53, 55, 57, 58, 60, 62, 63, 66, 68, 69, 70, 71, 74, 76, 77, 78, 80, 82, 83, 85, 87, 88, 90, 94, 95, 96, 100, 142, 143, 145, 146, 148, 149, 150, 163, 164, 165, 166, 171, 176, 178, 180, 182, 186, 188, 189, 198], "pair": [0, 1, 8, 9, 10, 11, 22, 24, 32, 33, 38, 39, 52, 58, 63, 64, 77, 83, 94, 95, 115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 142, 148, 176, 180, 186], "updat": [0, 1, 2, 4, 5, 7, 12, 13, 172, 182, 186], "return": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 151, 152, 153, 154, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179, 182, 183, 184, 187, 193, 196, 198, 200, 201], "feedback": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 186, 198], "indic": [0, 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16, 17, 20, 21, 22, 24, 32, 33, 38, 39, 46, 48, 49, 50, 51, 52, 58, 63, 64, 71, 73, 74, 75, 76, 77, 83, 113, 114, 115, 116, 117, 118, 119, 120, 142, 145, 172, 186, 192, 198], "which": [0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 32, 33, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 164, 165, 171, 172, 174, 175, 176, 178, 180, 181, 182, 184, 186, 187, 188, 192, 193, 196, 198, 200], "present": [0, 1, 2, 4, 5, 7, 12, 13, 14, 15, 16, 17, 171, 192, 196], "ndarrai": [0, 1, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 104, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 151, 152, 160, 162, 166, 167, 168, 169, 170, 172, 176, 180, 193, 196, 198], "possibli": [0, 1], "noisi": [0, 1], "whether": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 125, 127, 128, 150, 153, 154, 172, 174, 175, 176, 180, 198], "end": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 172, 176, 180, 182, 201], "For": [0, 1, 8, 9, 10, 11, 12, 13, 113, 114, 171, 172, 173, 174, 175, 177, 178, 179, 180, 182, 184, 186, 187, 192, 193, 196, 198, 200], "api": [0, 1, 8, 9, 12, 13, 16, 17, 28, 29, 30, 31, 32, 33, 94, 95, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 142, 143, 144, 145, 146, 147, 148, 149, 171, 172, 181, 184, 187, 188], "consist": [0, 1, 8, 9, 10, 11, 12, 13, 16, 17, 28, 29, 30, 31, 32, 33, 52, 53, 63, 67, 68, 69, 71, 73, 74, 77, 78, 81, 82, 86, 87, 94, 95, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 142, 144, 147, 171, 172, 184, 186, 187, 188, 198], "empti": [0, 1, 18, 19, 20, 21], "dict": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 99, 101, 102, 110, 111, 112, 113, 114, 150, 151, 152, 153, 158, 161, 163, 164, 165, 166, 172, 174, 175, 176, 177, 178, 184], "addit": [0, 1, 8, 9, 10, 11, 12, 13, 18, 19, 20, 21, 142, 143, 144, 145, 146, 147, 148, 149, 172, 174, 175, 178, 184, 186, 188, 198], "inform": [0, 1, 18, 19, 20, 21, 110, 111, 171, 176, 182, 189, 192], "mai": [0, 1, 12, 13, 18, 19, 20, 21, 38, 40, 46, 50, 63, 65, 71, 75, 115, 116, 117, 118, 119, 120, 172, 181, 186, 200], "us": [0, 1, 5, 6, 7, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 41, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 102, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 126, 127, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 153, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 186, 188, 192, 193, 196, 198, 200, 201], "packag": [0, 1, 12, 13, 18, 19, 20, 21, 172, 184, 188, 189, 193, 196, 198, 200, 201], "user": [0, 1, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 115, 116, 117, 118, 119, 120, 171, 174, 175, 188, 193, 196, 198, 200], "thi": [0, 1, 10, 11, 18, 19, 20, 21, 28, 29, 30, 31, 32, 33, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 166, 171, 172, 173, 174, 175, 177, 178, 179, 181, 182, 184, 186, 187, 189, 192, 196, 198, 200], "unavail": [0, 1, 12, 13, 18, 19, 20, 21, 180], "tupl": [0, 1, 12, 13, 18, 19, 20, 21, 22, 24, 25, 28, 31, 32, 33, 34, 36, 110, 112, 113, 114, 150, 157, 176, 180, 198], "seed": [0, 1, 10, 11, 12, 13, 18, 19, 20, 21, 150, 156, 172], "abstract": [2, 3, 4, 8, 9, 14, 15, 22, 23, 24, 25, 88, 89, 90, 91, 92, 93, 121, 122, 142, 143, 171, 172, 176, 182, 189, 193, 196, 198], "current": [2, 4, 5, 7, 174, 186, 192], "chosen": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 176, 180, 182, 186, 187], "next": [2, 4, 5, 7, 10, 11, 115, 116, 136, 137, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 186, 187, 188, 192, 193, 195, 196, 198, 200, 201], "mean_reward_funct": [2, 3, 5, 6, 193], "condit": [2, 3, 5, 6, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 102, 105, 110, 111, 113, 114, 118, 119, 120, 133, 134, 135, 139, 140, 141, 171, 172, 176, 182, 186, 187, 188, 200], "sample_reward": [2, 3, 6], "mathemat": [5, 16, 28], "synthet": [5, 32, 182, 193], "system": [5, 12, 13, 16, 171, 186, 189, 195, 196], "custom": [5, 6, 7, 16, 17, 20, 21, 28, 29, 30, 31, 32, 33, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 115, 116, 117, 118, 119, 120, 172, 175, 180, 182, 184, 201], "attribut": [5, 6, 7, 10, 11, 16, 17, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 148, 149, 150, 153, 154, 192], "linear": [5, 6], "log": [8, 9, 10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 77, 78, 83, 94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 159, 164, 165, 171, 172, 173, 176, 177, 179, 182, 184, 186, 187, 188, 192, 200], "basedataset": [8, 10, 11, 184], "obtain_episod": [8, 9, 10, 11, 94, 95, 110, 112, 174, 175, 178, 184, 188], "rollout": [8, 9, 10, 11, 18, 19, 20, 21, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 184], "behavior": [8, 9, 10, 11, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 153, 154, 171, 172, 173, 174, 175, 176, 179, 180, 182, 184, 187, 188, 189, 192, 200], "obtain": [8, 9, 10, 11, 26, 27, 88, 89, 94, 95, 110, 111, 113, 114, 172, 173, 174, 175, 176, 177, 178, 180, 184, 198], "10000": [8, 9, 10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 100, 101, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 163, 174, 175, 176, 178, 184, 188], "trajectori": [8, 9, 10, 11, 38, 39, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 171, 175, 176, 180, 182, 186, 187, 188, 198], "gener": [8, 9, 10, 11, 22, 24, 32, 33, 94, 95, 96, 104, 115, 116, 117, 118, 119, 120, 150, 153, 154, 172, 182, 186, 187, 189, 192, 193, 196, 198], "roll": [8, 9, 10, 11], "out": [8, 9, 10, 11, 171, 186, 188, 192], "logged_dataset": [8, 9, 10, 11, 94, 95, 110, 111, 112, 113, 114, 150, 154, 159, 172, 173, 174, 175, 176, 178, 179, 180, 184, 188, 200, 201], "": [8, 9, 10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 165, 171, 172, 173, 174, 175, 176, 178, 179, 182, 184, 186, 187, 188, 189, 192, 193, 196, 198], "multipleloggeddataset": [8, 9, 10, 11, 94, 95, 110, 111, 112, 113, 114, 150, 172, 178, 184], "contain": [8, 9, 10, 11, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 102, 110, 111, 112, 113, 114, 150, 153, 154, 172, 173, 176, 178, 179, 180, 184, 193], "multipl": [8, 9, 10, 11, 20, 21, 94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 171, 172, 173, 174, 175, 179, 180, 182, 184, 188, 200, 201], "should": [8, 9, 18, 19, 20, 21, 26, 27, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 150, 163, 164, 165, 166, 171, 172, 173, 179, 180, 181, 184, 188, 192, 198], "kei": [8, 9, 10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 110, 111, 112, 113, 114, 150, 158, 159, 172, 174, 178, 180, 182, 184, 186, 192, 198], "size": [8, 9, 10, 11, 16, 17, 34, 35, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 164, 165, 166, 172, 176, 180, 184, 193, 198], "step_per_trajectori": [8, 9, 10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 176, 180, 184], "action_kei": [8, 9, 10, 11, 94, 95, 110, 111, 112, 176, 180, 184], "action_mean": [8, 9, 10, 11, 20, 21, 94, 95, 110, 111, 112, 176, 180, 184, 198], "state_kei": [8, 9, 10, 11, 94, 95, 110, 111, 112, 176, 180, 184], "termin": [8, 9, 10, 11, 94, 95, 110, 111, 112, 176, 180, 184, 188], "pscore": [8, 9, 10, 11, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 110, 111, 112, 115, 117, 118, 120, 130, 131, 132, 133, 134, 135, 136, 138, 139, 141, 142, 143, 145, 146, 148, 149, 172, 174, 175, 176, 180, 184], "record": [8, 9, 10, 11, 94, 95, 113, 114, 171, 174, 175, 179], "str": [8, 9, 10, 11, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 126, 127, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 157, 176, 180, 193, 196], "either": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 77, 79, 81, 84, 86, 88, 90, 94, 95, 150, 153, 154, 171, 172, 173, 174, 175, 176, 178, 179, 180, 184, 186, 187, 196, 198, 200], "If": [8, 9, 10, 11, 18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 150, 153, 154, 172, 182, 183, 192, 193, 196, 198, 201], "list": [8, 9, 10, 11, 94, 95, 96, 108, 109, 110, 111, 112, 113, 114, 150, 151, 152, 171, 172, 173, 174, 175, 178, 179, 184, 186, 187, 193, 196, 198], "dictionari": [8, 9, 10, 11, 20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 101, 102, 110, 111, 112, 113, 114, 150, 153, 163, 164, 165, 166, 172, 173, 178, 179], "map": [8, 9, 10, 11, 20, 21, 22, 24, 32, 33, 176, 180, 198], "index": [8, 9, 10, 11, 20, 21, 22, 24, 32, 33, 113, 114, 176, 180, 196, 198], "specif": [8, 9, 10, 11, 20, 21, 171, 172, 175, 178, 181, 184, 186, 188, 192, 193, 196, 200], "under": [8, 9, 10, 11, 16, 17, 38, 39, 46, 47, 48, 49, 50, 51, 52, 58, 63, 64, 71, 72, 73, 74, 75, 76, 77, 83, 88, 89, 96, 97, 98, 99, 102, 104, 105, 106, 107, 110, 111, 113, 114, 115, 117, 118, 120, 130, 131, 132, 133, 134, 135, 136, 138, 139, 141, 142, 145, 146, 148, 149, 171, 172, 184, 186, 189, 192, 198], "reach": [8, 9, 10, 11], "pre": [8, 9, 10, 11, 26, 27, 34, 35, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 198], "propens": [8, 9, 10, 11, 115, 117, 118, 120, 130, 131, 132, 133, 134, 135, 136, 138, 139, 141, 142, 145, 146, 148, 149, 184], "being": [8, 9, 10, 11, 115, 117, 118, 120, 130, 131, 132, 133, 134, 135, 136, 138, 139, 141, 142, 145, 146, 148, 149, 172, 186], "stand": [8, 9, 10, 11, 115, 117, 118, 120, 130, 131, 132, 133, 134, 135, 136, 138, 139, 141, 142, 145, 146, 148, 149], "score": [8, 9, 10, 11, 115, 117, 118, 120, 130, 131, 132, 133, 134, 135, 136, 138, 139, 141, 142, 145, 146, 148, 149, 184, 192], "loggeddataset": [8, 9, 10, 11, 94, 95, 110, 111, 112, 150, 154, 159], "obtain_step": [8, 9, 10, 11, 184], "handl": [10, 110, 113, 171, 172, 175, 179, 182, 184, 188, 201], "syntheticdataset": [10, 94, 95, 110, 111, 112, 113, 114, 174, 175, 178, 182, 184, 188], "env": [10, 11, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 150, 151, 152, 155, 156, 172, 173, 174, 175, 178, 179, 181, 182, 184, 188, 193, 196, 198], "max_episode_step": [10, 11, 94, 95, 110, 112, 113, 114, 174, 175, 178, 184, 188], "info_kei": [10, 11], "data": [10, 11, 38, 40, 42, 52, 53, 54, 56, 59, 61, 63, 65, 67, 77, 78, 79, 81, 84, 86, 94, 95, 110, 111, 112, 113, 114, 150, 164, 165, 166, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 184, 186, 189, 192], "directli": [10, 11, 184, 186], "off": [10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 173, 183, 184, 186, 189, 192, 193, 196, 198, 201], "moreov": [10, 11, 52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 171, 172, 182, 184, 186, 188, 200], "compat": [10, 11, 142, 143, 144, 145, 146, 147, 148, 149, 171, 180, 181, 182, 184, 186, 193, 196, 198], "offlin": [10, 11, 77, 83, 113, 114, 115, 116, 117, 118, 119, 120, 172, 173, 174, 175, 178, 179, 187, 189, 192, 193, 196, 198, 201], "d3rlpy_dataset": [10, 11], "mdpdataset": [10, 11, 150, 151, 152, 184, 188], "episode_termin": [10, 11, 184, 188], "discrete_act": [10, 11, 184, 188], "extern": [10, 11, 94, 95, 142, 143, 144, 145, 146, 147, 148, 149, 172, 188], "document": [10, 11, 94, 95, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 171, 172, 182, 186, 188, 189, 191, 193, 194, 195, 196, 197, 198, 199, 201], "about": [10, 11, 94, 95, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 172, 178, 182, 188, 192, 193, 196, 198], "compon": [10, 11, 18, 19, 20, 21, 94, 95, 110, 111, 112, 113, 114, 193], "prepar": [10, 11, 94, 95, 110, 111, 112, 113, 114, 173, 174, 175, 178, 179, 184], "scope": [10, 11, 94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 172, 173, 174, 175, 176, 178, 179, 180, 183, 185, 188, 189, 193, 195, 196, 198, 200, 201], "epsilongreedyhead": [10, 11, 94, 95, 110, 111, 112, 113, 114, 142, 184, 188, 196], "rtbgym": [10, 11, 94, 95, 110, 111, 112, 113, 114, 182, 188, 189, 195], "doubledqn": [10, 11, 94, 95, 110, 111, 112, 113, 114, 188], "buffer": [10, 11, 94, 95, 110, 111, 112, 113, 114, 186, 188], "replaybuff": [10, 11, 94, 95, 110, 111, 112, 113, 114, 188], "explor": [10, 11, 94, 95, 110, 111, 112, 113, 114, 142, 145, 184, 186, 188, 189, 201], "constantepsilongreedi": [10, 11, 94, 95, 110, 111, 112, 113, 114, 188], "rtbenv": [10, 11, 18, 20, 21, 26, 27, 94, 95, 110, 111, 112, 113, 114, 188], "train": [10, 11, 26, 27, 94, 95, 110, 111, 112, 113, 114, 118, 119, 172, 181, 182, 186, 188], "ddqn": [10, 11, 94, 95, 110, 111, 112, 113, 114, 188], "maxlen": [10, 11, 94, 95, 110, 111, 112, 113, 114, 188], "epsilon": [10, 11, 94, 95, 110, 111, 112, 113, 114, 142, 145, 182, 184, 186, 188, 196], "fit_onlin": [10, 11, 94, 95, 110, 111, 112, 113, 114, 188], "n_step": [10, 11, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 184, 188], "n_steps_per_epoch": [10, 11, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 188], "convert": [10, 11, 94, 95, 110, 111, 112, 113, 114, 142, 143, 145, 148, 150, 155, 156, 184, 188], "stochast": [10, 11, 18, 19, 22, 23, 24, 25, 28, 29, 30, 32, 33, 94, 95, 110, 111, 112, 113, 114, 142, 143, 145, 146, 148, 149, 172, 182, 184, 188, 198], "collect": [10, 11, 18, 19, 94, 95, 110, 111, 112, 113, 114, 171, 172, 173, 174, 175, 177, 178, 179, 184, 186, 188, 200], "behavior_polici": [10, 11, 94, 95, 110, 111, 112, 113, 114, 172, 173, 174, 175, 176, 178, 179, 180, 184, 188], "action_spac": [10, 11, 94, 95, 110, 111, 112, 113, 114, 142, 146, 172, 174, 175, 184, 188], "n": [10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 150, 164, 165, 166, 172, 176, 184, 186, 187, 188], "ddqn_epsilon_0": [10, 11, 94, 95, 110, 111, 112, 113, 114, 188, 200], "obs_kei": [10, 11, 20, 21], "search_volum": [10, 11, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33], "impress": [10, 11, 18, 19, 20, 21, 22, 23, 24, 25, 28, 29, 31, 32, 33, 198], "click": [10, 11, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 32, 33, 171, 198], "convers": [10, 11, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 30, 32, 33, 198], "average_bid_pric": [10, 11], "obtain_trajectori": [10, 11, 110, 111, 113, 114], "obtain_info": [10, 11, 174, 175], "true": [10, 11, 12, 13, 14, 15, 16, 17, 22, 23, 26, 27, 28, 29, 30, 32, 33, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 153, 154, 172, 174, 175, 179, 184, 186, 188, 192, 196, 198, 200, 201], "700": [10, 11], "7": [10, 11, 18, 19, 20, 21, 32, 33, 110, 111, 113, 114, 150, 164, 171, 172, 182, 184, 188, 189], "16681005": [10, 11], "27825594": [10, 11], "46415888": [10, 11], "77426368": [10, 11], "29154967": [10, 11], "2": [10, 11, 18, 19, 28, 31, 32, 33, 38, 39, 40, 42, 43, 44, 46, 50, 52, 53, 54, 56, 59, 61, 63, 65, 67, 68, 69, 71, 75, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 151, 160, 162, 164, 165, 167, 168, 169, 170, 171, 172, 176, 180, 181, 184, 186, 187, 188, 189, 192, 193, 196, 198], "15443469": [10, 11], "59381366": [10, 11], "9948425": [10, 11], "remaining_budget": [10, 11], "budget_consumption_r": [10, 11], "cost_per_mille_of_impress": [10, 11], "winning_r": [10, 11], "adjust_r": [10, 11, 26, 27], "00000000e": [10, 11], "00": [10, 11], "03": [10, 11], "9": [10, 11, 113, 114, 171, 172, 188, 189, 192, 201], "29616093e": [10, 11], "01": [10, 11], "83918812e": [10, 11], "4": [10, 11, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 172, 173, 174, 175, 178, 182, 188, 189, 192, 201], "71334329e": [10, 11], "91000000e": [10, 11], "63333333e": [10, 11], "6": [10, 11, 110, 112, 171, 172, 178, 182, 185, 188, 189, 192], "66810054e": [10, 11], "54000000e": [10, 11], "02": [10, 11], "40904716e": [10, 11], "59381366e": [10, 11], "10000000e": [10, 11], "36058700e": [10, 11], "90049751e": [10, 11], "201": [10, 11], "205": [10, 11], "217": [10, 11], "191": [10, 11], "186": [10, 11, 189], "199": [10, 11], "8": [10, 11, 171, 172, 182, 188, 189, 192], "21": [10, 11, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 105, 110, 111, 113, 114, 171, 172, 175, 176, 182, 189], "24": [10, 11, 171, 172, 182, 189, 192, 198], "18": [10, 11, 110, 111, 112, 113, 114, 171, 172, 182, 189], "544": [10, 11], "55223881": [10, 11], "24390244": [10, 11], "523": [10, 11], "24423963": [10, 11], "172": [10, 11], "58706468": [10, 11], "2565445": [10, 11], "458": [10, 11], "76344086": [10, 11], "73": [10, 11], "dataset_id": [10, 11, 94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 172, 176, 178, 180, 184], "quickstart": [10, 11, 94, 95, 110, 111, 112, 113, 114, 171, 172, 177, 182, 183, 184, 186, 187, 192, 200], "relat": [10, 11, 110, 111, 112, 113, 114, 172, 182, 187, 188, 200], "tutori": [10, 11, 110, 111, 112, 113, 114, 189], "n_dataset": [10, 11, 150, 153, 154, 178, 184], "record_unclipped_act": [10, 11], "path": [10, 11, 94, 95, 96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 153, 154, 172, 178, 184], "save_relative_path": [10, 11, 94, 95, 150, 153, 154], "call": [10, 11, 26, 27, 94, 95, 110, 112, 113, 114, 172, 178, 192, 200], "save": [10, 11, 94, 95, 96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 153, 154, 172, 174, 184], "intend": [10, 11, 26, 27, 94, 95, 171, 182, 193, 196, 198], "ha": [10, 11, 20, 21, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 52, 58, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 83, 142, 143, 144, 145, 146, 147, 148, 149, 171, 172, 175, 178, 182, 184, 192, 196], "fix": [10, 11, 176, 180, 181], "length": [10, 11, 28, 29, 30, 31, 32, 33, 38, 41, 45, 46, 51, 63, 66, 70, 71, 76, 94, 95, 172, 176, 180], "set": [10, 11, 18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 150, 153, 154, 171, 172, 174, 175, 178, 182, 184, 186, 187, 188, 189, 192], "non": [10, 11, 171, 172, 184, 192], "stationari": [10, 11, 52, 58, 77, 83, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 115, 116, 117, 118, 119, 120, 172, 184, 189], "cartpol": [10, 11, 172, 184, 192], "taxi": [10, 11, 172, 184], "liu": [10, 11, 46, 47, 48, 49, 50, 51, 52, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 84, 85, 86, 87, 110, 111, 115, 116, 117, 118, 119, 120, 189], "et": [10, 11, 38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 115, 116, 117, 118, 119, 120], "al": [10, 11, 38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 115, 116, 117, 118, 119, 120], "2018": [10, 11, 12, 13, 18, 19, 20, 21, 26, 27, 32, 33, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 189], "uehara": [10, 11, 38, 39, 42, 43, 44, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 189], "2020": [10, 11, 38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 189], "pleas": [10, 11, 18, 19, 94, 95, 110, 111, 113, 114, 142, 143, 144, 145, 146, 147, 148, 149, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 188, 192, 193, 196, 198, 201], "consid": [10, 11, 52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 172, 184, 186, 187, 188, 192], "masatoshi": [10, 11, 38, 39, 42, 43, 44, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 189], "jiawei": [10, 11, 38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 189], "huang": [10, 11, 28, 31, 32, 33, 38, 39, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 189], "nan": [10, 11, 38, 39, 40, 42, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 189], "jiang": [10, 11, 38, 39, 40, 42, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 189], "minimax": [10, 11, 38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 172, 182, 189], "weight": [10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 110, 112, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 172, 176, 180, 186, 189], "q": [10, 11, 38, 39, 40, 42, 46, 47, 48, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 71, 72, 73, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 115, 116, 117, 118, 119, 120, 123, 124, 126, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 148, 171, 172, 181, 182, 184, 187, 189], "qiang": [10, 11, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 189], "lihong": [10, 11, 38, 40, 42, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 189], "li": [10, 11, 38, 40, 42, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 189], "ziyang": [10, 11, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 189], "dengyong": [10, 11, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 189], "zhou": [10, 11, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 84, 85, 86, 87, 189], "break": [10, 11, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 189], "curs": [10, 11, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 189], "horizon": [10, 11, 38, 45, 46, 51, 52, 59, 60, 61, 62, 63, 70, 71, 76, 77, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 171, 172, 189], "infinit": [10, 11, 52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 94, 95, 189], "estim": [10, 11, 28, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 101, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 151, 152, 163, 164, 165, 166, 179, 180, 184, 188, 189, 192, 200, 201], "basehead": [10, 11, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 142, 144, 145, 146, 147, 148, 149, 173, 174, 175, 178, 179, 184], "independ": [10, 11, 172], "more": [10, 11, 171, 172, 176, 182, 186, 187, 188, 192, 193, 196, 198, 200, 201], "than": [10, 11, 38, 40, 41, 46, 50, 63, 65, 66, 71, 75, 115, 117, 118, 120, 142, 145, 146, 148, 149, 171, 172, 176, 182, 184, 186, 187, 188, 192, 193, 196, 198, 200, 201], "instead": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 38, 41, 63, 66, 94, 95, 142, 146, 171, 172, 178, 184, 186, 192, 196], "gain": [10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 28, 29, 30, 32, 33, 171, 182, 196], "unclip": [10, 11], "directori": [10, 11, 94, 95, 150, 153, 154, 174], "absolut": [10, 11, 94, 95, 150, 153, 154, 184], "rel": [10, 11, 94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 173, 174, 175, 184], "note": [10, 11, 18, 19, 20, 21, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 113, 114, 150, 153, 154, 171, 172, 173, 174, 175, 176, 178, 179, 180, 182, 184, 185, 186, 187, 188, 192, 193, 196, 198, 200], "option": [10, 11, 18, 19, 20, 21, 26, 27, 94, 95, 150, 153, 154, 172, 173, 174, 175, 176, 178, 179, 180, 184, 192, 193, 196, 198], "wa": [10, 11, 94, 95, 150, 153, 154], "ad": [10, 11, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 94, 95, 150, 153, 154, 172, 186, 198], "order": [10, 11, 94, 95, 150, 153, 154, 192], "run": [10, 11, 94, 95, 150, 153, 154, 173], "properli": [10, 11, 94, 95, 150, 153, 154], "otherwis": [10, 11, 94, 95, 142, 146, 150, 153, 154, 172], "recommend": [10, 11, 12, 13, 14, 15, 16, 17, 20, 21, 94, 95, 150, 153, 154, 171, 182, 186, 189, 195, 196, 198, 201], "access": [10, 11, 94, 95, 172, 174, 178, 184, 186], "logged_dataset_0": [10, 11], "get": [10, 11, 94, 95, 110, 111, 112, 113, 114, 150, 151, 152, 153, 154, 172, 175, 178, 184, 201], "id": [10, 11, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 172, 176, 178, 180, 198], "minimum_rollout_length": [10, 11, 94, 95], "maximum_rollout_length": [10, 11, 94, 95], "obtain_trajectories_from_single_interact": [10, 11], "distribut": [10, 11, 18, 19, 28, 31, 32, 33, 34, 35, 38, 40, 41, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 133, 134, 135, 139, 140, 141, 142, 146, 149, 150, 166, 173, 180, 184, 186, 189, 192, 193, 196, 198, 200, 201], "standard": [10, 11, 18, 19, 20, 21, 28, 31, 32, 33, 34, 35, 110, 113, 114, 142, 146, 149, 150, 166, 171, 172, 174, 182, 186, 192, 200, 201], "multiplloggededataset": [10, 11], "befor": [10, 11, 94, 95, 172, 186, 187], "argument": [10, 11, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 172, 174, 175, 193, 196, 198], "irrelev": [10, 11, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109], "finit": [10, 11, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109], "whole": [10, 11, 171, 178, 182], "singl": [10, 11, 18, 19, 20, 21, 110, 112, 113, 114, 171, 172, 173, 174, 175, 178, 179, 184], "last": [10, 11, 172, 192], "after": [10, 11, 174, 175, 186, 187, 188, 193, 196, 198], "seed_env": [10, 11], "By": [10, 11, 38, 41, 63, 66, 171, 172, 179, 186, 192], "we": [10, 11, 18, 19, 20, 21, 28, 29, 30, 32, 33, 52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 186, 187, 188, 192, 193, 196, 198, 200], "can": [10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 115, 116, 117, 118, 119, 120, 171, 172, 173, 174, 175, 176, 178, 179, 182, 183, 184, 186, 187, 188, 192, 193, 196, 198, 200], "recenv": 12, "n_item": [12, 13, 14, 15, 16, 17, 196], "n_user": [12, 13, 18, 19, 28, 29, 30, 31, 32, 33, 196, 198], "item_feature_dim": [12, 13, 14, 15, 16, 17, 196], "user_feature_dim": [12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 196, 198], "item_feature_vector": [12, 13, 14, 15, 16, 17, 196], "user_feature_vector": [12, 13, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 196, 198], "usermodel": [12, 13, 16, 193, 196], "partial": [12, 13, 77, 83, 189, 193, 196], "po": [12, 13, 193, 196], "mdp": [12, 13, 172, 186, 187, 193, 196], "A": [12, 13, 14, 15, 16, 17, 20, 21, 26, 27, 28, 31, 32, 33, 46, 47, 48, 50, 51, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 118, 119, 120, 133, 134, 135, 139, 140, 141, 142, 145, 146, 148, 149, 171, 172, 182, 183, 184, 186, 187, 188, 189, 192, 193, 196, 198, 201], "repres": [12, 13, 14, 15, 16, 17, 172, 187, 196], "prefer": [12, 13, 14, 15, 16, 17, 189, 192, 196], "chang": [12, 13, 14, 15, 16, 17, 172, 186, 192, 196, 200], "over": [12, 13, 14, 15, 16, 17, 20, 21, 38, 45, 46, 51, 63, 70, 71, 76, 172, 173, 182, 186, 192, 196], "time": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 32, 33, 38, 41, 63, 66, 171, 172, 174, 182, 184, 186, 187, 193, 195, 196, 198, 201], "depend": [12, 13, 14, 15, 16, 17, 172, 174, 186, 196], "unobserv": [12, 13, 14, 15, 16, 17, 186, 196], "item": [12, 13, 14, 15, 16, 17, 171, 196], "engag": [12, 13, 14, 15, 16, 17, 196], "signal": [12, 13, 14, 15, 16, 17, 196], "baseusermodel": [12, 13, 14, 16, 17, 196], "model": [12, 13, 16, 17, 18, 19, 20, 21, 26, 27, 32, 33, 94, 95, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 178, 180, 181, 184, 186, 188, 189, 196, 198], "user_prefecture_dynam": [12, 13, 196], "how": [12, 13, 16, 17, 113, 114, 171, 172, 178, 181, 182, 184, 187, 192, 193, 196, 198, 200], "through": [12, 13, 18, 19, 22, 23, 32, 33, 172, 174, 184, 186, 188, 198], "reward_funct": [12, 13, 14, 15, 16, 17, 196], "respond": [12, 13], "discreterandompolici": [12, 13, 20, 21, 196], "sample_action_onlin": [12, 13, 142, 143, 184], "022": [12, 13], "david": [12, 13, 189], "rohd": [12, 13, 189], "stephen": [12, 13, 52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 189], "bonner": [12, 13, 189], "travi": [12, 13, 189], "dunlop": [12, 13, 189], "flavian": [12, 13, 189], "vasil": [12, 13, 189], "alexandro": [12, 13, 189], "karatzogl": [12, 13, 189], "recogym": [12, 13, 171, 189], "problem": [12, 13, 115, 116, 117, 118, 119, 120, 171, 172, 182, 184, 187, 188, 189, 192, 196, 198], "product": [12, 13, 16, 17, 28, 29, 30, 32, 33, 38, 45, 46, 51, 63, 70, 71, 76, 171, 172, 182, 188, 189, 192, 196], "advertis": [12, 13, 18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 171, 182, 189, 195, 198], "user_preference_dynam": [12, 13, 14, 15, 16, 17, 196], "user_id": [12, 13, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 198], "These": [12, 13, 18, 19, 20, 21, 171], "determin": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 186, 192, 196, 198], "you": [14, 15, 16, 17, 18, 19, 20, 21, 172, 173, 174, 175, 178, 182, 183, 184, 192, 193, 196, 198, 201], "rec": [16, 196], "sarah": [16, 17, 189], "dean": [16, 17, 189], "jami": [16, 17, 189], "morgenstern": [16, 17, 189], "dynam": [16, 17, 189], "person": [16, 17, 189], "2022": [16, 17, 110, 111, 113, 114, 189], "alpha": [16, 17, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 97, 99, 100, 101, 105, 107, 108, 109, 110, 111, 112, 113, 114, 150, 163, 164, 165, 166, 172, 174, 175, 176, 179, 186, 187, 188, 196, 198], "user_featur": [16, 17, 196], "amplifi": [16, 17, 196], "item_featur": [16, 17, 196], "control": [16, 17, 172, 174, 186, 192, 198], "fast": [16, 17], "evolv": [16, 17], "inner": [16, 17, 28, 29, 30, 32, 33, 192, 196], "real": [18, 19, 20, 21, 26, 27, 28, 31, 32, 33, 171, 173, 174, 175, 176, 178, 179, 182, 184, 186, 188, 189, 195, 196, 198, 201], "bid": [18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 31, 32, 33, 182, 189, 196, 201], "object": [18, 19, 26, 27, 34, 36, 115, 116, 117, 118, 119, 120, 150, 151, 152, 157, 172, 186, 196, 198], "cost_ind": [18, 19, 32, 33, 198], "initial_budget": [18, 19, 20, 21, 198], "3000": [18, 19], "n_ad": [18, 19, 28, 29, 30, 31, 32, 33, 198], "ad_feature_dim": [18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 198], "ad_feature_vector": [18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 198], "ad_sampling_r": [18, 19, 32, 33, 198], "user_sampling_r": [18, 19, 32, 33, 198], "winningpricedistribut": [18, 19, 28, 32, 33, 198], "clickthroughr": [18, 19, 28, 32, 33, 198], "conversionr": [18, 19, 28, 32, 33, 198], "standard_bid_price_distribut": [18, 19, 28, 31, 32, 33, 198], "normaldistribut": [18, 19, 28, 31, 32, 33, 34, 198], "mean": [18, 19, 26, 27, 28, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 96, 101, 102, 110, 111, 113, 114, 150, 163, 164, 165, 166, 171, 172, 173, 176, 177, 182, 184, 186, 187, 192, 198], "50": [18, 19, 20, 21, 28, 31, 32, 33, 184, 186, 189, 198], "std": [18, 19, 28, 31, 32, 33, 34, 35, 113, 114, 171, 173, 176, 182, 192, 198], "minimum_standard_bid_pric": [18, 19, 28, 31, 32, 33, 198], "search_volume_distribut": [18, 19, 32, 33, 198], "200": [18, 19, 32, 33], "20": [18, 19, 28, 31, 32, 33, 171, 172, 175, 178, 182, 189], "minimum_search_volum": [18, 19, 32, 33, 198], "rtbsyntheticsimul": [18, 19, 32], "auction": [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 189, 198], "result": [18, 19, 22, 24, 32, 33, 110, 112, 113, 114, 171, 172, 174, 175, 177, 178, 182, 186, 187, 188, 192, 200, 201], "constrain": [18, 19, 20, 21, 26, 27, 32, 33, 198], "24h": [18, 19, 20, 21], "dai": [18, 19, 20, 21, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 189, 198], "seven": [18, 19, 20, 21, 198], "per": [18, 19, 20, 21, 22, 23, 28, 29, 30, 32, 33, 38, 39, 40, 41, 42, 43, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 112, 171, 174, 182, 198], "week": [18, 19, 20, 21, 198], "have": [18, 19, 20, 21, 171, 172, 184, 186, 201], "search": [18, 19, 20, 21, 26, 27, 32, 33, 171, 198], "volum": [18, 19, 20, 21, 32, 33, 189, 198], "dure": [18, 19, 20, 21, 176, 180, 192, 198], "do": [18, 19, 20, 21, 171, 172, 192, 200], "NOT": [18, 19, 20, 21], "correspond": [18, 19, 20, 21, 88, 89, 113, 114, 115, 116, 118, 119, 150, 161, 172, 174], "statist": [18, 19, 20, 21, 88, 89, 110, 111, 113, 114, 171, 172, 173, 182, 184, 188, 189, 192, 201], "includ": [18, 19, 20, 21, 52, 58, 77, 83, 96, 105, 106, 110, 111, 112, 113, 114, 142, 143, 144, 145, 146, 147, 148, 149, 171, 173, 174, 175, 186, 187, 188, 192, 193, 196, 198, 200], "remain": [18, 19, 20, 21, 63, 66, 67, 68, 69, 71, 73, 74, 171, 172, 192, 198], "budget": [18, 19, 20, 21, 26, 27, 32, 33, 113, 114, 189, 192, 198], "previou": [18, 19, 20, 21, 38, 41, 63, 66, 110, 112, 172, 186, 188, 192, 198], "consumpt": [18, 19, 20, 21, 198], "rate": [18, 19, 20, 21, 22, 23, 26, 27, 32, 33, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 172, 173, 182, 186, 198], "cost": [18, 19, 20, 21, 22, 24, 32, 33, 171, 182, 186, 187, 198], "mill": [18, 19, 20, 21, 198], "win": [18, 19, 20, 21, 22, 25, 28, 31, 32, 33, 198], "adjust": [18, 19, 20, 21, 26, 27, 198], "price": [18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 31, 32, 33, 198], "individu": [18, 19, 20, 21], "_": [18, 19, 20, 21, 26, 27, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 185, 186, 187, 188, 193, 196, 198], "t": [18, 19, 20, 21, 26, 27, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 166, 171, 172, 176, 182, 186, 187, 188, 193, 196, 198], "predict": [18, 19, 20, 21, 26, 27, 94, 95, 115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 172, 176, 180, 188, 198], "const": [18, 19, 20, 21, 26, 27], "ground": [18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 32, 33, 110, 111, 113, 114, 171, 172, 179, 188, 198, 201], "truth": [18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 32, 33, 110, 111, 113, 114, 172, 179, 188, 198, 201], "abov": [18, 19, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 171, 172, 173, 178, 184, 186, 192, 193, 196, 198, 200], "equat": [18, 19, 172, 186], "customizedrtbenv": [18, 19, 20, 198], "wrapper": [18, 19, 20, 21, 142, 171, 182, 188], "total": [18, 19, 20, 21, 22, 24, 32, 33, 34, 35, 110, 111, 112, 113, 114, 186, 198], "discount": [18, 19, 20, 21, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 176, 180, 186, 187, 189, 198], "factor": [18, 19, 20, 21, 26, 27, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 176, 180, 186, 187, 198], "cumul": [18, 19, 20, 21, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 96, 98, 106, 110, 111, 113, 114, 173, 186, 200, 201], "constraint": [18, 19, 20, 21, 38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 115, 116, 117, 118, 119, 120, 189, 198], "exce": [18, 19, 20, 21, 113, 114], "outcom": [18, 19, 22, 24, 26, 27, 32, 33], "aris": [18, 19, 20, 21, 32, 33, 172, 186, 192, 198], "candid": [18, 19, 32, 33, 113, 114, 172, 173, 174, 179, 182, 187, 188, 192], "probabl": [18, 19, 20, 21, 22, 24, 28, 31, 32, 33, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 142, 143, 145, 148, 150, 164, 165, 171, 172, 176, 180, 184, 186, 187, 193, 196, 198], "basewinningpricedistribut": [18, 19, 22, 28, 31, 32, 33, 198], "baseclickandconversionr": [18, 19, 22, 28, 29, 30, 32, 33, 198], "whose": [18, 19, 28, 31, 32, 33, 52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 113, 114, 186, 198], "averag": [18, 19, 20, 21, 28, 31, 32, 33, 96, 100, 110, 111, 112, 113, 114, 172, 176, 186, 188, 198], "30": [18, 19, 171, 172, 174, 175, 184, 189], "13": [18, 19, 110, 111, 113, 114, 171, 172, 182, 184, 189], "44": [18, 19, 186, 189], "di": [18, 19, 20, 21, 26, 27, 32, 33], "wu": [18, 19, 20, 21, 26, 27, 32, 33, 189], "xiujun": [18, 19, 20, 21, 26, 27, 32, 33], "chen": [18, 19, 20, 21, 26, 27, 32, 33, 113, 114, 189], "xun": [18, 19, 20, 21, 26, 27, 32, 33], "yang": [18, 19, 20, 21, 26, 27, 32, 33, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 189], "hao": [18, 19, 20, 21, 26, 27, 32, 33], "wang": [18, 19, 20, 21, 26, 27, 32, 33, 113, 114, 189], "qing": [18, 19, 20, 21, 26, 27, 32, 33], "tan": [18, 19, 20, 21, 26, 27, 32, 33], "xiaoxun": [18, 19, 20, 21, 26, 27, 32, 33], "zhang": [18, 19, 20, 21, 26, 27, 32, 33, 113, 114, 115, 116, 117, 118, 119, 120, 189], "jian": [18, 19, 20, 21, 26, 27, 32, 33], "xu": [18, 19, 20, 21, 26, 27, 32, 33, 189], "kun": [18, 19, 20, 21, 26, 27, 32, 33], "gai": [18, 19, 20, 21, 26, 27, 32, 33], "free": [18, 19, 20, 21, 26, 27, 32, 33, 182, 192, 193, 196, 198, 201], "displai": [18, 19, 20, 21, 26, 27, 32, 33, 192, 198], "jun": [18, 19, 20, 21, 26, 27, 32, 33, 189], "zhao": [18, 19, 20, 21, 26, 27, 32, 33, 189], "guang": [18, 19, 20, 21, 26, 27, 32, 33], "qiu": [18, 19, 20, 21, 26, 27, 32, 33], "ziyu": [18, 19, 20, 21, 26, 27, 32, 33, 113, 114, 189], "guan": [18, 19, 20, 21, 26, 27, 32, 33], "wei": [18, 19, 20, 21, 26, 27, 32, 33, 189], "xiaofei": [18, 19, 20, 21, 26, 27, 32, 33], "he": [18, 19, 20, 21, 26, 27, 32, 33, 189], "deep": [18, 19, 20, 21, 26, 27, 32, 33, 77, 83, 113, 114, 150, 151, 152, 189], "sponsor": [18, 19, 20, 21, 26, 27, 32, 33], "occur": [18, 19, 22, 23, 24, 25, 28, 29, 30, 31, 32, 33, 198], "In": [18, 19, 28, 29, 30, 32, 33, 113, 114, 171, 172, 173, 174, 180, 181, 186, 187, 188, 189, 192, 198, 200], "bidder": [18, 19, 20, 21, 198], "second": [18, 19, 22, 24, 25, 28, 31, 32, 33, 184, 186, 192, 198], "check": [18, 19, 34, 36, 150, 153, 157, 158, 159, 178], "cancel": [18, 19], "aggreg": [18, 19, 172], "original_env": [20, 21, 198], "reward_predictor": [20, 21, 26, 27, 198], "scaler": [20, 21, 26, 27, 94, 95, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 151, 152, 198], "action_min": [20, 21, 198], "action_max": [20, 21, 198], "maker": [20, 21], "three": [20, 21, 186, 188, 189, 192, 195, 200], "customizedenv": [20, 21], "where": [20, 21, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 150, 164, 165, 166, 171, 172, 176, 184, 186, 187, 188, 192, 193, 196, 198], "standard_bid_pric": [20, 21, 26, 27, 28, 31, 32, 33, 198], "approxim": [20, 21, 26, 27, 38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 115, 116, 117, 118, 119, 120, 121, 122, 172, 186], "all": [20, 21, 46, 47, 48, 50, 63, 64, 65, 67, 71, 72, 73, 75, 77, 78, 79, 81, 84, 86, 94, 95, 110, 111, 112, 113, 114, 118, 119, 123, 126, 133, 134, 172, 192, 200], "transform": [20, 21, 142, 144, 150, 151, 152, 161, 172, 182, 184], "infti": [20, 21, 172], "within": [20, 21, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 150, 163, 164, 165, 166, 198], "tune": [20, 21], "rtb": [20, 21, 28, 32, 33, 182, 198], "origin": [20, 21, 172, 198], "baseestim": [20, 21, 26, 27], "machin": [20, 21, 26, 27, 189, 198], "one": [20, 21, 26, 27, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 171, 172, 184, 188, 192, 200], "scale": [20, 21, 26, 27, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 151, 152, 172, 174, 175, 178, 193, 196, 198], "constant": [20, 21, 26, 27, 198], "autofit": [20, 21], "auto_fit_scal": [20, 21, 26, 27], "automat": [20, 21, 115, 116, 117, 118, 119, 120, 173], "np": [20, 21, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 94, 95, 96, 97, 98, 105, 110, 111, 113, 114, 172, 175, 176, 180, 193, 196, 198], "logspac": [20, 21], "sklearn": [20, 21], "linear_model": [20, 21], "logisticregress": [20, 21], "11": [20, 21, 171, 172, 182, 189, 192], "75": [20, 21], "continuo": [20, 21], "ani": [20, 21, 150, 151, 152, 172, 178, 182, 184, 188, 193, 196, 198, 201], "basesimul": [22, 26, 27], "generate_auct": [22, 24, 32, 33], "ad_id": [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 198], "map_idx_to_featur": [22, 24, 32, 33], "n_sampl": [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 142, 145, 146, 148, 149, 150, 160, 162, 167, 168, 169, 170], "calc_and_sample_outcom": [22, 24, 32, 33], "bid_pric": [22, 24, 25, 26, 27, 28, 31, 32, 33, 198], "queri": [22, 24, 32, 33, 171], "rais": [22, 24, 32, 33], "compar": [22, 25, 28, 31, 32, 33, 38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 110, 111, 112, 113, 114, 171, 172, 173, 174, 175, 178, 186, 188, 192, 200, 201], "sample_outcom": [22, 23, 25, 28, 29, 30, 31, 32, 33, 198], "winning_pric": [22, 25, 28, 31, 32, 33, 198], "ctr": [22, 23, 28, 29, 31, 32, 33, 198], "cvr": [22, 23, 28, 30, 32, 33], "calc_prob": [22, 23, 28, 29, 30, 32, 33, 198], "py": [26, 27, 181, 183, 184], "formula": [26, 27, 186], "determine_bid_pric": [26, 27], "custom_set_scal": [26, 27], "100000": [26, 27, 188], "fit": [26, 27, 38, 39, 46, 47, 52, 53, 58, 63, 64, 71, 72, 77, 78, 83, 94, 95, 115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 172, 174, 184, 186, 188, 198], "reciproc": [26, 27], "bid_scal": [26, 27], "custom_set_reward_predictor": [26, 27], "predictor": [26, 27, 52, 53, 77, 78, 172, 198], "fit_reward_predictor": [26, 27], "advanc": [26, 27, 171, 186, 187, 188, 189], "use_reward_predictor": [26, 27], "x": [26, 27, 28, 31, 32, 33, 34, 37, 46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 142, 143, 145, 146, 147, 148, 149, 150, 152, 160, 162, 164, 165, 166, 167, 168, 169, 170, 172, 175, 178, 192, 200], "y": [26, 27, 110, 111, 112, 113, 114, 150, 160, 162, 167, 168, 169, 170, 173, 192, 200], "concaten": [26, 27], "winningdistribut": [28, 31, 32, 33], "gamma": [28, 31, 32, 33, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 176, 180, 186, 187, 188, 198], "p": [28, 31, 32, 33, 38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 110, 112, 150, 164, 165, 166, 172, 186, 187, 189, 193, 196, 198], "k": [28, 31, 32, 33, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 172, 174, 177, 183, 186, 187, 188, 200, 201], "frac": [28, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 112, 142, 148, 150, 164, 165, 166, 172, 184, 186, 192], "exp": [28, 31, 32, 33, 142, 148, 184], "theta": [28, 31, 32, 33, 186], "hyperparamet": [28, 31, 32, 33, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 160, 162, 167, 169, 170, 172, 174, 175, 184, 186, 189], "trend": [28, 29, 30, 31, 32, 33, 201], "cycl": [28, 29, 30, 31, 32, 33], "wen": [28, 31, 32, 33], "yuan": [28, 31, 32, 33, 52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 189], "zhu": [28, 31, 32, 33], "yueh": [28, 31, 32, 33], "shih": [28, 31, 32, 33], "ying": [28, 31, 32, 33], "hsuan": [28, 31, 32, 33], "lee": [28, 31, 32, 33, 189], "chih": [28, 31, 32, 33, 189], "peng": [28, 31, 32, 33], "jiun": [28, 31, 32, 33], "long": [28, 31, 32, 33, 186], "regress": [28, 31, 32, 33], "2017": [28, 31, 32, 33, 110, 112, 189], "two": [28, 29, 30, 32, 33, 113, 114, 172, 182, 184, 192, 201], "coeffici": [28, 29, 30, 32, 33, 113, 114, 115, 116, 117, 118, 119, 120, 171, 172, 182], "coef": [28, 29, 30, 32, 33], "time_coef": [28, 29, 30, 32, 33], "first": [28, 29, 30, 32, 33, 110, 111, 113, 114, 171, 172, 178, 179, 184, 186, 187, 188, 192, 200], "linearli": [28, 29, 30, 32, 33], "Then": [28, 29, 30, 32, 33, 94, 95, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 174, 175, 178, 180, 184, 187, 200], "multipli": [28, 29, 30, 32, 33], "short": [28, 29, 30, 32, 33], "denot": [28, 29, 30, 32, 33, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 172, 186, 187, 193, 196, 198], "n_use": [32, 33], "tool": [34, 150, 171, 172, 182, 184, 186, 188], "normal": [34, 35, 38, 42, 43, 44, 46, 48, 49, 52, 56, 57, 61, 62, 63, 67, 68, 69, 71, 73, 74, 77, 81, 82, 86, 87, 142, 146, 149, 150, 151, 152, 166, 171, 175, 182, 184, 193, 196, 198], "deviat": [34, 35, 113, 114, 142, 146, 149, 150, 166, 172, 186, 192, 196], "variabl": [34, 35, 172, 187], "random_vari": [34, 35], "sigmoid": [34, 198], "check_arrai": [34, 150], "expected_dim": [34, 36, 150, 157], "expected_dtyp": [34, 36, 150, 157], "min_val": [34, 36, 150, 157], "max_val": [34, 36, 150, 157], "input": [34, 36, 110, 111, 112, 113, 114, 150, 153, 157, 158, 160, 162, 167, 168, 169, 170, 173, 176, 177, 179, 182, 184, 188], "valid": [34, 36, 150, 157, 172, 177, 188, 200, 201], "dtype": [34, 36, 150, 157], "allow": [34, 36, 150, 157, 171, 174, 182], "design": [38, 39, 40, 41, 42, 43, 44, 45, 46, 52, 58, 182, 184, 192, 193, 196, 198], "determinist": [38, 39, 40, 41, 42, 43, 44, 45, 46, 52, 58, 94, 95, 142, 144, 145, 146, 149, 184, 189, 200], "directmethod": [38, 52, 58, 63, 77, 83, 172, 174, 188], "direct": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 171, 182, 188, 192], "dm": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 171, 173, 174, 175, 177, 178, 180, 182, 188, 192, 200, 201], "baseoffpolicyestim": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 93, 110, 111, 112, 172, 176, 188], "hat": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 112, 113, 114, 150, 164, 165, 166, 172, 186, 187, 192], "j": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 113, 114, 172, 186, 187, 188, 192], "mathrm": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 111, 112, 113, 114, 150, 166, 172, 184, 186, 187], "pi": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 148, 149, 172, 184, 186, 187, 188, 192, 193, 196, 198], "mathcal": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 112, 118, 119, 120, 133, 134, 135, 139, 140, 141, 142, 145, 146, 148, 149, 172, 184, 186, 187, 193, 196, 198], "d": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 184, 186, 187], "sum_": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 111, 112, 113, 114, 142, 148, 172, 184, 186, 187, 188, 192, 198], "s_0": [38, 39, 46, 47, 48, 50, 52, 54, 56, 58, 59, 61, 63, 64, 71, 72, 73, 75, 77, 79, 81, 83, 84, 86, 115, 116, 117, 118, 119, 120, 136, 137, 138, 139, 140, 141, 172, 186, 187], "v": [38, 39, 52, 58, 63, 64, 77, 83, 94, 95, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 150, 164, 172, 186, 192], "s_t": [38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 186, 187], "a_t": [38, 39, 40, 42, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 186, 187], "r_t": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 186, 187, 188, 198], "dataset": [38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 77, 83, 94, 95, 110, 111, 112, 113, 114, 150, 151, 152, 153, 154, 159, 171, 172, 173, 176, 179, 182, 186, 187, 189, 200, 201], "low": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 172, 174, 175], "varianc": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 96, 102, 103, 110, 111, 113, 114, 150, 164, 171, 172, 174, 176, 177, 179, 182, 186, 187, 188, 200, 201], "produc": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 171, 172, 192, 200], "larger": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 172, 192], "bia": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 83, 172, 174, 186], "due": [38, 39, 45, 46, 47, 51, 52, 58, 63, 64, 70, 71, 72, 76, 77, 83, 171, 172, 176, 180, 186, 192], "error": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 110, 112, 113, 114, 171, 172, 173, 182, 189, 192, 201], "There": [38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 184, 186], "sever": [38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 113, 114, 171, 182, 186, 187, 188, 198], "fqe": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 94, 95, 172, 174, 175, 184], "le": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 113, 114, 189], "2019": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 71, 72, 77, 83, 115, 116, 117, 118, 119, 120, 189], "mql": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 94, 95, 115, 116, 118, 119, 171, 172, 182], "implement": [38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 142, 145, 146, 148, 149, 173, 174, 175, 176, 178, 179, 186, 187, 188, 189, 192, 200, 201], "provid": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 150, 164, 165, 171, 172, 173, 174, 175, 176, 178, 180, 181, 182, 184, 186, 187, 188, 192, 193, 195, 196, 198, 200, 201], "avail": [38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 171, 172, 173, 176, 178, 179, 180, 182, 183, 192, 193, 196, 198, 200], "weight_value_learn": [38, 39, 46, 47, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95], "estimator_nam": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 110, 111, 113, 114, 176, 188, 201], "hoang": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 189], "cameron": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 189], "voloshin": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 189], "yisong": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 189], "yue": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 189], "batch": [38, 39, 46, 47, 52, 58, 63, 64, 71, 72, 77, 83, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 184, 189], "estimate_policy_valu": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 110, 112, 172, 176], "state_action_value_predict": [38, 39, 40, 42, 46, 47, 48, 50, 52, 53, 54, 56, 59, 61, 63, 64, 65, 67, 71, 72, 73, 75, 77, 78, 79, 81, 84, 86, 94, 95, 110, 111, 112, 113, 114, 172, 176, 180], "kwarg": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 164, 165, 166, 176, 198], "row": [38, 39, 40, 42, 52, 53, 54, 56, 59, 61, 94, 95], "v_hat": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "estimate_interv": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 110, 112, 172, 176], "05": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 99, 100, 101, 102, 107, 108, 109, 110, 111, 112, 113, 114, 150, 163, 164, 165, 166, 172, 174, 176], "ci": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 110, 111, 112, 113, 114, 172, 174, 176, 178, 179], "bootstrap": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 96, 100, 101, 108, 110, 112, 113, 114, 150, 163, 171, 172, 174, 176, 179, 182, 186, 189], "n_bootstrap_sampl": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 100, 101, 108, 110, 112, 113, 114, 150, 163, 176], "confid": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 96, 101, 110, 111, 112, 113, 114, 150, 163, 164, 165, 166, 171, 175, 176, 178, 179, 189, 200], "interv": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 96, 101, 110, 111, 112, 113, 114, 150, 163, 164, 165, 166, 172, 174, 175, 176, 178, 189, 200], "nonparametr": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 101, 110, 112, 150, 163], "signific": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 100, 101, 107, 108, 109, 110, 111, 112, 113, 114, 150, 163, 164, 165, 166, 174], "hoeffd": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 110, 112, 113, 114, 150, 165, 171, 172, 177, 179, 182], "bernstein": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 110, 112, 113, 114, 150, 164, 171, 172, 177, 179, 182], "ttest": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 110, 112, 113, 114, 172, 179], "resampl": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 100, 101, 108, 110, 112, 113, 114, 150, 163], "perform": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 100, 101, 108, 110, 112, 113, 114, 150, 163, 171, 172, 184, 186, 187, 188, 192, 193, 196, 198, 200, 201], "estimated_confidence_interv": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 150, 163, 164, 165, 166], "store": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 101, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 150, 163, 164, 165, 166, 178, 186], "upper": [38, 39, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 101, 150, 163, 164, 165, 166, 176], "lower": [38, 39, 40, 41, 42, 43, 44, 45, 46, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 101, 110, 111, 113, 114, 150, 163, 164, 165, 166, 172, 173, 176, 179, 182, 186, 188, 192, 200, 201], "bound": [38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 101, 113, 114, 150, 163, 164, 165, 166, 172, 186, 189], "trajectorywiseimportancesampl": [38, 44, 63, 69, 94, 95, 110, 112, 113, 114, 172, 174, 188], "ti": [38, 41, 45, 46, 50, 51, 63, 66, 70, 71, 75, 76, 94, 95, 110, 112, 113, 114, 171, 173, 174, 175, 178, 182, 188, 200, 201], "wise": [38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 96, 104, 110, 111, 113, 114, 171, 175, 176, 178, 182, 187, 188, 198, 200], "via": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 171, 172, 173, 175, 177, 182, 186, 189, 201], "w_": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 115, 117, 118, 120, 136, 138, 139, 141, 172], "delta": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 110, 111, 172, 187], "a_": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 77, 78, 79, 80, 81, 82, 115, 116, 118, 119, 133, 134, 136, 137, 138, 139, 140, 141, 172, 186], "prod_": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 172, 186, 187], "pi_0": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 113, 114, 115, 117, 118, 120, 136, 138, 139, 141, 172], "quantifi": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 186, 192], "similar": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 88, 90, 91, 92, 93, 172, 173, 175, 178, 179, 182, 184, 186, 198], "between": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 168, 171, 172, 186, 187, 192], "taken": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62], "cdot": [38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 71, 72, 73, 74, 75, 76, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 150, 166, 172, 186, 187, 198], "kernel": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 88, 89, 91, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 160, 162, 167, 169, 170, 172], "bandwidth": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 160, 162, 167, 168, 169, 170, 172, 174, 175], "often": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 171, 172, 186, 188, 192, 196, 198], "becom": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 172, 178, 180, 186, 200], "small": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 171, 172, 186], "larg": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 70, 71, 75, 76, 113, 114, 172, 186, 200], "those": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 171, 178, 180, 186, 192], "abl": [38, 45, 171, 173, 175, 192], "correct": [38, 40, 41, 45, 46, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 115, 116, 117, 118, 119, 120, 172, 189], "shift": [38, 40, 41, 45, 46, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 171, 172, 186, 187, 192], "howev": [38, 40, 41, 45, 46, 50, 51, 63, 65, 66, 70, 71, 75, 76, 115, 116, 117, 118, 119, 120, 172, 186, 187, 192], "suffer": [38, 40, 41, 45, 46, 50, 51, 63, 65, 66, 70, 71, 75, 76, 115, 116, 117, 118, 119, 120, 172, 186], "high": [38, 40, 41, 45, 46, 50, 51, 63, 65, 66, 70, 71, 75, 76, 110, 112, 150, 164, 165, 171, 175, 179, 189, 192], "entir": [38, 45, 46, 51, 63, 70, 71, 76, 94, 95, 172], "nathan": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 67, 68, 69, 77, 78, 189], "kallu": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 67, 68, 69, 77, 78, 189], "angela": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 189], "optim": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 115, 116, 117, 118, 119, 120, 142, 148, 171, 172, 186, 189, 192], "treatment": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 189], "doina": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "precup": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "richard": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "sutton": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "satind": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "singh": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "elig": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "trace": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "2000": [38, 41, 43, 44, 45, 52, 55, 57, 60, 62, 63, 66, 68, 69, 70, 77, 80, 82, 85, 87, 189], "evaluation_policy_act": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 110, 111, 112, 113, 114, 115, 116, 117, 130, 131, 132, 136, 137, 138, 172, 176, 180], "gaussian": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 88, 89, 91, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 146, 149, 150, 167, 172, 182], "choic": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 142, 143, 145, 148, 171, 172, 174, 179, 184, 192, 200], "pi_b": [38, 40, 41, 42, 43, 44, 45, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 186, 187, 192], "epanechnikov": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 88, 89, 91, 150, 162, 172], "triangular": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 88, 89, 91, 150, 169, 172], "cosin": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 88, 89, 91, 150, 160, 172], "uniform": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 88, 89, 91, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 150, 170, 172, 186, 192, 193, 196, 198], "smooth": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 172], "actionscal": [38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 94, 95, 110, 111, 112, 115, 116, 117, 130, 131, 132, 136, 137, 138], "perdecisionimportancesampl": [38, 43, 63, 68, 94, 95, 110, 112, 113, 114, 172, 174, 188], "pdi": [38, 40, 41, 52, 54, 55, 56, 57, 60, 61, 62, 63, 65, 66, 77, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 110, 112, 113, 114, 171, 173, 174, 177, 178, 182, 188, 192, 200, 201], "s_": [38, 40, 41, 42, 43, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 66, 67, 68, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 186, 187], "wrt": [38, 41, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 66, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 172], "still": [38, 40, 41, 46, 50, 63, 65, 66, 71, 75, 172], "doublyrobust": [38, 42, 63, 67, 172, 174, 188], "doubli": [38, 40, 42, 46, 48, 50, 52, 54, 56, 59, 61, 63, 65, 67, 71, 73, 75, 77, 79, 81, 84, 86, 171, 182, 188, 189], "robust": [38, 40, 42, 46, 48, 50, 52, 54, 56, 59, 61, 63, 65, 67, 71, 73, 75, 77, 79, 81, 84, 86, 171, 182, 188, 189], "dr": [38, 40, 42, 52, 54, 59, 63, 65, 77, 79, 84, 171, 173, 174, 175, 177, 178, 180, 182, 188, 192, 200, 201], "left": [38, 40, 42, 46, 47, 48, 49, 50, 51, 52, 54, 59, 61, 63, 65, 67, 71, 72, 73, 74, 75, 76, 77, 79, 81, 84, 86, 110, 111, 112, 113, 114, 172, 186, 187, 188, 192, 198, 200], "right": [38, 40, 42, 46, 47, 48, 49, 50, 51, 52, 54, 59, 61, 63, 65, 67, 71, 72, 73, 74, 75, 76, 77, 79, 81, 84, 86, 110, 111, 112, 113, 114, 172, 186, 187, 188, 192, 198], "reason": [38, 40, 46, 50, 63, 65, 71, 75, 172], "accur": [38, 40, 46, 50, 63, 65, 71, 75, 171, 172, 186, 187, 188, 192], "satisfi": [38, 40, 46, 50, 63, 65, 71, 75, 172, 173, 186, 188, 198], "quit": [38, 40, 46, 50, 63, 65, 71, 75, 172], "philip": [38, 40, 42, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 67, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 111, 112, 189], "thoma": [38, 40, 42, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 63, 65, 67, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 111, 112, 189], "emma": [38, 40, 42, 46, 47, 48, 49, 50, 51, 52, 54, 56, 59, 61, 63, 65, 67, 71, 72, 73, 74, 75, 76, 77, 79, 81, 84, 86, 110, 111, 189], "brunskil": [38, 40, 42, 46, 47, 48, 49, 50, 51, 52, 54, 56, 59, 61, 63, 65, 67, 71, 72, 73, 74, 75, 76, 77, 79, 81, 84, 86, 110, 111, 189], "effici": [38, 40, 42, 43, 44, 52, 53, 54, 56, 59, 61, 63, 65, 67, 68, 69, 77, 78, 79, 81, 84, 86, 171, 172, 186, 189, 192], "selfnormalizedti": [38, 63, 172, 174], "snti": [38, 44, 46, 49, 63, 69, 71, 74, 171, 174, 175, 182, 200], "self": [38, 42, 43, 44, 46, 48, 49, 52, 56, 57, 61, 62, 63, 67, 68, 69, 71, 73, 74, 77, 81, 82, 86, 87, 88, 90, 92, 93, 94, 95, 171, 176, 182, 193, 196, 198], "r_": [38, 42, 43, 44, 52, 53, 63, 67, 68, 69, 172, 186, 187, 188], "max": [38, 42, 43, 44, 63, 67, 68, 69, 113, 114, 123, 126, 150, 151, 152, 164, 165, 172, 174, 175, 176, 186, 187], "intrins": [38, 42, 43, 44, 63, 67, 68, 69, 189], "stabl": [38, 42, 43, 44, 63, 67, 68, 69, 189], "selfnormalizedpdi": [38, 63, 172, 174], "snpdi": [38, 43, 63, 68, 171, 172, 174, 182, 200], "selfnormalizeddr": [38, 63, 172, 174], "sndr": [38, 42, 52, 56, 61, 63, 67, 77, 81, 86, 171, 172, 174, 175, 182], "cumulativedistributiondm": [46, 71, 172, 175, 188], "cdf_dm": [46, 47, 71, 72, 188, 201], "cdf": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 171, 172, 176, 178, 182, 188, 201], "basecumulativedistributionopeestim": [46, 47, 48, 49, 50, 71, 72, 73, 74, 75, 76, 88, 110, 111, 172, 176, 188], "f": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 113, 114, 172, 176, 187, 188], "m": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 172, 186, 187, 188, 189], "g": [46, 47, 48, 50, 71, 72, 73, 75, 110, 111, 171, 172, 174, 175, 178, 180, 182, 186, 187, 196, 198], "mathbb": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 150, 164, 165, 166, 172, 186, 187, 188, 193, 196, 198], "leq": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 150, 164, 165, 166, 172, 187, 188, 192, 198], "mid": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 94, 95, 110, 111, 112, 113, 114, 118, 119, 120, 133, 134, 135, 139, 140, 141, 142, 145, 148, 172, 187, 188, 193, 196, 198], "yash": [46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 111, 189], "chandak": [46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 111, 189], "scott": [46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 111, 112, 189], "niekum": [46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 111, 112, 189], "bruno": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "castro": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111], "da": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 172, 189], "silva": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "erik": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "miller": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "univers": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "2021": [46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 59, 60, 61, 62, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 189, 196, 198], "audrei": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "leqi": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "zachari": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "c": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 198], "lipton": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "kamyar": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "azizzadenesh": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 189], "risk": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 96, 97, 102, 105, 110, 111, 113, 114, 172, 173, 176, 177, 182, 183, 186, 188, 189, 193, 200, 201], "assess": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 113, 114, 174, 175, 179, 180, 182, 183, 187, 189, 193, 201], "contextu": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 171, 189], "bandit": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 77, 83, 110, 111, 171, 189], "estimate_cumulative_distribution_funct": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 110, 111, 172, 175, 176, 178], "reward_scal": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 98, 110, 111, 172, 176], "foral": [46, 47, 48, 50, 51, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 118, 119, 120, 133, 134, 135, 139, 140, 141, 172], "n_partit": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 175, 178], "axi": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 112, 113, 114, 172, 174, 175, 176, 178, 180, 198, 200], "plot": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 112, 113, 114, 172, 173, 192], "estimated_cumulative_distribution_funct": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 113, 114], "n_episod": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76], "estimate_mean": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 110, 111, 172, 175, 176, 178], "estimated_mean": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76], "estimate_vari": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 110, 111, 172, 175, 176, 178, 188], "estimated_vari": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76], "estimate_conditional_value_at_risk": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 110, 111, 172, 175, 176, 178, 188], "n_alpha": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 105, 110, 111, 113, 114], "proport": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 99, 102, 105, 110, 111, 113, 114, 175, 176, 179], "shade": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 99, 102, 105, 110, 111, 113, 114], "region": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 99, 102, 105, 110, 111, 113, 114, 175, 176, 179], "linspac": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 96, 97, 105, 110, 111, 113, 114, 175, 176], "estimated_conditional_value_at_risk": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 113, 114], "cvar": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 96, 97, 102, 110, 111, 113, 114, 171, 172, 173, 176, 177, 179, 182, 187, 188, 200], "estimate_interquartile_rang": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 110, 111, 172, 175, 176, 178], "interquartil": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 88, 89, 96, 99, 102, 107, 110, 111, 113, 114, 171, 176, 182, 200], "estimated_interquartile_rang": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76, 110, 111, 113, 114, 176], "cumulativedistributionti": [46, 49, 71, 74, 110, 111, 113, 114, 172, 175, 188], "cdf_ti": [46, 51, 71, 76], "basecumulativedistributionopeyestim": [46, 51], "induc": [46, 51, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 118, 119, 120, 133, 134, 135, 139, 140, 141], "cumulativedistributiontdr": [46, 48, 71, 73, 172, 175, 188], "cdf_tdr": [46, 50, 71, 75], "tdr": [46, 50, 71, 75, 182], "quad": [46, 48, 50, 52, 54, 55, 56, 57, 59, 61, 62, 71, 73, 75, 77, 79, 81, 82, 84, 86, 115, 116, 117, 118, 119, 120, 136, 137, 138, 139, 140, 141, 172, 186], "a_0": [46, 48, 50, 71, 73, 75, 115, 116, 118, 119, 136, 137, 138, 139, 140, 141, 172], "cumulativedistributionsnti": [46, 71, 110, 111, 113, 114, 172, 175], "cdf_snti": [46, 49, 71, 74], "cumulativedistributionsntdr": [46, 71, 175], "cdf_sntdr": [46, 48, 71, 73], "sntdr": [46, 48, 71, 73], "margin": [52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 90, 92, 93, 94, 95, 110, 112, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 176, 177, 178, 180, 184, 187, 188], "doublereinforcementlearn": [52, 77, 172, 174], "drl": [52, 53, 77, 78, 174, 182], "doubl": [52, 53, 77, 78, 171, 189], "cross": [52, 53, 77, 78, 94, 95, 172, 174], "n_k": [52, 53, 77, 78, 94, 95, 172], "rho": [52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 172], "approx": [52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 186], "visit": [52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 172], "fold": [52, 53, 77, 78, 94, 95, 172], "_j": [52, 53, 77, 78, 94, 95, 172, 186], "th": [52, 53, 77, 78, 94, 95, 113, 114, 172, 173, 192], "split": [52, 53, 77, 78, 94, 95, 172], "subset": [52, 53, 77, 78, 94, 95, 172], "setminu": [52, 53, 77, 78, 94, 95, 172], "achiev": [52, 53, 77, 78, 172, 198], "semiparametr": [52, 53, 77, 78, 172], "wai": [52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 172, 184, 186], "augment": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 171, 172, 182, 186], "lagrangian": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 171, 172, 182, 189], "alm": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 171, 172, 182], "mwl": [52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 115, 116, 117, 118, 119, 120, 171, 172, 182], "mengjiao": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 189], "ofir": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 189], "nachum": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 113, 114, 115, 116, 117, 118, 119, 120, 189], "bo": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 189], "dale": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 189], "schuurman": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 189], "regular": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 182, 189], "state_action_marginal_importance_weight": [52, 53, 54, 55, 56, 57, 77, 78, 79, 80, 81, 82, 94, 95, 110, 111, 112, 113, 114, 172, 176, 180], "statemarginaldm": [52, 77, 172], "basestatemarginalopeestim": [52, 58, 59, 60, 61, 62, 77, 83, 85, 86, 87, 88], "differ": [52, 58, 77, 83, 94, 95, 110, 111, 112, 113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 174, 178, 181, 186, 187, 192], "sm_dm": [52, 58, 77, 83], "initial_state_value_predict": [52, 54, 56, 58, 59, 61, 77, 79, 81, 83, 84, 86, 94, 95, 110, 111, 112, 113, 114, 172, 176, 180], "statemarginali": [52, 62, 77, 87, 172, 174], "sm_i": [52, 60, 77, 85], "sm": [52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 171, 172, 182], "sope": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 171, 174, 182, 189], "combin": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 171, 172, 186, 192], "t_1": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87], "t_2": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87], "addition": [52, 54, 55, 56, 57, 59, 60, 61, 62, 171, 173, 174, 175, 179, 192], "reduc": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 112, 172, 186, 187], "vanilla": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 112, 186], "caus": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87], "across": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 172, 175, 178, 184, 192, 200], "variou": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 171, 172, 182, 184, 187, 188, 192, 195, 200], "christina": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 189], "giguer": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 189], "spectrum": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 171, 182, 189], "n_step_pdi": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 112, 172, 174], "state_marginal_importance_weight": [52, 59, 60, 61, 62, 77, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 172, 176, 180], "zero": [52, 54, 55, 56, 57, 59, 60, 61, 62, 77, 79, 80, 81, 82, 84, 85, 86, 87, 110, 112, 115, 116, 117, 118, 119, 120, 142, 148, 172], "statemarginaldr": [52, 61, 77, 86, 172, 174], "sm_dr": [52, 59, 77, 84], "statemarginalsni": [52, 77, 172, 174], "sm_sni": [52, 62, 77, 87], "sni": [52, 57, 62, 77, 82, 87, 171, 182], "statemarginalsndr": [52, 77, 172, 174], "sm_sndr": [52, 61, 77, 86], "stateactionmarginali": [52, 57, 77, 82, 172, 174], "sam_i": [52, 55, 77, 80], "sam": [52, 54, 55, 56, 57, 77, 79, 80, 81, 82, 171, 172, 182], "basestateactionmarginalopeestim": [52, 54, 55, 56, 57, 77, 79, 80, 81, 82, 84, 88], "stateactionmarginaldr": [52, 56, 77, 81, 172, 174], "sam_dr": [52, 54, 77, 79], "stateactionmarginalsni": [52, 77, 172, 174], "sam_sni": [52, 57, 77, 82], "l": [52, 57, 115, 116, 117, 118, 119, 120, 186], "stateactionmarginalsndr": [52, 77, 172, 174], "sam_sndr": [52, 56, 77, 81], "w": [52, 56, 94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 192], "evaluation_policy_action_dist": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 94, 95, 110, 111, 112, 113, 114, 118, 119, 120, 133, 134, 135, 139, 140, 141, 172, 176, 180], "enabl": [63, 70, 71, 76, 110, 111, 123, 125, 127, 128, 142, 147, 171, 172, 173, 174, 175, 178, 179, 181, 182, 184, 186, 188, 198, 200], "unbias": [63, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 84, 85, 172, 186], "longer": [63, 67, 68, 69, 71, 73, 74, 172], "quartil": [71, 72, 73, 74, 75, 76, 110, 111, 113, 114, 172, 173, 176, 179, 182, 187, 188, 200, 201], "cumulativedistributiontrajectorywisedr": [71, 75], "yuta": [77, 83, 182, 183, 189, 192, 193, 196, 198, 201], "saito": [77, 83, 182, 183, 189, 192, 193, 196, 198, 201], "shunsuk": [77, 83, 189], "aihara": [77, 83, 189], "megumi": [77, 83, 189], "matsutani": [77, 83, 189], "yusuk": [77, 83, 189], "narita": [77, 83, 189], "open": [77, 83, 181, 182, 189, 196, 198], "pipelin": [77, 83, 182, 183, 189, 192, 193, 196, 198, 201], "toward": [77, 83, 171, 182, 183, 189, 192, 193, 201], "realist": [77, 83, 189], "reproduc": [77, 83, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 189], "takuma": [77, 83, 189], "seno": [77, 83, 189], "michita": [77, 83, 189], "imai": [77, 83, 189], "alina": [77, 83, 189], "beygelzim": [77, 83, 189], "langford": [77, 83, 189], "offset": [77, 83, 189], "tree": [77, 83, 189], "label": [77, 83, 189], "2009": [77, 83, 189], "correctli": [77, 79, 80, 81, 82, 84, 85, 86, 87], "privat": [88, 89, 90, 91, 92, 93], "_estimate_trajectory_valu": [88, 90, 91, 92, 93, 176], "_calc_behavior_policy_pscore_discret": [88, 90, 91, 92, 93], "_calc_behavior_policy_pscore_continu": [88, 90, 91, 92, 93], "_calc_evaluation_policy_pscore_discret": [88, 90, 91, 92, 93], "_calc_similarity_weight": [88, 90, 91, 92, 93], "properti": [88, 89, 90, 91, 92, 93, 150, 153, 171, 174, 176, 178, 187, 192, 198], "_estimate_confidence_interv": [88, 90, 91, 92, 93, 176], "_kernel_funct": [88, 89, 91], "basemarginalopeestim": [88, 92, 93], "_calc_marginal_importance_weight": [88, 90, 92, 93], "specifi": [88, 90, 94, 95, 110, 111, 112, 115, 116, 117, 118, 119, 120, 123, 126, 127, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 173, 174, 175, 178, 179, 188], "basestatemarginaloffpolicyestim": [88, 90], "basestateactionmarginaloffpolicyestim": [88, 90], "_aggregate_trajectory_wise_statistics_discret": [88, 89], "summari": [88, 89, 171], "_aggregate_trajectory_wise_statistics_continu": [88, 89], "_target_value_given_idx": [88, 89, 176], "idx": [88, 89, 176], "meta": [94, 110, 113, 171, 184], "creat": [94, 95, 110, 111, 112, 113, 114, 174, 175, 178, 182, 188], "createopeinput": [94, 110, 111, 112, 113, 114, 172, 174, 175, 178, 188], "model_arg": [94, 95, 172, 174, 175], "state_scal": [94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 174, 175], "devic": [94, 95, 115, 116, 117, 118, 119, 120, 123, 126, 127, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "cuda": [94, 95, 115, 116, 117, 118, 119, 120, 123, 126, 127, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 174, 175], "state_action_du": [94, 95, 172], "state_action_valu": [94, 95, 172], "state_action_weight": [94, 95, 172], "state_du": [94, 95, 172], "state_valu": [94, 95, 172], "state_weight": [94, 95, 172], "hidden_dim": [94, 95, 123, 124, 125, 126, 127, 128, 129, 172], "hidden": [94, 95, 123, 124, 125, 126, 127, 128, 129, 172], "dim": [94, 95, 172], "except": [94, 95, 172], "obtain_whole_input": [94, 95, 110, 111, 112, 113, 114, 172, 174, 175, 178, 188], "overwrit": [94, 95], "describ": [94, 95, 110, 111, 112, 113, 114, 142, 143, 144, 145, 146, 147, 148, 149, 171, 172, 182, 184, 186, 187, 188, 192, 193, 196, 198], "torch": [94, 95, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 174, 175], "offpolicyevalu": [94, 95, 110, 113, 114, 172, 173, 174, 178, 179, 188, 200], "ddqn_": [94, 95, 110, 111, 112, 113, 114, 188], "base_polici": [94, 95, 110, 111, 112, 113, 114, 142, 144, 145, 146, 147, 148, 149, 184, 188, 196], "random_": [94, 95, 110, 111, 112, 113, 114, 188, 193, 196, 198], "prep": [94, 95, 110, 111, 112, 113, 114, 172, 174, 175, 178, 188], "input_dict": [94, 95, 110, 111, 112, 113, 114, 150, 153, 158, 172, 173, 174, 175, 176, 178, 179, 180, 188, 200, 201], "evaluation_polici": [94, 95, 110, 111, 112, 113, 114, 172, 173, 174, 175, 176, 178, 179, 180, 188], "require_value_predict": [94, 95, 172, 174, 175], "n_trajectories_on_policy_evalu": [94, 95, 110, 111, 112, 113, 114, 172, 174, 175, 178, 188], "build_and_fit_fq": [94, 95], "k_fold": [94, 95, 172, 174], "gradient": [94, 95, 115, 116, 117, 118, 119, 120, 123, 125, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 182, 189], "build_and_fit_state_action_dual_model": [94, 95], "build_and_fit_state_action_value_model": [94, 95], "build_and_fit_state_action_weight_model": [94, 95], "build_and_fit_state_dual_model": [94, 95], "build_and_fit_state_value_model": [94, 95], "mvl": [94, 95, 115, 117, 118, 120, 172], "build_and_fit_state_weight_model": [94, 95], "obtain_initial_st": [94, 95], "resample_initial_st": [94, 95, 172], "evaluation_policy_initial_st": [94, 95], "obtain_evaluation_policy_act": [94, 95], "sim": [94, 95, 115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 146, 149, 150, 164, 165, 166, 172, 186, 187, 193, 196, 198], "obtain_evaluation_policy_action_dist": [94, 95], "its": [94, 95, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 142, 143, 146, 148, 149, 171, 172, 174, 178, 186, 188, 192, 200], "obtain_evaluation_policy_action_prob_for_observed_state_act": [94, 95], "evaluation_policy_pscor": [94, 95], "obtain_state_action_value_predict": [94, 95], "dice_q": [94, 95, 172], "obtain_initial_state_value_predict": [94, 95], "dice_v": [94, 95, 172], "obtain_state_action_marginal_importance_weight": [94, 95], "dice": [94, 95, 171, 172, 174, 175, 182], "state_action_weight_predict": [94, 95], "obtain_state_marginal_importance_weight": [94, 95], "state_weight_predict": [94, 95], "behavior_policy_nam": [94, 95, 110, 111, 112, 113, 114, 150, 153, 154, 172, 178, 184], "require_weight_predict": [94, 95, 172, 174], "q_function_method": [94, 95, 172, 174, 175], "v_function_method": [94, 95, 172, 174, 175], "w_function_method": [94, 95, 172, 174], "use_stationary_distribution_on_policy_evalu": [94, 95, 172], "seealso": [94, 95, 110, 111, 112], "appli": [94, 95, 110, 111, 112, 172, 174, 175, 176, 184, 189], "same": [94, 95, 110, 111, 113, 114, 150, 153, 172, 173, 176, 178, 184, 192, 200], "without": [94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 172, 178, 184, 186, 188, 189], "multipleinputdict": [94, 95, 110, 111, 112, 113, 114, 150, 172, 178], "input_dict_": [94, 95, 172], "evaluation_policy_nam": [94, 95, 180], "on_policy_policy_valu": [94, 95, 96, 100, 104, 108, 110, 111, 112, 113, 114, 172, 176, 180], "use_base_model": [94, 95, 188], "On": [94, 95, 96, 172, 180, 182, 192, 200], "opeinputdict": [94, 95, 110, 111, 112, 113, 114, 150, 153, 158], "comparison": [96, 172, 182, 188, 192, 201], "visualize_on_policy_policy_valu": [96, 184], "policy_nam": [96, 105, 106, 107, 108, 109], "evaluate_on_stationary_distribut": [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109], "fig_dir": [96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 174], "fig_nam": [96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 174], "png": [96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 174], "visual": [96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 171, 172, 174, 175, 177, 178, 182, 184, 188, 201], "algobas": [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 142, 143, 144, 145, 146, 147, 148, 149, 184], "bar": [96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114], "figur": [96, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 171, 174, 188, 192, 200], "visualize_on_policy_policy_value_with_vari": 96, "estimated_policy_valu": [96, 109, 110, 111, 112, 113, 114, 174, 176], "visualize_on_policy_cumulative_distribution_funct": [96, 184], "use_custom_reward_scal": [96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 175], "scale_min": [96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 175, 178], "scale_max": [96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114, 175, 178], "legend": [96, 105, 106, 110, 111, 112, 113, 114, 173, 174, 175, 178], "on_policy_cumulative_distribution_funct": [96, 106], "chundak": [96, 97, 98, 99, 102, 105, 106, 107, 110, 111], "must": [96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 112, 113, 114, 173, 178, 179], "partit": [96, 97, 98, 99, 102, 105, 106, 107, 110, 111, 113, 114], "visualize_on_policy_conditional_value_at_risk": [96, 184], "on_policy_conditional_value_at_risk": [96, 97, 105, 107], "visualize_on_policy_interquartile_rang": [96, 184], "on_policy_interquartile_rang": [96, 107], "calc_on_policy_statist": 96, "quartile_alpha": [96, 102, 113, 114, 179], "cvar_alpha": [96, 102, 113, 114, 179], "statistics_dict": [96, 102], "use_bootstrap": [96, 100], "calc_on_policy_policy_value_interv": [96, 184], "on_policy_confidence_interv": [96, 101], "calc_on_policy_vari": [96, 184], "on_policy_vari": [96, 103], "calc_on_policy_conditional_value_at_risk": [96, 184], "calc_on_policy_interquartile_rang": 96, "interquartile_range_dict": [96, 99, 110, 111, 175], "calc_on_policy_cumulative_distribution_funct": [96, 184], "cumulative_distribution_funct": [96, 98], "n_unique_reward": [96, 98, 110, 111], "rollout_policy_onlin": [96, 184], "ope_estim": [110, 111, 112, 113, 114, 172, 174, 175, 178, 188, 200, 201], "disable_reward_after_don": [110, 111, 112], "simultan": [110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 172], "conduct": [110, 111, 112, 113, 114, 171, 172, 174, 175, 178, 179, 182, 188, 192, 200, 201], "r": [110, 111, 112, 113, 114, 130, 131, 132, 133, 134, 135, 172, 186, 187, 189, 193, 196, 198], "onc": [110, 111, 112, 172, 178, 182, 184, 186], "policy_value_dict": [110, 112, 175], "on_polici": [110, 111, 112], "15": [110, 112, 171, 172, 174, 182, 189, 192], "95": [110, 111, 112, 113, 114], "103809657474702": [110, 112], "16": [110, 112, 171, 172, 182, 189], "95314065192053": [110, 112], "12": [110, 112, 171, 172, 182, 184, 189], "69": [110, 112], "4885685147584351": [110, 112], "2752568547701335": [110, 112], "estimators_nam": [110, 111, 112, 113, 114], "compared_estim": [110, 111, 112, 113, 114, 173, 174, 175, 178, 188], "ope_estimator_nam": [110, 111, 112], "policy_value_interval_dict": [110, 112], "josiah": [110, 112, 189], "hanna": [110, 112, 189], "peter": [110, 112, 189], "stone": [110, 112, 189], "georgio": [110, 112, 189], "theochar": [110, 112, 189], "mohammad": [110, 112, 113, 114, 189], "ghavamzadeh": [110, 112, 189], "improv": [110, 112, 186, 189], "2015": [110, 112, 189], "summarize_off_policy_estim": [110, 112, 172, 174, 178], "summar": [110, 112, 171, 174, 188], "policy_value_df_dict": [110, 112, 174, 178], "policy_value_interval_df_dict": [110, 112, 174, 178], "datafram": [110, 112, 113, 114, 179], "evaluate_performance_of_ope_estim": [110, 112, 172, 174], "metric": [110, 112, 113, 114, 171, 174, 177, 183, 186, 187, 188, 189, 192, 200, 201], "ee": [110, 112, 174], "return_by_datafram": [110, 112, 113, 114, 172, 173, 178, 179, 188], "accuraci": [110, 112, 171, 172, 177, 182, 187, 192], "squar": [110, 112, 113, 114, 171, 172, 182, 192], "se": [110, 112, 174], "v_": [110, 112, 186], "format": [110, 112, 113, 114, 179], "eval_metric_ope_dict": [110, 112], "eval_metric_ope_df": [110, 112, 174], "visualize_off_policy_estim": [110, 112, 172, 174, 178, 179, 188, 200, 201], "is_rel": [110, 111, 112, 113, 114, 174, 175], "hue": [110, 111, 112, 113, 114, 174, 175, 178], "sharei": [110, 111, 112, 113, 114, 173, 188, 200, 201], "share": [110, 111, 112, 113, 114, 175], "among": [110, 111, 112, 113, 114, 171, 172, 174, 182, 186, 187, 188, 192], "visualize_policy_value_with_multiple_estim": [110, 111, 112, 172, 178], "plot_typ": [110, 111, 112, 113, 114, 172, 178], "estimated_policy_value_multipl": [110, 111, 112, 113, 114], "scatter": [110, 111, 112, 113, 114, 172, 173, 178, 200], "violin": [110, 111, 112, 113, 114, 172, 178, 200], "empir": [110, 111, 112, 113, 114, 150, 164, 171, 172, 182, 186, 189], "cumulativedistributionop": [110, 113, 114, 172, 173, 175, 178, 179, 188, 201], "cumutivedistributionop": [110, 111], "some": [110, 111, 113, 114, 172, 178, 182, 184, 186, 187, 192, 198], "itself": [110, 111, 171], "u": [110, 111, 171, 172, 184, 185, 186, 192, 201], "mu": [110, 111, 150, 164, 165, 166, 172, 186, 187], "int_": [110, 111, 172, 187], "sigma": [110, 111, 142, 146, 149, 150, 166, 172, 174, 175, 184, 187], "min": [110, 111, 150, 151, 152, 172, 174, 175, 176, 186, 187], "math": [110, 111, 196], "df": [110, 111, 172, 187], "lim": [110, 111, 172, 187], "rightarrow": [110, 111, 172, 186, 187, 193, 196, 198], "cd_i": [110, 111, 113, 114, 172, 188, 201], "cd_sni": [110, 111, 113, 114, 188, 201], "cd_ope": [110, 111, 113, 114, 172, 173, 174, 175, 178, 188, 201], "variance_dict": [110, 111, 172, 175, 188], "6216": [110, 111], "cdf_i": [110, 111, 188, 201], "19": [110, 111, 113, 114, 171, 172, 182, 189], "201934808340265": [110, 111], "cdf_sni": [110, 111, 188, 201], "25": [110, 111, 171, 172, 182, 189, 192], "315555555555555": [110, 111], "512806887023064": [110, 111], "591854902638273": [110, 111], "158545530356914": [110, 111], "obtain_reward_scal": [110, 111, 172], "cumulative_distribution_dict": [110, 111], "mean_dict": [110, 111], "conditional_value_at_risk_dict": [110, 111], "quartile_nam": [110, 111], "visualize_cumulative_distribution_funct": [110, 111, 172, 175, 178, 179, 188, 201], "n_col": [110, 111, 113, 114, 172, 173, 175, 178, 188, 201], "column": [110, 111, 113, 114, 175], "visualize_policy_valu": [110, 111, 172, 175, 178, 179], "visualize_conditional_value_at_risk": [110, 111, 172, 175, 178, 179], "visualize_interquartile_rang": [110, 111, 172, 175, 178, 179], "visualize_cumulative_distribution_function_with_multiple_estim": [110, 111, 113, 114, 172, 178], "ci_hu": [110, 111, 113, 114, 178], "driven": [110, 111, 113, 114, 192], "ci_behavior_polici": [110, 111, 113, 114, 178], "enumer": [110, 111, 113, 114, 176, 178, 184], "visualize_variance_with_multiple_estim": [110, 111, 113, 114, 172, 178], "estimated_variance_multipl": [110, 111, 113, 114], "visualize_conditional_value_at_risk_with_multiple_estim": [110, 111, 113, 114, 178], "estimated_conditional_value_at_risk_multipl": [110, 111, 113, 114], "visualize_lower_quartile_with_multiple_estim": [110, 111, 113, 114, 172], "select": [113, 114, 171, 172, 173, 175, 184, 186, 189, 192, 193, 196, 198, 201], "offpolicyselect": [113, 172, 173, 178, 179, 188, 200, 201], "cumulative_distribution_op": [113, 114, 172, 173, 178, 179, 188, 201], "best": [113, 114, 142, 143, 171, 172, 173, 182, 187, 188, 192], "arg": [113, 114, 172, 186, 187], "contrast": [113, 114, 181, 192], "ops_dict": [113, 114], "select_by_policy_valu": [113, 114, 172, 173, 178, 179, 188], "return_metr": [113, 114, 172, 173, 178, 188], "estimated_rank": [113, 114], "3624954": [113, 114], "3827044": [113, 114], "estimated_relative_policy_valu": [113, 114], "44732354": [113, 114], "02592848": [113, 114], "mean_squared_error": [113, 114], "94": [113, 114], "79587393975419": [113, 114], "rank_correl": [113, 114], "spearmanrresult": [113, 114], "correl": [113, 114, 171, 172, 182, 192], "9999999999999999": [113, 114], "pvalu": [113, 114], "regret": [113, 114, 171, 172, 173, 182, 192], "type_i_error_r": [113, 114], "type_ii_error_r": [113, 114], "safety_threshold": [113, 114, 173, 178, 201], "284": [113, 114], "02806424": [113, 114], "13847486": [113, 114], "22141357": [113, 114], "48363651": [113, 114], "45349619733373": [113, 114], "vladislav": [113, 114, 189], "kurenkov": [113, 114, 189], "sergei": [113, 114, 189], "kolesnikov": [113, 114, 189], "show": [113, 114, 172, 173, 174, 175, 176, 178, 179, 188, 189, 192], "your": [113, 114, 172, 181, 182, 183, 189, 192, 193, 196, 198, 201], "matter": [113, 114, 171, 187, 189], "shengpu": [113, 114, 189], "jenna": [113, 114, 189], "wien": [113, 114, 189], "practic": [113, 114, 171, 182, 187, 188, 189, 192], "consider": [113, 114, 189], "healthcar": [113, 114, 189], "justin": [113, 114, 189], "fu": [113, 114, 189], "norouzi": [113, 114, 189], "georg": [113, 114, 189], "tucker": [113, 114, 189], "alexand": [113, 114, 189], "novikov": [113, 114, 189], "michael": [113, 114, 189], "yutian": [113, 114, 189], "avir": [113, 114, 189], "kumar": [113, 114, 189], "cosmin": [113, 114, 189], "paduraru": [113, 114, 189], "levin": [113, 114, 189], "tom": [113, 114, 189], "pain": [113, 114, 189], "benchmark": [113, 114, 171, 182, 183, 189, 193, 201], "andrea": [113, 114, 189], "michi": [113, 114, 189], "caglar": [113, 114, 189], "gulcehr": [113, 114, 189], "konrad": [113, 114, 189], "zolna": [113, 114, 189], "nando": [113, 114, 189], "de": [113, 114, 189], "freita": [113, 114, 189], "obtain_true_selection_result": [113, 114, 178, 179], "return_vari": [113, 114, 179], "return_lower_quartil": [113, 114, 179], "return_conditional_value_at_risk": [113, 114, 179], "oracl": [113, 114, 172, 177, 198], "ground_truth_dict": [113, 114], "ground_truth_df": [113, 114], "rank": [113, 114, 171, 172, 179, 182, 187, 188, 192], "policy_valu": [113, 114], "relative_policy_valu": [113, 114], "ranking_by_lower_quartil": [113, 114], "lower_quartil": [113, 114], "ranking_by_conditional_value_at_risk": [113, 114], "conditional_value_at_risk": [113, 114], "sort": [113, 114], "return_true_valu": [113, 114, 179], "top_k_in_eval_metr": [113, 114, 173], "relative_safety_criteria": [113, 114, 173, 178], "term": [113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 186, 192], "ii": [113, 114, 171, 172, 173, 182], "mani": [113, 114, 171, 186], "threshold": [113, 114, 172, 173], "unsaf": [113, 114, 186], "requir": [113, 114, 172, 173, 174, 186, 188], "safe": [113, 114, 171], "90": [113, 114, 173], "ranking_df_dict": [113, 114, 178], "metric_df": [113, 114, 172, 178, 188], "true_rank": [113, 114], "true_policy_valu": [113, 114], "true_relative_policy_valu": [113, 114], "hypothet": [113, 114], "test": [113, 114, 150, 166, 171, 172, 176, 182, 188, 192], "neg": [113, 114, 142, 148, 192], "posit": [113, 114, 115, 116, 117, 118, 119, 120, 186], "undetect": [113, 114], "select_by_policy_value_via_cumulative_distribution_op": [113, 114, 172, 173, 178, 179, 188], "select_by_policy_value_lower_bound": [113, 114, 172, 173, 178, 179], "estimated_policy_value_lower_bound": [113, 114], "estimated_relative_policy_value_lower_bound": [113, 114], "select_by_lower_quartil": [113, 114, 172, 173, 178, 179, 188], "estimated_lower_quartil": [113, 114], "true_lower_quartil": [113, 114], "select_by_conditional_value_at_risk": [113, 114, 172, 173, 178, 179], "true_conditional_value_at_risk": [113, 114], "visualize_policy_value_for_select": [113, 114, 172, 179], "estimated_policy_value_standard_op": [113, 114], "box": [113, 114, 200], "visualize_cumulative_distribution_function_for_select": [113, 114, 172, 179], "visualize_policy_value_of_cumulative_distribution_ope_for_select": [113, 114, 172, 179], "estimated_policy_value_cumulative_distribution_op": [113, 114], "bw": [113, 114], "visualize_conditional_value_at_risk_for_select": [113, 114, 172, 179], "visualize_interquartile_range_for_select": [113, 114, 172, 179], "visualize_policy_value_with_multiple_estimates_standard_op": [113, 114, 172], "estimated_policy_value_multiple_standard_op": [113, 114], "visualize_policy_value_with_multiple_estimates_cumulative_distribution_op": [113, 114, 172], "estimated_policy_value_multiple_cumulative_distribution_op": [113, 114], "obtain_topk_policy_value_selected_by_standard_op": [113, 114, 173, 178], "max_topk": [113, 114], "return_safety_violation_r": [113, 114, 173], "topk": [113, 114, 172], "deploy": [113, 114, 171, 172, 186, 187, 188, 189, 201], "shown": [113, 114, 171, 172, 192], "deploi": [113, 114, 186, 187, 192, 201], "safeti": [113, 114, 171, 172, 173, 182, 187], "violat": [113, 114, 171, 173, 182], "topk_metric_dict": [113, 114], "topk_metric_df": [113, 114, 173, 178], "top": [113, 114, 171, 172, 177, 186, 187, 188, 191, 193, 194, 196, 197, 198, 199, 200, 201], "tradeoff": [113, 114, 171, 172, 174, 177, 182, 183, 187, 193, 200, 201], "here": [113, 114, 172, 173, 174, 175, 176, 178, 179, 180, 182, 184, 186, 188, 192, 193, 196, 198], "worst": [113, 114, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 172, 173, 182, 192], "safety_violation_r": [113, 114, 173], "sharpe_ratio": [113, 114, 173], "total_n_dataset": [113, 114], "wosrt": [113, 114], "regard": [113, 114, 192], "obtain_topk_policy_value_selected_by_cumulative_distribution_op": [113, 114, 173, 178], "obtain_topk_policy_value_selected_by_lower_bound": [113, 114, 173, 178], "clip_sharpe_ratio": [113, 114, 173], "ope_alpha": [113, 114, 173, 201], "clip": [113, 114, 142, 146, 172, 176], "sharpratio": [113, 114, 171, 173, 182, 183, 187, 201], "1e2": [113, 114], "obtain_topk_conditional_value_at_risk_selected_by_standard_op": [113, 114, 173, 178], "obtain_topk_conditional_value_at_risk_selected_by_cumulative_distribution_op": [113, 114], "obtain_topk_lower_quartile_selected_by_standard_op": [113, 114, 173, 178], "obtain_topk_lower_quartile_selected_by_cumulative_distribution_op": [113, 114, 173, 178], "visualize_topk_policy_value_selected_by_standard_op": [113, 114, 172, 173, 178, 188], "ymax_sharpe_ratio": [113, 114, 173], "visualize_ci": [113, 114, 178], "plot_ci": [113, 114], "plot_alpha": [113, 114], "plot_n_bootstrap_sampl": [113, 114], "topk_policy_value_standard_op": [113, 114], "sharp": [113, 114, 171, 182], "ratio": [113, 114, 171, 182], "report": [113, 114, 173, 192], "ymax_sharp_ratio": [113, 114], "visualize_topk_policy_value_selected_by_cumulative_distribution_op": [113, 114, 172, 173, 178], "topk_policy_value_cumulative_distribution_op": [113, 114], "visualize_topk_policy_value_selected_by_lower_bound": [113, 114, 172, 173, 178], "ope_ci": [113, 114], "ope_n_bootstrap_sampl": [113, 114], "topk_policy_value_standard_ope_lower_bound": [113, 114], "visualize_topk_conditional_value_at_risk_selected_by_standard_op": [113, 114, 172, 173, 178], "topk_cvar_standard_op": [113, 114], "visualize_topk_conditional_value_at_risk_selected_by_cumulative_distribution_op": [113, 114, 172], "topk_cvar_cumulative_distribution_op": [113, 114], "visualize_topk_lower_quartile_selected_by_standard_op": [113, 114, 172, 173, 178], "topk_lower_quartile_standard_op": [113, 114], "visualize_topk_lower_quartile_selected_by_cumulative_distribution_op": [113, 114, 172, 173, 178, 201], "topk_lower_quartile_cumulative_distribution_op": [113, 114], "visualize_policy_value_for_valid": [113, 114, 172, 173, 178], "share_ax": [113, 114, 172, 178, 188, 201], "validation_policy_value_standard_op": [113, 114], "ax": [113, 114, 173, 182], "visualize_policy_value_of_cumulative_distribution_ope_for_valid": [113, 114, 172, 173], "validation_policy_value_cumulative_distribution_op": [113, 114], "visualize_policy_value_lower_bound_for_valid": [113, 114, 172, 178], "validation_policy_value_lower_bound": [113, 114], "visualize_variance_for_valid": [113, 114, 172, 173, 178, 188, 201], "validation_vari": [113, 114], "visualize_lower_quartile_for_valid": [113, 114, 172, 173], "validation_lower_quartil": [113, 114], "visualize_conditional_value_at_risk_for_valid": [113, 114, 172, 173, 178], "validation_conditional_value_at_risk": [113, 114], "continuousdicestateactionwightvaluelearn": 115, "q_function": [115, 116, 118, 119, 130, 131, 133, 134, 172, 184], "w_function": [115, 116, 117, 118, 119, 120, 136, 137, 138, 139, 140, 141], "best_dic": [115, 116, 117, 118, 119, 120, 174], "batch_siz": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172], "128": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "q_lr": [115, 116, 118, 119, 174], "0001": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "w_lr": [115, 116, 117, 118, 119, 120, 174], "lambda_lr": [115, 116, 117, 118, 119, 120], "alpha_q": [115, 116, 117, 118, 119, 120], "alpha_w": [115, 116, 117, 118, 119, 120], "alpha_r": [115, 116, 117, 118, 119, 120], "enable_lambda": [115, 116, 117, 118, 119, 120], "baseweightvaluelearn": [115, 116, 117, 118, 119, 120, 121, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "relax": [115, 116, 117, 118, 119, 120], "primal": [115, 116, 117, 118, 119, 120], "dual": [115, 116, 117, 118, 119, 120], "theori": [115, 116, 117, 118, 119, 120], "behind": [115, 116, 117, 118, 119, 120, 172, 192], "max_": [115, 116, 117, 118, 119, 120, 172, 186, 192, 198], "min_": [115, 116, 117, 118, 119, 120, 186], "lambda": [115, 116, 117, 118, 119, 120, 186], "special": [115, 116, 117, 118, 119, 120], "dualdic": [115, 116, 117, 118, 119, 120, 171, 172, 182, 189], "gendic": [115, 116, 117, 118, 119, 120, 171, 172, 182, 189], "gradientdic": [115, 116, 117, 118, 119, 120, 171, 182, 189], "algaedic": [115, 116, 117, 118, 119, 120, 171, 172, 182, 189], "bestdic": [115, 116, 117, 118, 119, 120, 171, 172, 182], "benefici": [115, 116, 117, 118, 119, 120, 171, 172, 187, 188], "adversari": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172], "manner": [115, 116, 117, 118, 119, 120, 175, 178, 179, 184, 186, 188, 198], "sinc": [115, 116, 117, 118, 119, 120, 171, 172, 186, 192], "convex": [115, 116, 117, 118, 119, 120], "instabl": [115, 116, 117, 118, 119, 120], "geq": [115, 116, 117, 118, 119, 120, 172, 192], "impos": [115, 116, 117, 118, 119, 120, 186], "continuousqfunct": [115, 116, 123, 130, 131], "continuousstateactionweightfunct": [115, 116, 123, 136, 137], "dual_dic": [115, 116, 117, 118, 119, 120, 172], "gen_dic": [115, 116, 117, 118, 119, 120], "algae_dic": [115, 116, 117, 118, 119, 120], "own": [115, 116, 117, 118, 119, 120, 171, 172, 188, 193, 196, 198], "1e": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 174, 175], "lambda_": [115, 116, 117, 118, 119, 120], "boolean": [115, 116, 117, 118, 119, 120], "shangtong": [115, 116, 117, 118, 119, 120, 189], "shimon": [115, 116, 117, 118, 119, 120, 189], "whiteson": [115, 116, 117, 118, 119, 120, 189], "rethink": [115, 116, 117, 118, 119, 120, 189], "ruiyi": [115, 116, 117, 118, 119, 120, 189], "ilya": [115, 116, 117, 118, 119, 120, 189], "kostrikov": [115, 116, 117, 118, 119, 120, 189], "yinlam": [115, 116, 117, 118, 119, 120, 189], "chow": [115, 116, 117, 118, 119, 120, 189], "arbitrari": [115, 116, 117, 118, 119, 120, 189], "experi": [115, 116, 117, 118, 119, 120, 171, 180, 182, 189, 192], "agnost": [115, 116, 117, 118, 119, 120, 189], "load": [115, 116, 117, 118, 119, 120, 121, 122, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 153, 154], "path_q": [115, 116, 118, 119], "path_w": [115, 116, 117, 118, 119, 120], "chose": [115, 116], "epoch": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "predict_q_funct": [115, 116, 118, 119, 130, 131, 133, 134], "q_valu": [115, 116, 117, 118, 119, 120, 130, 131, 133, 134], "predict_v_funct": [115, 116, 118, 119, 130, 131, 132, 133, 134, 135], "v_function": [115, 116, 117, 118, 120, 130, 131, 132, 133, 135], "predict_valu": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 142, 143, 144, 145, 146, 147, 148, 149], "predict_weight": [115, 116, 117, 118, 119, 120, 136, 137, 138, 139, 140, 141], "w_hat": [115, 116, 117, 118, 119, 120], "fit_predict": [115, 116, 117, 118, 119, 120, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "continuousdicestatewightvaluelearn": 115, "v_lr": [115, 117, 118, 120], "alpha_v": [115, 117, 118, 120], "aim": [115, 117, 118, 120, 142, 147, 171, 172, 182, 184, 186, 187, 192, 198], "rather": [115, 117, 118, 120, 142, 145, 146, 148, 149, 171, 172, 184, 187], "w_a": [115, 117, 118, 120, 136, 138, 139, 141], "analog": [115, 117, 118, 120], "although": [115, 117, 118, 120], "vfunction": [115, 117, 118, 120, 123], "stateweightfunct": [115, 117, 118, 120, 123, 136, 138, 139, 141], "path_v": [115, 117, 118, 120], "kawrg": [115, 117], "discretedicestateactionwightvaluelearn": 118, "discreteaugmentedlagrangianstateactionwightvaluelearn": [118, 119], "discreteqfunct": [118, 119, 123, 130, 132, 133, 134, 135], "discretestateactionweightfunct": [118, 119, 123, 139, 140], "predict_q_function_for_all_act": [118, 119, 133, 134], "discretedicestatewightvaluelearn": 118, "discreteaugmentedlagrangianstatewightvaluelearn": [118, 120], "nn": [123, 124, 125, 126, 127, 128, 129], "network": [123, 124, 125, 126, 127, 128, 129], "enable_gradient_revers": [123, 125, 127, 128], "revers": [123, 125, 127, 128, 150, 151, 152], "layer": [123, 125, 127, 128], "loss": [123, 125, 127, 128, 186], "maxim": [123, 125, 127, 128, 172, 186, 196, 198], "argmax": [123, 126], "continuousminimaxstateactionvaluelearn": 130, "lr": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172], "hold": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 150, 164, 165, 172], "minim": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172, 174, 175, 186], "rh": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "lh": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "l_q": [130, 131, 133, 134], "discrimin": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "hilbert": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172], "rkh": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172], "max_q": [130, 131, 133, 134], "tild": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172], "_t": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 172], "regularization_weight": [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "continuousminimaxstatevaluelearn": 130, "l_v": [130, 132, 133, 135], "max_v": [130, 132, 133, 135], "discreteminimaxstateactionvaluelearn": 133, "discreteminimaxstatevaluelearn": 133, "continuousminimaxstateactionweightlearn": 136, "continuousminimaxstateactionwightlearn": [136, 137], "l_w": [136, 137, 138, 139, 140, 141], "max_w": [136, 137, 138, 139, 140, 141], "_0": [136, 137, 138, 139, 140, 141], "importance_weight": [136, 137, 138, 139, 140, 141], "continuousminimaxstateweightlearn": 136, "continuousminimaxstatewightlearn": [136, 138], "predict_state_marginal_importance_weight": [136, 138, 139, 141], "predict_state_action_marginal_importance_weight": [136, 138, 139, 141], "discreteminimaxstateactionweightlearn": 139, "discreteminimaxstateactionwightlearn": [139, 140], "discreteminimaxstateweightlearn": 139, "discreteminimaxstatewightlearn": [139, 141], "greedi": [142, 143, 145, 182, 184, 186], "To": [142, 143, 144, 145, 146, 147, 148, 149, 172, 173, 174, 175, 176, 178, 179, 181, 184, 186, 187, 192, 198], "ensur": [142, 143, 144, 145, 146, 147, 148, 149], "inherit": [142, 143, 144, 145, 146, 147, 148, 149], "sample_action_and_output_pscor": [142, 143, 145, 146, 147, 148, 149], "calc_action_choice_prob": [142, 143, 145, 147, 148, 184], "calc_pscore_given_act": [142, 143, 145, 146, 147, 148, 149, 184], "predict_value_onlin": [142, 143, 184], "with_std": [142, 143], "sample_action_and_output_pscore_onlin": [142, 143, 184], "choos": [142, 145, 148, 171, 176, 180, 186, 187, 188, 192, 198], "take": [142, 145, 171, 176, 184, 186, 187, 192, 193, 196, 198], "thu": [142, 145, 146, 148, 149, 171, 186, 192], "softmaxhead": [142, 184], "tau": [142, 148, 172, 184, 186, 187, 188], "softmax": [142, 148, 182, 184], "temperatur": [142, 148, 184], "lead": [142, 148, 172, 186], "sub": [142, 148, 171, 186, 188, 189, 193, 196, 198, 201], "gaussianhead": [142, 184], "truncatedgaussianhead": [142, 146, 184], "truncnorm": [142, 149], "continuousevalhead": [142, 184], "add": [150, 153, 154, 172, 176, 178, 184, 196], "n_eval_polici": [150, 153, 178], "use_same_eval_policy_across_dataset": [150, 153, 178], "l2_distanc": 150, "calcil": [150, 168], "l2": [150, 168], "distanc": [150, 168, 186], "n_dim": [150, 160, 162, 167, 168, 169, 170], "gaussian_kernel": 150, "kernel_dens": [150, 160, 162, 167, 169, 170], "densiti": [150, 160, 162, 167, 169, 170, 172, 176], "triangular_kernel": 150, "trianglar": [150, 160, 162, 169, 170], "epanechnikov_kernel": 150, "cosine_kernel": 150, "uniform_kernel": 150, "estimate_confidence_interval_by_bootstrap": 150, "estimate_confidence_interval_by_hoeffd": 150, "inequ": [150, 164, 165, 174, 176, 179], "x_": [150, 164, 165], "sqrt": [150, 164, 165, 166, 172, 176, 186, 192], "estimate_confidence_interval_by_empirical_bernstein": 150, "estimate_confidence_interval_by_t_test": 150, "student": [150, 166, 171, 172, 176, 182], "assum": [150, 166, 172, 173, 174, 175, 178, 179, 186], "assumpt": [150, 166, 172], "deriv": [150, 166, 171, 172, 174, 176, 186, 187, 188, 200], "t_": [150, 166, 172], "respect": [150, 166, 172, 173, 179, 186, 198], "defaultdict_to_dict": 150, "dict_": [150, 161], "defaultdict": [150, 161], "check_logged_dataset": 150, "check_input_dict": 150, "newgymapiwrapp": [150, 181], "old": [150, 155, 156], "26": [150, 155, 156, 171, 172, 181, 182, 189, 192], "new": [150, 155, 156, 171, 172, 176, 182, 184, 186, 187, 188, 192], "ones": [150, 155, 156, 171], "oldgymapiwrapp": [150, 181], "tempor": [150, 151, 152, 186], "support": [150, 151, 152, 171, 173, 174, 175, 179, 182, 186, 187, 188, 200, 201], "numpi": [150, 151, 152, 193, 196, 198], "entri": [150, 151, 152], "transform_numpi": [150, 151, 152], "fit_with_env": [150, 151, 152], "tensor": [150, 151, 152], "reverse_transform": [150, 151, 152], "reverse_transform_numpi": [150, 151, 152], "get_param": [150, 151, 152], "param": [150, 151], "flag": [150, 151, 152], "deepcopi": [150, 151], "minmaxscal": [150, 174, 175], "deepli": [150, 152], "copi": [150, 152], "get_typ": [151, 152], "sequenti": [171, 172], "ubiquit": 171, "world": [171, 173, 174, 175, 176, 178, 179, 184, 186, 188, 189, 201], "valuabl": [171, 192], "futur": [171, 172, 192], "engin": 171, "respons": 171, "most": [171, 172, 186, 192], "counterfactu": [171, 182, 186, 187, 192], "particularli": [171, 172, 184, 186, 192, 193, 196, 198, 200], "effect": [171, 172, 186, 192], "substitut": [171, 172, 187], "b": [171, 182, 188, 192], "studi": [171, 189, 192], "grow": 171, "interest": [171, 172, 187, 188, 192], "especi": [171, 186, 192], "theoret": [171, 187], "been": [171, 186], "propos": [171, 173, 183, 192, 201], "recent": [171, 172], "insight": [171, 182, 192], "perspect": [171, 189], "crucial": [171, 186], "unfortun": [171, 192], "despit": 171, "few": 171, "extens": [171, 182], "lack": [171, 192], "plaform": 171, "limit": [171, 186], "flexibli": [171, 172], "framework": [171, 186], "focu": [171, 192], "account": [171, 192], "friendli": [171, 200], "interpret": 171, "well": [171, 172, 186, 187, 192], "It": [171, 172, 174, 181, 182, 184, 186, 192], "critic": [171, 182, 189, 192], "fill": 171, "gap": 171, "further": [171, 172, 184, 186, 187], "facilit": [171, 182, 186], "research": [171, 182, 187, 189, 192, 193, 196, 198], "build": 171, "put": 171, "emphasi": 171, "distinct": [171, 182, 188], "our": [171, 183, 184, 187, 192, 193, 196, 198, 201], "advantag": [171, 186, 192], "quick": 171, "exis": 171, "section": [171, 184, 188, 192], "flexibl": [171, 172, 182], "bridg": 171, "streamlin": [171, 172, 182, 184, 193, 196, 198], "mainli": [171, 188], "four": [171, 172, 184, 192, 193, 196, 198], "bottom": [171, 200], "workflow": [171, 182, 188, 192], "orl": [171, 184], "environment": [171, 193, 196, 198], "algorithm": [171, 172, 174, 175, 182, 184, 186, 188, 189, 193, 196, 198], "experiment": [171, 182], "easi": [171, 174, 182, 184], "alreadi": [171, 184, 192], "friedli": 171, "therefor": [171, 172, 181, 186, 192], "smoothli": [171, 184], "connect": 171, "opl": [171, 172, 184], "final": [171, 172, 173, 174, 175, 176, 178, 179, 180, 182, 184, 186, 187, 188, 192, 193, 196, 198, 200], "As": [171, 172, 186, 192], "review": [171, 186, 187, 189], "14": [171, 172, 182, 189, 198], "cut": 171, "edg": 171, "quickli": 171, "help": [171, 186], "practition": [171, 182, 193, 196, 198], "comprehens": [171, 186, 187, 192], "detail": [171, 172, 174, 182, 184, 187, 188, 192], "descript": [171, 184, 186], "implemet": 171, "17": [171, 182, 189], "22": [171, 172, 182, 189], "23": [171, 172, 182, 189], "spearman": [171, 172, 182], "paper": [171, 182, 183, 186, 187, 192, 193, 196, 198, 201], "discuss": [171, 175, 187, 192, 200], "briefli": 171, "later": [171, 184], "besid": [171, 186], "differenti": 171, "roughli": 171, "just": [171, 174, 184], "doe": [171, 176, 181, 186], "know": 171, "life": 171, "situat": [171, 187, 188, 192, 195], "stabli": 171, "good": 171, "qualiti": [171, 172, 186, 200], "sometim": [171, 172, 186, 187, 192], "extrem": 171, "hurt": 171, "satisfact": 171, "serious": 171, "bad": 171, "diriv": 171, "car": 171, "catastroph": 171, "accid": 171, "avoid": [171, 192], "even": [171, 172, 184, 186, 192], "less": [171, 172], "believ": 171, "releas": [171, 182, 185], "boost": 171, "uniqu": 171, "erron": [171, 192], "cannot": [171, 192], "sole": [171, 192], "reli": [171, 184, 192], "begin": [171, 172, 192], "filter": [171, 172, 192], "poor": [171, 192, 201], "identifi": [171, 192], "reliabl": [171, 182, 188, 192], "illustr": [171, 188, 192], "convent": [171, 177, 187, 192], "measur": [171, 172, 182, 192], "mse": [171, 172, 173, 192], "28": [171, 187, 189], "29": [171, 189, 192], "worsk": 171, "main": [171, 182, 187, 189, 192], "proposs": [171, 182], "comparion": 171, "criteria": 171, "colum": 171, "31": [171, 189, 192], "particular": [171, 172, 186, 192, 198], "32": [171, 189], "cd": [171, 175, 176, 178, 179, 182, 183, 200], "abbrevi": 171, "metic": 171, "benckmark": 171, "hope": 171, "serv": 171, "mileston": 171, "develop": [171, 182, 186, 192], "corl": [171, 189], "33": [171, 189], "rllib": [171, 189], "34": [171, 189], "35": [171, 189], "testb": 171, "neorl": [171, 189], "36": [171, 189], "37": [171, 189], "rl4r": [171, 189], "38": [171, 189], "auctiongym": [171, 189], "39": [171, 189], "dope": 171, "cob": 171, "obp": 171, "40": [171, 189], "remark": 171, "highli": 171, "inspir": [171, 189, 192], "openbanditpipelin": 171, "demonstr": [171, 172, 192, 200], "success": 171, "prototyp": 171, "toolkit": 171, "prev": [171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 183, 184, 186, 187, 188, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200], "back": [171, 186, 191, 193, 194, 196, 197, 198, 199], "formul": [171, 172, 184, 186, 192, 193, 196, 198], "proceed": [172, 182, 189], "try": [172, 201], "multiple_input_dict": [172, 178], "nest": [172, 184], "encod": [172, 184], "vectorencoderfactori": [172, 174, 175, 184], "meanqfunctionfactori": [172, 174, 175, 184], "encoder_factori": [172, 174, 175, 184], "hidden_unit": [172, 174, 175, 184], "q_func_factori": [172, 174, 175, 184], "learning_r": [172, 174, 175], "64": 172, "createinputdict": 172, "logic": 172, "goal": [172, 184, 186, 187, 188, 198], "need": [172, 174, 175, 184, 186, 187, 188], "re": [172, 198], "turn": 172, "notat": [172, 188], "ope_dict": 172, "effort": [172, 178], "multiple_ope_dict": 172, "somewhat": [172, 186], "star": [172, 192], "vari": [172, 192], "color": 172, "affect": [172, 174, 200], "code": [172, 182, 184, 187, 188, 193, 196, 198, 200, 201], "my": 172, "common": 172, "want": 172, "them": 172, "contribut": [172, 176, 187, 192, 201], "welcom": [172, 176, 182, 183, 193, 196, 198], "read": [172, 176], "guidelin": [172, 176, 182, 184, 188, 193, 196, 198, 201], "md": [172, 176, 182, 193, 196, 198, 201], "explain": [172, 182, 184], "approach": [172, 186, 189], "leverag": [172, 186, 187], "techniqu": 172, "natur": [172, 189], "ldot": [172, 186], "past": [172, 186], "hybrid": [172, 174, 175, 180, 186], "introduc": 172, "baselin": [172, 188, 192], "recurs": [172, 186], "form": [172, 173, 192], "residu": 172, "reduct": [172, 189], "purpos": [172, 184], "version": [172, 181, 192], "ast": [172, 198], "exponenti": 172, "latter": [172, 192], "part": [172, 187], "allevi": [172, 186], "someth": 172, "s_1": 172, "s_2": 172, "alwai": [172, 192], "marigin": 172, "smi": [172, 174, 177], "smdr": [172, 174, 177], "similarli": [172, 174, 175, 178, 186, 200], "sami": [172, 174, 177], "samdr": [172, 174, 177], "w_t": 172, "domin": 172, "relationship": 172, "d_": 172, "q_": [172, 186], "middl": 172, "ident": 172, "express": 172, "notic": [172, 192], "slightli": [172, 174], "question": [172, 182, 186, 187, 193, 196, 198, 201], "would": [172, 192], "possibl": [172, 174, 178, 184], "potenti": [172, 186, 192, 196, 198, 201], "let": [172, 186, 187, 192, 193, 196, 198], "interpol": [172, 174], "estimation_dict": 172, "equival": 172, "optimist": 172, "overestim": [172, 186, 192], "displaystyl": [172, 184], "conserv": [172, 182, 189, 192], "hand": [172, 173, 186, 192, 200], "naiv": [172, 174, 176, 186], "reject": 172, "almost": [172, 192, 200], "everi": [172, 186, 198], "address": [172, 186], "issu": [172, 182, 186], "57": [172, 189], "58": [172, 189, 192], "overlin": 172, "h": 172, "meet": 172, "xk": 172, "dx": 172, "co": 172, "multi": [172, 186], "dot": 172, "spefici": 172, "cumulativedistributionoffpolicyevalu": 172, "cd_dm": [172, 175, 188, 201], "cd_dr": [172, 188, 201], "cdf_dict": [172, 175, 178], "multiple_cdf_dict": 172, "contrari": 172, "ponit": 172, "adopt": 172, "vulner": 172, "consequ": [172, 176, 180], "variat": 172, "plan": 172, "softwar": [172, 182], "thei": [172, 186, 192], "cumulativedistributionsndr": [172, 188], "rake": 172, "preserv": [172, 187, 192], "surpass": 172, "eas": 172, "ranking_df": [172, 178, 179, 188], "safety_criteria": [172, 188], "obtain_oracle_selection_result": 172, "page": [173, 174, 175, 177, 178, 179, 182, 183, 184, 187, 192, 193, 201], "what": [173, 174, 175, 179, 186, 187], "protococol": 173, "unecessari": [173, 174, 175, 178, 179], "readi": [173, 179, 188], "dictionati": [173, 179], "createinput": 173, "simplic": [173, 174, 175, 179], "convension": 173, "rankcorr": [173, 192], "ranking_dict": [173, 179, 188], "metric_dict": 173, "portfolio": [173, 192], "find": [173, 182, 192], "amond": 173, "obtain_topk_conditional_value_at_risk_selected_by_cumulative_distirbution_op": [173, 178], "visualize_topk_conditional_value_at_risk_selected_by_cumulative_distirbution_op": [173, 178], "again": [173, 175, 192, 200], "sure": [174, 175], "suffici": [174, 175], "use_gpu": [174, 175], "is_avail": [174, 175], "k_hold": 174, "util": [174, 175, 178, 181, 184, 198], "smsni": 174, "smsndr": 174, "samsni": 174, "samsndr": 174, "complet": 174, "easili": [174, 175, 188], "offer": [174, 182, 192], "disabl": 174, "fig": 174, "bandwith": 174, "veri": [175, 186, 192], "capabl": 175, "cd_ti": 175, "cd_tdr": 175, "cd_snti": 175, "cd_sntdr": 175, "curv": 175, "bin": [175, 176, 178], "simular": 175, "accompani": 175, "esitm": 175, "cvar_dict": [175, 188], "side": [175, 176, 179, 180], "depict": 175, "off_polici": [175, 184, 186], "etim": 176, "dataclass": [176, 193, 196, 198], "naiveaverageestim": 176, "naive_averag": 176, "def": [176, 193, 196, 198], "__post__init__": 176, "reshap": 176, "full": 176, "cumprod": 176, "estimated_trajectory_valu": 176, "newaxi": 176, "sum": 176, "overrid": [176, 182, 184], "discretetrajectorywiseimportancesampl": 176, "estimate_condifence_interval_by_ttest": 176, "len": [176, 198], "scipi": 176, "stat": 176, "ppf": 176, "ddof": 176, "customhighconfidenceti": 176, "callabl": 176, "custom_ci": 176, "estimate_confidence_interval_by_ttest": 176, "auxiliari": 176, "disctionari": [176, 180], "actual": [176, 180, 198], "naivecumulativedistributionestim": 176, "naive_cdf": 176, "trajectory_reward": 176, "sort_idx": 176, "trajectory_wise_reward": 176, "argsort": 176, "sorted_importance_weight": 176, "trajectory_wise_importance_weight": 176, "cumulative_dens": 176, "cumsum": 176, "histogram": 176, "astyp": [176, 198], "insert": 176, "diff": 176, "zeros_lik": 176, "idx_": 176, "nonzero": 176, "elif": 176, "els": 176, "lower_idx_": 176, "relative_probability_dens": 176, "median_idx_": 176, "upper_idx_": 176, "median": 176, "extent": 177, "integr": [177, 201], "multiple_logged_dataset": [178, 184], "seeed": 178, "manual": [178, 184], "single_logged_dataset": [178, 184], "breviti": 178, "essenti": 178, "accessbl": 178, "single_input_dict": 178, "emploi": [178, 192], "point": [178, 186, 200], "visualize_interquartile_range_with_multiple_estim": 178, "metric_df_dict": 178, "topk_metric_df_dict": 178, "funnction": 178, "behavipr": 178, "visualize_lower_bound_for_valid": 178, "real_world": 178, "With": 179, "verifi": 179, "oracle_selection_dict": 179, "oracle_selection_df": 179, "pf": 180, "apllic": 180, "feasibl": 180, "whrea": 180, "xxx": [181, 182, 183, 185, 192, 193, 201], "ai": 181, "act": [181, 193, 196, 198], "older": 181, "solv": 181, "incompat": 181, "ofrl": 181, "latest": 181, "xxx_v0": 181, "env_": [181, 198], "python": [182, 183, 193, 196, 198], "platform": [182, 189, 193, 196, 198], "straightforward": 182, "incorpor": 182, "seri": [182, 184], "topic": 182, "divers": 182, "typic": [182, 188, 192], "gentl": 182, "introduct": 182, "concept": 182, "varieti": [182, 188, 201], "why": [182, 186, 188, 201], "traincandidatepolici": [182, 184], "beyond": 182, "mere": 182, "protocol": [182, 187, 188, 201], "firstli": 182, "secondli": 182, "motiv": [182, 192], "configur": [182, 189], "recgym": [182, 195], "commerc": [182, 195], "basicgym": [182, 195], "cite": [182, 183, 192, 193, 196, 198, 201], "haruka": [182, 183, 192, 193, 196, 198, 201], "kiyohara": [182, 183, 192, 193, 196, 198, 201], "ren": [182, 183, 192, 193, 201], "kishimoto": [182, 183, 192, 193, 201], "kosuk": [182, 183, 192, 193, 196, 198, 201], "kawakami": [182, 183, 192, 193, 196, 198, 201], "ken": [182, 183, 189, 192, 193, 201], "kobayashi": [182, 183, 192, 193, 201], "kazuhid": [182, 183, 192, 193, 201], "nakata": [182, 183, 192, 193, 201], "preprint": [182, 183, 189, 192, 193, 196, 198, 201], "come": [182, 183, 192, 193, 201], "soon": [182, 183, 192, 193, 201], "articl": [182, 183, 192, 193, 196, 198, 201], "kiyohara2023toward": [182, 183, 192, 193, 201], "author": [182, 183, 192, 193, 196, 198, 201], "nataka": [182, 183, 192, 193, 201], "titl": [182, 183, 192, 193, 196, 198, 201], "journal": [182, 183, 189, 192, 193, 196, 198, 201], "github": [182, 183, 189, 192, 193, 201], "repositori": [182, 183, 192, 193, 201], "year": [182, 183, 192, 193, 196, 198, 201], "2023": [182, 183, 192, 193, 201], "feel": [182, 193, 196, 198, 201], "googlegroup": [182, 185, 201], "com": [182, 183, 185, 201], "hk844": [182, 193, 196, 198, 201], "cornel": [182, 193, 196, 198, 201], "edu": [182, 193, 196, 198, 201], "project": [182, 193, 196, 198], "instal": [182, 188, 201], "exist": [182, 196], "actor": [182, 189], "extrapol": 182, "diverg": [182, 192], "clone": [182, 183], "uncertainti": 182, "implicit": [182, 189], "discretehead": 182, "continuoushead": 182, "hcope": 182, "background": 182, "galleri": [182, 201], "scopr": 182, "licens": 182, "faq": 182, "pypi": [183, 185], "pip": 183, "git": 183, "http": [183, 189], "hakuhodo": 183, "technologi": 183, "instanti": [184, 193, 196, 198], "head": [184, 188], "eps_03": 184, "sigma_10": 184, "leav": 184, "unnecessari": 184, "logged_dataset_": 184, "synthetic_dataset": 184, "multiple_logged_dataset_1": 184, "behavior_policy_1": 184, "behavior_policy_2": 184, "multiple_logged_dataset_2": 184, "discretecql": [184, 188], "cql": [184, 186, 188], "offlinerl_dataset": [184, 188], "train_episod": 184, "test_episod": 184, "train_test_split": 184, "test_siz": 184, "eval_episod": 184, "alreaadi": 184, "simpl": [184, 198], "smoothen": 184, "cql_b1": 184, "cql_b2": 184, "cql_b3": 184, "fitting_arg": 184, "learn_base_polici": 184, "policy_wrapp": 184, "eps_00": 184, "eps_07": 184, "eval_polici": 184, "apply_head": 184, "base_policies_nam": 184, "obtain_evaluation_polici": 184, "algorithms_nam": 184, "cql_b1_eps_00": 184, "cql_b1_eps_03": 184, "cql_b1_eps_07": 184, "cql_b1_softmax": 184, "cql_b2_eps_00": 184, "cql_b2_eps_03": 184, "cql_b2_eps_07": 184, "cql_b2_softmax": 184, "cql_b3_eps_00": 184, "cql_b3_eps_03": 184, "cql_b3_eps_07": 184, "cql_b3_softmax": 184, "evalhead": 184, "role": 184, "pi_": [184, 186], "det": 184, "degre": [184, 186], "construct": 184, "truncatednorm": 184, "calc_on_policy_policy_interquartile_rang": 184, "googl": [185, 201], "group": [185, 201], "xx": 185, "preval": 186, "formal": [186, 187], "langl": [186, 187, 193, 196, 198], "p_r": [186, 187, 193, 196, 198], "rangl": [186, 187, 193, 196, 198], "d_0": [186, 187], "p_": 186, "41": [186, 189], "42": [186, 189], "43": [186, 189], "45": [186, 189], "46": [186, 189], "One": [186, 198], "ascent": 186, "theta_": 186, "leftarrow": 186, "eta": 186, "nabla": 186, "theta_k": 186, "_n": 186, "benefit": 186, "ineffici": 186, "unstabl": [186, 200], "pursu": 186, "amount": 186, "replai": 186, "tau_": 186, "structur": 186, "bellman": 186, "td": 186, "_k": 186, "pi_k": [186, 192], "randomli": 186, "though": [186, 192], "enhanc": [186, 192], "known": 186, "deadli": [186, 189], "triad": [186, 189], "47": [186, 189], "fail": [186, 192], "Such": 186, "seen": 186, "stabil": [186, 189], "suitabl": 186, "power": 186, "entail": 186, "phase": 186, "48": [186, 189], "huge": 186, "robot": 186, "49": [186, 189], "overcom": 186, "ingredi": 186, "answer": [186, 187], "bias": 186, "heavili": [186, 200], "tackl": 186, "deal": [186, 187], "appar": 186, "seem": 186, "investig": 186, "recal": 186, "propto": 186, "target": 186, "problemat": 186, "tend": 186, "coincident": 186, "higher": [186, 192], "propag": 186, "eventu": 186, "detriment": [186, 192], "aforement": 186, "mitig": 186, "brac": 186, "51": [186, 189], "discrep": [186, 192], "kl": 186, "wassertein": 186, "forc": 186, "understim": 186, "explicitli": [186, 192], "generaliz": 186, "penalti": 186, "keep": 186, "too": 186, "adequ": 186, "modal": 186, "imit": 186, "td3": 186, "bc": 186, "52": [186, 189], "strong": [186, 192], "whilst": 186, "promot": 186, "predefin": 186, "intuit": 186, "unreli": 186, "simpli": 186, "penal": 186, "bear": 186, "53": [186, 189], "pess": 186, "pessimist": 186, "ensembl": 186, "pessim": 186, "_m": 186, "balanc": 186, "combo": [186, 189], "54": [186, 189], "exploit": 186, "ood": 186, "sacrific": 186, "explicit": 186, "iql": 186, "55": [186, 189], "asymmetr": 186, "psi": 186, "l_2": 186, "distinctli": 186, "z": [186, 192], "prevent": 186, "coverag": 186, "expertis": 186, "greatli": [186, 192, 200], "taxonomi": [186, 189], "reader": [186, 187], "survei": [186, 187, 189], "56": [186, 189], "awesom": [186, 187], "literatur": [186, 187, 192], "challeng": 187, "togeth": 187, "implementaion": 187, "hopefulli": 188, "better": [188, 192], "start": [188, 201], "screen": 188, "promis": 188, "overview": [188, 192], "update_start_step": 188, "train_logged_dataset": 188, "test_logged_dataset": 188, "now": [188, 192], "scorer": 188, "si": 188, "cql_": 188, "sensit": 188, "cumulativedistributionsni": 188, "cd_sndr": [188, 201], "cdf_dr": [188, 201], "cdf_sndr": [188, 201], "ranking_dict_": 188, "hado": 189, "van": 189, "hasselt": 189, "arthur": 189, "guez": 189, "silver": 189, "aaai": 189, "confer": 189, "artifici": 189, "intellig": 189, "2094": 189, "2100": 189, "aurick": 189, "neural": 189, "1179": 189, "1191": 189, "15th": 189, "acm": 189, "sigkdd": 189, "intern": 189, "knowledg": 189, "discoveri": 189, "mine": 189, "129": 189, "138": 189, "36th": 189, "97": 189, "3703": 189, "3712": 189, "pmlr": 189, "17th": 189, "759": 189, "766": 189, "33rd": 189, "652": 189, "661": 189, "2139": 189, "2148": 189, "23714": 189, "23726": 189, "roceed": 189, "25th": 189, "5022": 189, "5050": 189, "27475": 189, "27490": 189, "3325": 189, "3334": 189, "37th": 189, "9659": 189, "9668": 189, "18958": 189, "18969": 189, "6551": 189, "6561": 189, "11194": 189, "11203": 189, "8th": 189, "represent": 189, "arxiv": [189, 192, 196, 198], "1912": 189, "02074": 189, "2380": 189, "2388": 189, "31st": 189, "2007": 189, "09055": 189, "9th": 189, "39th": 189, "11729": 189, "11752": 189, "chengchun": 189, "shi": 189, "2212": 189, "06355": 189, "shayan": 189, "doroudi": 189, "fair": 189, "grante": 189, "submiss": [189, 192], "1606": 189, "01540": 189, "2111": 189, "03788": 189, "deni": 189, "tarasov": 189, "nikulin": 189, "dmitri": 189, "akimov": 189, "orient": 189, "3rd": 189, "workshop": 189, "launchpad": 189, "eric": 189, "liang": 189, "liaw": 189, "robert": 189, "nishihara": 189, "philipp": 189, "moritz": 189, "roi": 189, "fox": 189, "goldberg": 189, "joseph": 189, "gonzalez": 189, "jordan": 189, "ion": 189, "stoica": 189, "proce": 189, "35th": 189, "3053": 189, "3062": 189, "jason": 189, "gauci": 189, "edoardo": 189, "conti": 189, "yitao": 189, "kittipat": 189, "virochsiri": 189, "yuchen": 189, "kaden": 189, "vivek": 189, "narayanan": 189, "xiaohui": 189, "ye": 189, "zhengx": 189, "fujimoto": 189, "facebook": 189, "1811": 189, "00260": 189, "rongjun": 189, "qin": 189, "songyi": 189, "gao": 189, "xingyuan": 189, "zhen": 189, "shengkai": 189, "zewen": 189, "weinan": 189, "yu": 189, "1808": 189, "00720": 189, "kai": 189, "zhene": 189, "zou": 189, "qilin": 189, "deng": 189, "shang": 189, "minghao": 189, "runz": 189, "xudong": 189, "shen": 189, "tangji": 189, "lyu": 189, "changji": 189, "fan": 189, "2110": 189, "11073": 189, "olivi": 189, "jeunen": 189, "sean": 189, "murphi": 189, "ben": 189, "allison": 189, "kdd": 189, "comput": [189, 192], "adkdd": 189, "url": 189, "www": 189, "amazon": 189, "scienc": 189, "public": [189, 193, 196, 198], "sham": 189, "kakad": 189, "2001": 189, "gui": 189, "lever": 189, "nicola": 189, "heess": 189, "degri": 189, "daan": 189, "wierstra": 189, "martin": 189, "riedmil": 189, "31th": 189, "387": 189, "395": 189, "2014": 189, "christoph": 189, "jch": 189, "watkin": 189, "dayan": 189, "279": 189, "292": 189, "1992": 189, "volodymyr": 189, "mnih": 189, "korai": 189, "kavukcuoglu": 189, "alex": 189, "grave": 189, "ioanni": 189, "antonogl": 189, "plai": 189, "atari": 189, "1312": 189, "5602": 189, "2013": 189, "vijai": 189, "konda": 189, "tsitsikli": 189, "1999": 189, "martha": 189, "white": 189, "29th": 189, "cofer": 189, "179": 189, "2012": 189, "yotam": 189, "doron": 189, "florian": 189, "strub": 189, "matteo": 189, "hessel": 189, "sonnerat": 189, "modayil": 189, "1812": 189, "02648": 189, "2005": 189, "01643": 189, "tatsuya": 189, "matsushima": 189, "hiroki": 189, "furuta": 189, "yutaka": 189, "matsuo": 189, "shixiang": 189, "gu": 189, "meger": 189, "2052": 189, "2062": 189, "yifan": 189, "1911": 189, "11361": 189, "shane": 189, "minimalist": 189, "20132": 189, "20145": 189, "matthew": 189, "soh": 189, "tianh": 189, "rafael": 189, "rafailov": 189, "aravind": 189, "rajeswaran": 189, "chelsea": 189, "finn": 189, "28954": 189, "28967": 189, "ashvin": 189, "nair": 189, "06169": 189, "figueiredo": 189, "prudencio": 189, "marco": 189, "roa": 189, "maximo": 189, "esther": 189, "luna": 189, "colombini": 189, "2203": 189, "01387": 189, "21st": 189, "84": 189, "1243": 189, "1251": 189, "haanvid": 189, "jongmin": 189, "yunseon": 189, "choi": 189, "wonseok": 189, "jeon": 189, "byung": 189, "yung": 189, "kyun": 189, "noh": 189, "kee": 189, "eung": 189, "kim": 189, "local": 189, "xxxx": 189, "eugen": 189, "ie": 189, "hsu": 189, "mladenov": 189, "vihan": 189, "jain": 189, "sanmit": 189, "narvekar": 189, "jing": 189, "rui": 189, "craig": 189, "boutili": 189, "recsim": 189, "1909": 189, "04847": 189, "60": [189, 192], "xiao": 189, "hongyang": 189, "qian": 189, "runjia": 189, "liuq": 189, "bowen": 189, "dan": 189, "finrl": 189, "autom": 189, "stock": 189, "trade": 189, "quantit": 189, "financ": [189, 192], "2011": 189, "09607": 189, "61": [189, 196], "23rd": 189, "econom": 189, "795": 189, "816": 189, "strongli": 189, "spin": 189, "up": 189, "educ": 189, "resourc": 189, "subsequ": 192, "esim": 192, "downstream": 192, "task": 192, "shortcom": 192, "experienc": 192, "stage": 192, "toi": 192, "underestim": 192, "neglig": 192, "detect": 192, "413": 192, "sharperatio": [192, 200], "idea": 192, "draw": 192, "novel": 192, "textbf": 192, "text": [192, 198], "accord": 192, "precis": 192, "biggl": 192, "biggr": 192, "behav": 192, "denomin": 192, "understand": [192, 200, 201], "showcas": 192, "scenario": 192, "so": [192, 193, 196, 198], "previous": 192, "mention": 192, "distinguish": 192, "clearli": 192, "numer": 192, "separ": 192, "decomposit": 192, "substanti": 192, "polic": 192, "overlook": 192, "stake": 192, "anoth": 192, "involv": 192, "give": 192, "difficult": 192, "decid": 192, "079": 192, "023": 192, "rrt": [192, 200], "estimatorstak": 192, "greater": 192, "poorli": 192, "exhibit": 192, "moder": 192, "signifi": 192, "emerg": 192, "guidanc": 192, "appropri": 192, "prove": 192, "reacher": 192, "invertedpendulum": 192, "hopper": 192, "swimmer": 192, "mujuco": 192, "mountaincar": 192, "acrobot": 192, "classic": 192, "nmse": 192, "nregret": 192, "dark": 192, "red": 192, "line": [192, 193, 196, 198, 201], "acknowledg": 192, "mdr": 192, "mi": 192, "becaus": 192, "nearli": 192, "perfect": 192, "parallel": 192, "neglect": 192, "evid": 192, "successfulli": 192, "riski": 192, "ge": [192, 198], "safer": 192, "worth": 192, "wherea": 192, "weak": 192, "wors": 192, "until": 192, "captur": 192, "characterist": 192, "overal": 192, "suggest": [192, 200], "meaning": 192, "least": 192, "sophist": 192, "outperform": 192, "associ": 192, "adapt": 192, "mostli": 192, "continuousrandompolici": [193, 198], "stuff": [193, 196, 198], "customizedstatetransitionfunct": 193, "__post_init__": [193, 196, 198], "check_random_st": [193, 196, 198], "state_coef": 193, "loc": [193, 196, 198], "action_coef": 193, "linalg": [193, 196], "norm": [193, 196], "ord": [193, 196], "customizedrewardfunct": 193, "sub_packag": [193, 196, 198], "bed": 195, "back_to_top": 195, "offlinegym": 196, "matplotlib": [196, 198], "pyplot": [196, 198], "plt": [196, 198], "dimension": 196, "acceler": [196, 198], "kiyohara2021acceler": [196, 198], "2109": [196, 198], "08331": [196, 198], "kpi": 198, "bid_": 198, "hour": 198, "exceed": 198, "c_t": 198, "clicktroughr": 198, "5000": 198, "daya": 198, "union": 198, "customizedwinningpricedistribut": 198, "customizedclickthroughr": 198, "ad_coef": 198, "user_coef": 198, "ad_lat": 198, "user_lat": 198, "rand": 198, "auto": 198, "custom_env": 198, "furthermor": 200, "sac_sigma_0": 200, "found": 200, "kind": 200, "citat": 201, "join": 201, "contact": 201}, "objects": {"basicgym.envs.simulator": [[2, 0, 0, "-", "base"], [5, 0, 0, "-", "function"]], "basicgym.envs.simulator.base": [[3, 1, 1, "", "BaseRewardFunction"], [4, 1, 1, "", "BaseStateTransitionFunction"]], "basicgym.envs.simulator.base.BaseRewardFunction": [[3, 2, 1, "", "mean_reward_function"], [3, 2, 1, "", "sample_reward"]], "basicgym.envs.simulator.base.BaseStateTransitionFunction": [[4, 2, 1, "", "step"]], "basicgym.envs.simulator.function": [[6, 1, 1, "", "RewardFunction"], [7, 1, 1, "", "StateTransitionFunction"]], "basicgym.envs.simulator.function.RewardFunction": [[6, 2, 1, "", "mean_reward_function"], [6, 2, 1, "", "sample_reward"]], "basicgym.envs.simulator.function.StateTransitionFunction": [[7, 2, 1, "", "step"]], "basicgym.envs": [[0, 0, 0, "-", "synthetic"]], "basicgym.envs.synthetic": [[1, 1, 1, "", "BasicEnv"]], "basicgym.envs.synthetic.BasicEnv": [[1, 2, 1, "", "reset"], [1, 2, 1, "", "step"]], "recgym.envs": [[12, 0, 0, "-", "rec"]], "recgym.envs.rec": [[13, 1, 1, "", "RECEnv"]], "recgym.envs.rec.RECEnv": [[13, 2, 1, "", "reset"], [13, 2, 1, "", "step"]], "recgym.envs.simulator": [[14, 0, 0, "-", "base"], [16, 0, 0, "-", "function"]], "recgym.envs.simulator.base": [[15, 1, 1, "", "BaseUserModel"]], "recgym.envs.simulator.base.BaseUserModel": [[15, 2, 1, "", "reward_function"], [15, 2, 1, "", "user_preference_dynamics"]], "recgym.envs.simulator.function": [[17, 1, 1, "", "UserModel"]], "recgym.envs.simulator.function.UserModel": [[17, 2, 1, "", "reward_function"], [17, 2, 1, "", "user_preference_dynamics"]], "rtbgym.envs": [[18, 0, 0, "-", "rtb"], [20, 0, 0, "-", "wrapper_rtb"]], "rtbgym.envs.rtb": [[19, 1, 1, "", "RTBEnv"]], "rtbgym.envs.rtb.RTBEnv": [[19, 2, 1, "", "reset"], [19, 2, 1, "", "step"]], "rtbgym.envs.simulator": [[22, 0, 0, "-", "base"], [26, 0, 0, "-", "bidder"], [28, 0, 0, "-", "function"], [32, 0, 0, "-", "rtb_synthetic"]], "rtbgym.envs.simulator.base": [[23, 1, 1, "", "BaseClickAndConversionRate"], [24, 1, 1, "", "BaseSimulator"], [25, 1, 1, "", "BaseWinningPriceDistribution"]], "rtbgym.envs.simulator.base.BaseClickAndConversionRate": [[23, 2, 1, "", "calc_prob"], [23, 2, 1, "", "sample_outcome"]], "rtbgym.envs.simulator.base.BaseSimulator": [[24, 2, 1, "", "calc_and_sample_outcome"], [24, 2, 1, "", "generate_auction"], [24, 2, 1, "", "map_idx_to_features"]], "rtbgym.envs.simulator.base.BaseWinningPriceDistribution": [[25, 2, 1, "", "sample_outcome"]], "rtbgym.envs.simulator.bidder": [[27, 1, 1, "", "Bidder"]], "rtbgym.envs.simulator.bidder.Bidder": [[27, 2, 1, "", "auto_fit_scaler"], [27, 2, 1, "", "custom_set_reward_predictor"], [27, 2, 1, "", "custom_set_scaler"], [27, 2, 1, "", "determine_bid_price"], [27, 2, 1, "", "fit_reward_predictor"]], "rtbgym.envs.simulator.function": [[29, 1, 1, "", "ClickThroughRate"], [30, 1, 1, "", "ConversionRate"], [31, 1, 1, "", "WinningPriceDistribution"]], "rtbgym.envs.simulator.function.ClickThroughRate": [[29, 2, 1, "", "calc_prob"], [29, 2, 1, "", "sample_outcome"]], "rtbgym.envs.simulator.function.ConversionRate": [[30, 2, 1, "", "calc_prob"], [30, 2, 1, "", "sample_outcome"]], "rtbgym.envs.simulator.function.WinningPriceDistribution": [[31, 2, 1, "", "sample_outcome"]], "rtbgym.envs.simulator.rtb_synthetic": [[33, 1, 1, "", "RTBSyntheticSimulator"]], "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator": [[33, 1, 1, "", "ClickThroughRate"], [33, 1, 1, "", "ConversionRate"], [33, 1, 1, "", "WinningPriceDistribution"], [33, 2, 1, "", "calc_and_sample_outcome"], [33, 2, 1, "", "generate_auction"], [33, 2, 1, "", "map_idx_to_features"]], "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ClickThroughRate": [[33, 2, 1, "", "calc_prob"], [33, 2, 1, "", "sample_outcome"]], "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ConversionRate": [[33, 2, 1, "", "calc_prob"], [33, 2, 1, "", "sample_outcome"]], "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.WinningPriceDistribution": [[33, 2, 1, "", "sample_outcome"]], "rtbgym.envs.wrapper_rtb": [[21, 1, 1, "", "CustomizedRTBEnv"]], "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv": [[21, 2, 1, "", "reset"], [21, 2, 1, "", "step"]], "rtbgym": [[34, 0, 0, "-", "utils"]], "rtbgym.utils": [[35, 1, 1, "", "NormalDistribution"], [36, 3, 1, "", "check_array"], [37, 3, 1, "", "sigmoid"]], "rtbgym.utils.NormalDistribution": [[35, 2, 1, "", "sample"]], "scope_rl.dataset": [[8, 0, 0, "-", "base"], [10, 0, 0, "-", "synthetic"]], "scope_rl.dataset.base": [[9, 1, 1, "", "BaseDataset"]], "scope_rl.dataset.base.BaseDataset": [[9, 2, 1, "", "obtain_episodes"], [9, 2, 1, "", "obtain_steps"]], "scope_rl.dataset.synthetic": [[11, 1, 1, "", "SyntheticDataset"]], "scope_rl.dataset.synthetic.SyntheticDataset": [[11, 2, 1, "", "obtain_episodes"], [11, 2, 1, "", "obtain_steps"]], "scope_rl.ope.continuous": [[38, 0, 0, "-", "basic_estimators"], [46, 0, 0, "-", "cumulative_distribution_estimators"], [52, 0, 0, "-", "marginal_estimators"]], "scope_rl.ope.continuous.basic_estimators": [[39, 1, 1, "", "DirectMethod"], [40, 1, 1, "", "DoublyRobust"], [41, 1, 1, "", "PerDecisionImportanceSampling"], [42, 1, 1, "", "SelfNormalizedDR"], [43, 1, 1, "", "SelfNormalizedPDIS"], [44, 1, 1, "", "SelfNormalizedTIS"], [45, 1, 1, "", "TrajectoryWiseImportanceSampling"]], "scope_rl.ope.continuous.basic_estimators.DirectMethod": [[39, 2, 1, "", "estimate_interval"], [39, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.basic_estimators.DoublyRobust": [[40, 2, 1, "", "estimate_interval"], [40, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling": [[41, 2, 1, "", "estimate_interval"], [41, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR": [[42, 2, 1, "", "estimate_interval"], [42, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS": [[43, 2, 1, "", "estimate_interval"], [43, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS": [[44, 2, 1, "", "estimate_interval"], [44, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling": [[45, 2, 1, "", "estimate_interval"], [45, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.cumulative_distribution_estimators": [[47, 1, 1, "", "CumulativeDistributionDM"], [48, 1, 1, "", "CumulativeDistributionSNTDR"], [49, 1, 1, "", "CumulativeDistributionSNTIS"], [50, 1, 1, "", "CumulativeDistributionTDR"], [51, 1, 1, "", "CumulativeDistributionTIS"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM": [[47, 2, 1, "", "estimate_conditional_value_at_risk"], [47, 2, 1, "", "estimate_cumulative_distribution_function"], [47, 2, 1, "", "estimate_interquartile_range"], [47, 2, 1, "", "estimate_mean"], [47, 2, 1, "", "estimate_variance"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR": [[48, 2, 1, "", "estimate_conditional_value_at_risk"], [48, 2, 1, "", "estimate_cumulative_distribution_function"], [48, 2, 1, "", "estimate_interquartile_range"], [48, 2, 1, "", "estimate_mean"], [48, 2, 1, "", "estimate_variance"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS": [[49, 2, 1, "", "estimate_conditional_value_at_risk"], [49, 2, 1, "", "estimate_cumulative_distribution_function"], [49, 2, 1, "", "estimate_interquartile_range"], [49, 2, 1, "", "estimate_mean"], [49, 2, 1, "", "estimate_variance"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR": [[50, 2, 1, "", "estimate_conditional_value_at_risk"], [50, 2, 1, "", "estimate_cumulative_distribution_function"], [50, 2, 1, "", "estimate_interquartile_range"], [50, 2, 1, "", "estimate_mean"], [50, 2, 1, "", "estimate_variance"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS": [[51, 2, 1, "", "estimate_conditional_value_at_risk"], [51, 2, 1, "", "estimate_cumulative_distribution_function"], [51, 2, 1, "", "estimate_interquartile_range"], [51, 2, 1, "", "estimate_mean"], [51, 2, 1, "", "estimate_variance"]], "scope_rl.ope.continuous.marginal_estimators": [[53, 1, 1, "", "DoubleReinforcementLearning"], [54, 1, 1, "", "StateActionMarginalDR"], [55, 1, 1, "", "StateActionMarginalIS"], [56, 1, 1, "", "StateActionMarginalSNDR"], [57, 1, 1, "", "StateActionMarginalSNIS"], [58, 1, 1, "", "StateMarginalDM"], [59, 1, 1, "", "StateMarginalDR"], [60, 1, 1, "", "StateMarginalIS"], [61, 1, 1, "", "StateMarginalSNDR"], [62, 1, 1, "", "StateMarginalSNIS"]], "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning": [[53, 2, 1, "", "estimate_interval"], [53, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR": [[54, 2, 1, "", "estimate_interval"], [54, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS": [[55, 2, 1, "", "estimate_interval"], [55, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR": [[56, 2, 1, "", "estimate_interval"], [56, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS": [[57, 2, 1, "", "estimate_interval"], [57, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM": [[58, 2, 1, "", "estimate_interval"], [58, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR": [[59, 2, 1, "", "estimate_interval"], [59, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS": [[60, 2, 1, "", "estimate_interval"], [60, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR": [[61, 2, 1, "", "estimate_interval"], [61, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS": [[62, 2, 1, "", "estimate_interval"], [62, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete": [[63, 0, 0, "-", "basic_estimators"], [71, 0, 0, "-", "cumulative_distribution_estimators"], [77, 0, 0, "-", "marginal_estimators"]], "scope_rl.ope.discrete.basic_estimators": [[64, 1, 1, "", "DirectMethod"], [65, 1, 1, "", "DoublyRobust"], [66, 1, 1, "", "PerDecisionImportanceSampling"], [67, 1, 1, "", "SelfNormalizedDR"], [68, 1, 1, "", "SelfNormalizedPDIS"], [69, 1, 1, "", "SelfNormalizedTIS"], [70, 1, 1, "", "TrajectoryWiseImportanceSampling"]], "scope_rl.ope.discrete.basic_estimators.DirectMethod": [[64, 2, 1, "", "estimate_interval"], [64, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.basic_estimators.DoublyRobust": [[65, 2, 1, "", "estimate_interval"], [65, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling": [[66, 2, 1, "", "estimate_interval"], [66, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR": [[67, 2, 1, "", "estimate_interval"], [67, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS": [[68, 2, 1, "", "estimate_interval"], [68, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS": [[69, 2, 1, "", "estimate_interval"], [69, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling": [[70, 2, 1, "", "estimate_interval"], [70, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.cumulative_distribution_estimators": [[72, 1, 1, "", "CumulativeDistributionDM"], [73, 1, 1, "", "CumulativeDistributionSNTDR"], [74, 1, 1, "", "CumulativeDistributionSNTIS"], [75, 1, 1, "", "CumulativeDistributionTDR"], [76, 1, 1, "", "CumulativeDistributionTIS"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM": [[72, 2, 1, "", "estimate_conditional_value_at_risk"], [72, 2, 1, "", "estimate_cumulative_distribution_function"], [72, 2, 1, "", "estimate_interquartile_range"], [72, 2, 1, "", "estimate_mean"], [72, 2, 1, "", "estimate_variance"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR": [[73, 2, 1, "", "estimate_conditional_value_at_risk"], [73, 2, 1, "", "estimate_cumulative_distribution_function"], [73, 2, 1, "", "estimate_interquartile_range"], [73, 2, 1, "", "estimate_mean"], [73, 2, 1, "", "estimate_variance"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS": [[74, 2, 1, "", "estimate_conditional_value_at_risk"], [74, 2, 1, "", "estimate_cumulative_distribution_function"], [74, 2, 1, "", "estimate_interquartile_range"], [74, 2, 1, "", "estimate_mean"], [74, 2, 1, "", "estimate_variance"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR": [[75, 2, 1, "", "estimate_conditional_value_at_risk"], [75, 2, 1, "", "estimate_cumulative_distribution_function"], [75, 2, 1, "", "estimate_interquartile_range"], [75, 2, 1, "", "estimate_mean"], [75, 2, 1, "", "estimate_variance"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS": [[76, 2, 1, "", "estimate_conditional_value_at_risk"], [76, 2, 1, "", "estimate_cumulative_distribution_function"], [76, 2, 1, "", "estimate_interquartile_range"], [76, 2, 1, "", "estimate_mean"], [76, 2, 1, "", "estimate_variance"]], "scope_rl.ope.discrete.marginal_estimators": [[78, 1, 1, "", "DoubleReinforcementLearning"], [79, 1, 1, "", "StateActionMarginalDR"], [80, 1, 1, "", "StateActionMarginalIS"], [81, 1, 1, "", "StateActionMarginalSNDR"], [82, 1, 1, "", "StateActionMarginalSNIS"], [83, 1, 1, "", "StateMarginalDM"], [84, 1, 1, "", "StateMarginalDR"], [85, 1, 1, "", "StateMarginalIS"], [86, 1, 1, "", "StateMarginalSNDR"], [87, 1, 1, "", "StateMarginalSNIS"]], "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning": [[78, 2, 1, "", "estimate_interval"], [78, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR": [[79, 2, 1, "", "estimate_interval"], [79, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS": [[80, 2, 1, "", "estimate_interval"], [80, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR": [[81, 2, 1, "", "estimate_interval"], [81, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS": [[82, 2, 1, "", "estimate_interval"], [82, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM": [[83, 2, 1, "", "estimate_interval"], [83, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR": [[84, 2, 1, "", "estimate_interval"], [84, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS": [[85, 2, 1, "", "estimate_interval"], [85, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR": [[86, 2, 1, "", "estimate_interval"], [86, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS": [[87, 2, 1, "", "estimate_interval"], [87, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope": [[88, 0, 0, "-", "estimators_base"], [94, 0, 0, "-", "input"], [96, 0, 0, "-", "online"], [110, 0, 0, "-", "ope"], [113, 0, 0, "-", "ops"]], "scope_rl.ope.estimators_base": [[89, 1, 1, "", "BaseCumulativeDistributionOPEEstimator"], [90, 1, 1, "", "BaseMarginalOPEEstimator"], [91, 1, 1, "", "BaseOffPolicyEstimator"], [92, 1, 1, "", "BaseStateActionMarginalOPEEstimator"], [93, 1, 1, "", "BaseStateMarginalOPEEstimator"]], "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator": [[89, 2, 1, "", "estimate_conditional_value_at_risk"], [89, 2, 1, "", "estimate_cumulative_distribution_function"], [89, 2, 1, "", "estimate_interquartile_range"], [89, 2, 1, "", "estimate_mean"], [89, 2, 1, "", "estimate_variance"]], "scope_rl.ope.estimators_base.BaseMarginalOPEEstimator": [[90, 2, 1, "", "estimate_interval"], [90, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.estimators_base.BaseOffPolicyEstimator": [[91, 2, 1, "", "estimate_interval"], [91, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator": [[92, 2, 1, "", "estimate_interval"], [92, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator": [[93, 2, 1, "", "estimate_interval"], [93, 2, 1, "", "estimate_policy_value"]], "scope_rl.ope.input": [[95, 1, 1, "", "CreateOPEInput"]], "scope_rl.ope.input.CreateOPEInput": [[95, 2, 1, "", "build_and_fit_FQE"], [95, 2, 1, "", "build_and_fit_state_action_dual_model"], [95, 2, 1, "", "build_and_fit_state_action_value_model"], [95, 2, 1, "", "build_and_fit_state_action_weight_model"], [95, 2, 1, "", "build_and_fit_state_dual_model"], [95, 2, 1, "", "build_and_fit_state_value_model"], [95, 2, 1, "", "build_and_fit_state_weight_model"], [95, 2, 1, "", "obtain_evaluation_policy_action"], [95, 2, 1, "", "obtain_evaluation_policy_action_dist"], [95, 2, 1, "", "obtain_evaluation_policy_action_prob_for_observed_state_action"], [95, 2, 1, "", "obtain_initial_state"], [95, 2, 1, "", "obtain_initial_state_value_prediction"], [95, 2, 1, "", "obtain_state_action_marginal_importance_weight"], [95, 2, 1, "", "obtain_state_action_value_prediction"], [95, 2, 1, "", "obtain_state_marginal_importance_weight"], [95, 2, 1, "", "obtain_whole_inputs"]], "scope_rl.ope.online": [[97, 3, 1, "", "calc_on_policy_conditional_value_at_risk"], [98, 3, 1, "", "calc_on_policy_cumulative_distribution_function"], [99, 3, 1, "", "calc_on_policy_interquartile_range"], [100, 3, 1, "", "calc_on_policy_policy_value"], [101, 3, 1, "", "calc_on_policy_policy_value_interval"], [102, 3, 1, "", "calc_on_policy_statistics"], [103, 3, 1, "", "calc_on_policy_variance"], [104, 3, 1, "", "rollout_policy_online"], [105, 3, 1, "", "visualize_on_policy_conditional_value_at_risk"], [106, 3, 1, "", "visualize_on_policy_cumulative_distribution_function"], [107, 3, 1, "", "visualize_on_policy_interquartile_range"], [108, 3, 1, "", "visualize_on_policy_policy_value"], [109, 3, 1, "", "visualize_on_policy_policy_value_with_variance"]], "scope_rl.ope.ope": [[111, 1, 1, "", "CumulativeDistributionOPE"], [112, 1, 1, "", "OffPolicyEvaluation"]], "scope_rl.ope.ope.CumulativeDistributionOPE": [[111, 2, 1, "", "estimate_conditional_value_at_risk"], [111, 2, 1, "", "estimate_cumulative_distribution_function"], [111, 2, 1, "", "estimate_interquartile_range"], [111, 2, 1, "", "estimate_mean"], [111, 2, 1, "", "estimate_variance"], [111, 2, 1, "", "obtain_reward_scale"], [111, 2, 1, "", "visualize_conditional_value_at_risk"], [111, 2, 1, "", "visualize_conditional_value_at_risk_with_multiple_estimates"], [111, 2, 1, "", "visualize_cumulative_distribution_function"], [111, 2, 1, "", "visualize_cumulative_distribution_function_with_multiple_estimates"], [111, 2, 1, "", "visualize_interquartile_range"], [111, 2, 1, "", "visualize_lower_quartile_with_multiple_estimates"], [111, 2, 1, "", "visualize_policy_value"], [111, 2, 1, "", "visualize_policy_value_with_multiple_estimates"], [111, 2, 1, "", "visualize_variance_with_multiple_estimates"]], "scope_rl.ope.ope.OffPolicyEvaluation": [[112, 2, 1, "", "estimate_intervals"], [112, 2, 1, "", "estimate_policy_value"], [112, 2, 1, "", "evaluate_performance_of_ope_estimators"], [112, 2, 1, "", "summarize_off_policy_estimates"], [112, 2, 1, "", "visualize_off_policy_estimates"], [112, 2, 1, "", "visualize_policy_value_with_multiple_estimates"]], "scope_rl.ope.ops": [[114, 1, 1, "", "OffPolicySelection"]], "scope_rl.ope.ops.OffPolicySelection": [[114, 2, 1, "", "obtain_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope"], [114, 2, 1, "", "obtain_topk_conditional_value_at_risk_selected_by_standard_ope"], [114, 2, 1, "", "obtain_topk_lower_quartile_selected_by_cumulative_distribution_ope"], [114, 2, 1, "", "obtain_topk_lower_quartile_selected_by_standard_ope"], [114, 2, 1, "", "obtain_topk_policy_value_selected_by_cumulative_distribution_ope"], [114, 2, 1, "", "obtain_topk_policy_value_selected_by_lower_bound"], [114, 2, 1, "", "obtain_topk_policy_value_selected_by_standard_ope"], [114, 2, 1, "", "obtain_true_selection_result"], [114, 2, 1, "", "select_by_conditional_value_at_risk"], [114, 2, 1, "", "select_by_lower_quartile"], [114, 2, 1, "", "select_by_policy_value"], [114, 2, 1, "", "select_by_policy_value_lower_bound"], [114, 2, 1, "", "select_by_policy_value_via_cumulative_distribution_ope"], [114, 2, 1, "", "visualize_conditional_value_at_risk_for_selection"], [114, 2, 1, "", "visualize_conditional_value_at_risk_for_validation"], [114, 2, 1, "", "visualize_conditional_value_at_risk_with_multiple_estimates"], [114, 2, 1, "", "visualize_cumulative_distribution_function_for_selection"], [114, 2, 1, "", "visualize_cumulative_distribution_function_with_multiple_estimates"], [114, 2, 1, "", "visualize_interquartile_range_for_selection"], [114, 2, 1, "", "visualize_lower_quartile_for_validation"], [114, 2, 1, "", "visualize_lower_quartile_with_multiple_estimates"], [114, 2, 1, "", "visualize_policy_value_for_selection"], [114, 2, 1, "", "visualize_policy_value_for_validation"], [114, 2, 1, "", "visualize_policy_value_lower_bound_for_validation"], [114, 2, 1, "", "visualize_policy_value_of_cumulative_distribution_ope_for_selection"], [114, 2, 1, "", "visualize_policy_value_of_cumulative_distribution_ope_for_validation"], [114, 2, 1, "", "visualize_policy_value_with_multiple_estimates_cumulative_distribution_ope"], [114, 2, 1, "", "visualize_policy_value_with_multiple_estimates_standard_ope"], [114, 2, 1, "", "visualize_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope"], [114, 2, 1, "", "visualize_topk_conditional_value_at_risk_selected_by_standard_ope"], [114, 2, 1, "", "visualize_topk_lower_quartile_selected_by_cumulative_distribution_ope"], [114, 2, 1, "", "visualize_topk_lower_quartile_selected_by_standard_ope"], [114, 2, 1, "", "visualize_topk_policy_value_selected_by_cumulative_distribution_ope"], [114, 2, 1, "", "visualize_topk_policy_value_selected_by_lower_bound"], [114, 2, 1, "", "visualize_topk_policy_value_selected_by_standard_ope"], [114, 2, 1, "", "visualize_variance_for_validation"], [114, 2, 1, "", "visualize_variance_with_multiple_estimates"]], "scope_rl.ope.weight_value_learning": [[115, 0, 0, "-", "augmented_lagrangian_learning_continuous"], [118, 0, 0, "-", "augmented_lagrangian_learning_discrete"], [121, 0, 0, "-", "base"], [123, 0, 0, "-", "function"], [130, 0, 0, "-", "minimax_value_learning_continuous"], [133, 0, 0, "-", "minimax_value_learning_discrete"], [136, 0, 0, "-", "minimax_weight_learning_continuous"], [139, 0, 0, "-", "minimax_weight_learning_discrete"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous": [[116, 1, 1, "", "ContinuousDiceStateActionWightValueLearning"], [117, 1, 1, "", "ContinuousDiceStateWightValueLearning"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning": [[116, 2, 1, "", "fit"], [116, 2, 1, "", "fit_predict"], [116, 2, 1, "", "load"], [116, 2, 1, "", "predict"], [116, 2, 1, "", "predict_q_function"], [116, 2, 1, "", "predict_v_function"], [116, 2, 1, "", "predict_value"], [116, 2, 1, "", "predict_weight"], [116, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning": [[117, 2, 1, "", "fit"], [117, 2, 1, "", "fit_predict"], [117, 2, 1, "", "load"], [117, 2, 1, "", "predict"], [117, 2, 1, "", "predict_value"], [117, 2, 1, "", "predict_weight"], [117, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete": [[119, 1, 1, "", "DiscreteDiceStateActionWightValueLearning"], [120, 1, 1, "", "DiscreteDiceStateWightValueLearning"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning": [[119, 2, 1, "", "fit"], [119, 2, 1, "", "fit_predict"], [119, 2, 1, "", "load"], [119, 2, 1, "", "predict"], [119, 2, 1, "", "predict_q_function"], [119, 2, 1, "", "predict_q_function_for_all_actions"], [119, 2, 1, "", "predict_v_function"], [119, 2, 1, "", "predict_value"], [119, 2, 1, "", "predict_weight"], [119, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning": [[120, 2, 1, "", "fit"], [120, 2, 1, "", "fit_predict"], [120, 2, 1, "", "load"], [120, 2, 1, "", "predict"], [120, 2, 1, "", "predict_value"], [120, 2, 1, "", "predict_weight"], [120, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.base": [[122, 1, 1, "", "BaseWeightValueLearner"]], "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner": [[122, 2, 1, "", "fit"], [122, 2, 1, "", "load"], [122, 2, 1, "", "predict"], [122, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.function": [[124, 1, 1, "", "ContinuousQFunction"], [125, 1, 1, "", "ContinuousStateActionWeightFunction"], [126, 1, 1, "", "DiscreteQFunction"], [127, 1, 1, "", "DiscreteStateActionWeightFunction"], [128, 1, 1, "", "StateWeightFunction"], [129, 1, 1, "", "VFunction"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous": [[131, 1, 1, "", "ContinuousMinimaxStateActionValueLearning"], [132, 1, 1, "", "ContinuousMinimaxStateValueLearning"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning": [[131, 2, 1, "", "fit"], [131, 2, 1, "", "fit_predict"], [131, 2, 1, "", "load"], [131, 2, 1, "", "predict"], [131, 2, 1, "", "predict_q_function"], [131, 2, 1, "", "predict_v_function"], [131, 2, 1, "", "predict_value"], [131, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning": [[132, 2, 1, "", "fit"], [132, 2, 1, "", "fit_predict"], [132, 2, 1, "", "load"], [132, 2, 1, "", "predict"], [132, 2, 1, "", "predict_v_function"], [132, 2, 1, "", "predict_value"], [132, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete": [[134, 1, 1, "", "DiscreteMinimaxStateActionValueLearning"], [135, 1, 1, "", "DiscreteMinimaxStateValueLearning"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning": [[134, 2, 1, "", "fit"], [134, 2, 1, "", "fit_predict"], [134, 2, 1, "", "load"], [134, 2, 1, "", "predict"], [134, 2, 1, "", "predict_q_function"], [134, 2, 1, "", "predict_q_function_for_all_actions"], [134, 2, 1, "", "predict_v_function"], [134, 2, 1, "", "predict_value"], [134, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning": [[135, 2, 1, "", "fit"], [135, 2, 1, "", "fit_predict"], [135, 2, 1, "", "load"], [135, 2, 1, "", "predict"], [135, 2, 1, "", "predict_v_function"], [135, 2, 1, "", "predict_value"], [135, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous": [[137, 1, 1, "", "ContinuousMinimaxStateActionWeightLearning"], [138, 1, 1, "", "ContinuousMinimaxStateWeightLearning"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning": [[137, 2, 1, "", "fit"], [137, 2, 1, "", "fit_predict"], [137, 2, 1, "", "load"], [137, 2, 1, "", "predict"], [137, 2, 1, "", "predict_weight"], [137, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning": [[138, 2, 1, "", "fit"], [138, 2, 1, "", "fit_predict"], [138, 2, 1, "", "load"], [138, 2, 1, "", "predict"], [138, 2, 1, "", "predict_state_action_marginal_importance_weight"], [138, 2, 1, "", "predict_state_marginal_importance_weight"], [138, 2, 1, "", "predict_weight"], [138, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete": [[140, 1, 1, "", "DiscreteMinimaxStateActionWeightLearning"], [141, 1, 1, "", "DiscreteMinimaxStateWeightLearning"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning": [[140, 2, 1, "", "fit"], [140, 2, 1, "", "fit_predict"], [140, 2, 1, "", "load"], [140, 2, 1, "", "predict"], [140, 2, 1, "", "predict_weight"], [140, 2, 1, "", "save"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning": [[141, 2, 1, "", "fit"], [141, 2, 1, "", "fit_predict"], [141, 2, 1, "", "load"], [141, 2, 1, "", "predict"], [141, 2, 1, "", "predict_state_action_marginal_importance_weight"], [141, 2, 1, "", "predict_state_marginal_importance_weight"], [141, 2, 1, "", "predict_weight"], [141, 2, 1, "", "save"]], "scope_rl.policy": [[142, 0, 0, "-", "head"]], "scope_rl.policy.head": [[143, 1, 1, "", "BaseHead"], [144, 1, 1, "", "ContinuousEvalHead"], [145, 1, 1, "", "EpsilonGreedyHead"], [146, 1, 1, "", "GaussianHead"], [147, 1, 1, "", "OnlineHead"], [148, 1, 1, "", "SoftmaxHead"], [149, 1, 1, "", "TruncatedGaussianHead"]], "scope_rl.policy.head.BaseHead": [[143, 2, 1, "", "calc_action_choice_probability"], [143, 2, 1, "", "calc_pscore_given_action"], [143, 2, 1, "", "predict_online"], [143, 2, 1, "", "predict_value_online"], [143, 2, 1, "", "sample_action_and_output_pscore"], [143, 2, 1, "", "sample_action_and_output_pscore_online"], [143, 2, 1, "", "sample_action_online"]], "scope_rl.policy.head.EpsilonGreedyHead": [[145, 2, 1, "", "calc_action_choice_probability"], [145, 2, 1, "", "calc_pscore_given_action"], [145, 2, 1, "", "sample_action_and_output_pscore"]], "scope_rl.policy.head.GaussianHead": [[146, 2, 1, "", "calc_pscore_given_action"], [146, 2, 1, "", "sample_action_and_output_pscore"]], "scope_rl.policy.head.OnlineHead": [[147, 2, 1, "", "calc_action_choice_probability"], [147, 2, 1, "", "calc_pscore_given_action"], [147, 2, 1, "", "sample_action_and_output_pscore"]], "scope_rl.policy.head.SoftmaxHead": [[148, 2, 1, "", "calc_action_choice_probability"], [148, 2, 1, "", "calc_pscore_given_action"], [148, 2, 1, "", "sample_action_and_output_pscore"]], "scope_rl.policy.head.TruncatedGaussianHead": [[149, 2, 1, "", "calc_pscore_given_action"], [149, 2, 1, "", "sample_action_and_output_pscore"]], "scope_rl": [[150, 0, 0, "-", "utils"]], "scope_rl.utils": [[151, 1, 1, "", "MinMaxActionScaler"], [152, 1, 1, "", "MinMaxScaler"], [153, 1, 1, "", "MultipleInputDict"], [154, 1, 1, "", "MultipleLoggedDataset"], [155, 1, 1, "", "NewGymAPIWrapper"], [156, 1, 1, "", "OldGymAPIWrapper"], [157, 3, 1, "", "check_array"], [158, 3, 1, "", "check_input_dict"], [159, 3, 1, "", "check_logged_dataset"], [160, 3, 1, "", "cosine_kernel"], [161, 3, 1, "", "defaultdict_to_dict"], [162, 3, 1, "", "epanechnikov_kernel"], [163, 3, 1, "", "estimate_confidence_interval_by_bootstrap"], [164, 3, 1, "", "estimate_confidence_interval_by_empirical_bernstein"], [165, 3, 1, "", "estimate_confidence_interval_by_hoeffding"], [166, 3, 1, "", "estimate_confidence_interval_by_t_test"], [167, 3, 1, "", "gaussian_kernel"], [168, 3, 1, "", "l2_distance"], [169, 3, 1, "", "triangular_kernel"], [170, 3, 1, "", "uniform_kernel"]], "scope_rl.utils.MinMaxActionScaler": [[151, 2, 1, "", "fit"], [151, 2, 1, "", "fit_with_env"], [151, 2, 1, "", "get_params"], [151, 2, 1, "", "get_type"], [151, 2, 1, "", "reverse_transform"], [151, 2, 1, "", "reverse_transform_numpy"], [151, 2, 1, "", "transform"]], "scope_rl.utils.MinMaxScaler": [[152, 2, 1, "", "fit"], [152, 2, 1, "", "fit_with_env"], [152, 2, 1, "", "get_params"], [152, 2, 1, "", "get_type"], [152, 2, 1, "", "reverse_transform"], [152, 2, 1, "", "transform"]], "scope_rl.utils.MultipleInputDict": [[153, 2, 1, "", "add"], [153, 2, 1, "", "get"], [153, 4, 1, "", "n_eval_policies"], [153, 4, 1, "", "use_same_eval_policy_across_dataset"]], "scope_rl.utils.MultipleLoggedDataset": [[154, 2, 1, "", "add"], [154, 2, 1, "", "get"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function", "4": "py:property"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"], "4": ["py", "property", "Python property"]}, "titleterms": {"basicgym": [0, 1, 2, 3, 4, 5, 6, 7, 193, 194], "env": [0, 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 194, 197, 199], "synthet": [0, 1, 10, 11, 184, 188], "basicenv": [1, 193], "simul": [2, 3, 4, 5, 6, 7, 14, 15, 16, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 194, 197, 199], "base": [2, 3, 4, 8, 9, 14, 15, 22, 23, 24, 25, 121, 122, 174, 175], "baserewardfunct": 3, "basestatetransitionfunct": 4, "function": [5, 6, 7, 16, 17, 28, 29, 30, 31, 123, 124, 125, 126, 127, 128, 129, 171, 175, 179, 187], "rewardfunct": 6, "statetransitionfunct": 7, "scope_rl": [8, 9, 10, 11, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170], "dataset": [8, 9, 10, 11, 174, 175, 177, 178, 180, 184, 188, 191], "basedataset": 9, "syntheticdataset": 11, "recgym": [12, 13, 14, 15, 16, 17, 196, 197], "rec": [12, 13], "recenv": [13, 196], "baseusermodel": 15, "usermodel": 17, "rtbgym": [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 198, 199], "rtb": [18, 19], "rtbenv": [19, 198], "wrapper_rtb": [20, 21], "customizedrtbenv": 21, "baseclickandconversionr": 23, "basesimul": 24, "basewinningpricedistribut": 25, "bidder": [26, 27], "clickthroughr": 29, "conversionr": 30, "winningpricedistribut": 31, "rtb_synthet": [32, 33], "rtbsyntheticsimul": 33, "util": [34, 35, 36, 37, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170], "normaldistribut": 35, "check_arrai": [36, 157], "sigmoid": 37, "op": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 171, 172, 173, 174, 175, 176, 177, 178, 179, 182, 188, 191, 192], "continu": [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 172, 174, 182], "basic_estim": [38, 39, 40, 41, 42, 43, 44, 45, 63, 64, 65, 66, 67, 68, 69, 70], "directmethod": [39, 64], "doublyrobust": [40, 65], "perdecisionimportancesampl": [41, 66], "selfnormalizeddr": [42, 67], "selfnormalizedpdi": [43, 68], "selfnormalizedti": [44, 69], "trajectorywiseimportancesampl": [45, 70], "cumulative_distribution_estim": [46, 47, 48, 49, 50, 51, 71, 72, 73, 74, 75, 76], "cumulativedistributiondm": [47, 72], "cumulativedistributionsntdr": [48, 73], "cumulativedistributionsnti": [49, 74], "cumulativedistributiontdr": [50, 75], "cumulativedistributionti": [51, 76], "marginal_estim": [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "doublereinforcementlearn": [53, 78], "stateactionmarginaldr": [54, 79], "stateactionmarginali": [55, 80], "stateactionmarginalsndr": [56, 81], "stateactionmarginalsni": [57, 82], "statemarginaldm": [58, 83], "statemarginaldr": [59, 84], "statemarginali": [60, 85], "statemarginalsndr": [61, 86], "statemarginalsni": [62, 87], "discret": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 182], "estimators_bas": [88, 89, 90, 91, 92, 93], "basecumulativedistributionopeestim": 89, "basemarginalopeestim": 90, "baseoffpolicyestim": 91, "basestateactionmarginalopeestim": 92, "basestatemarginalopeestim": 93, "input": [94, 95, 172, 174, 175, 178, 180], "createopeinput": 95, "onlin": [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 182, 184, 186], "calc_on_policy_conditional_value_at_risk": 97, "calc_on_policy_cumulative_distribution_funct": 98, "calc_on_policy_interquartile_rang": 99, "calc_on_policy_policy_valu": 100, "calc_on_policy_policy_value_interv": 101, "calc_on_policy_statist": 102, "calc_on_policy_vari": 103, "rollout_policy_onlin": 104, "visualize_on_policy_conditional_value_at_risk": 105, "visualize_on_policy_cumulative_distribution_funct": 106, "visualize_on_policy_interquartile_rang": 107, "visualize_on_policy_policy_valu": 108, "visualize_on_policy_policy_value_with_vari": 109, "cumulativedistributionop": 111, "offpolicyevalu": 112, "offpolicyselect": 114, "weight_value_learn": [115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141], "augmented_lagrangian_learning_continu": [115, 116, 117], "continuousdicestateactionwightvaluelearn": 116, "continuousdicestatewightvaluelearn": 117, "augmented_lagrangian_learning_discret": [118, 119, 120], "discretedicestateactionwightvaluelearn": 119, "discretedicestatewightvaluelearn": 120, "baseweightvaluelearn": 122, "continuousqfunct": 124, "continuousstateactionweightfunct": 125, "discreteqfunct": 126, "discretestateactionweightfunct": 127, "stateweightfunct": 128, "vfunction": 129, "minimax_value_learning_continu": [130, 131, 132], "continuousminimaxstateactionvaluelearn": 131, "continuousminimaxstatevaluelearn": 132, "minimax_value_learning_discret": [133, 134, 135], "discreteminimaxstateactionvaluelearn": 134, "discreteminimaxstatevaluelearn": 135, "minimax_weight_learning_continu": [136, 137, 138], "continuousminimaxstateactionweightlearn": 137, "continuousminimaxstateweightlearn": 138, "minimax_weight_learning_discret": [139, 140, 141], "discreteminimaxstateactionweightlearn": 140, "discreteminimaxstateweightlearn": 141, "polici": [142, 143, 144, 145, 146, 147, 148, 149, 172, 173, 174, 175, 177, 178, 179, 182, 184, 186, 187, 188, 191, 192], "head": [142, 143, 144, 145, 146, 147, 148, 149], "basehead": 143, "continuousevalhead": 144, "epsilongreedyhead": 145, "gaussianhead": 146, "onlinehead": [147, 184], "softmaxhead": 148, "truncatedgaussianhead": 149, "minmaxactionscal": 151, "minmaxscal": 152, "multipleinputdict": 153, "multipleloggeddataset": 154, "newgymapiwrapp": 155, "oldgymapiwrapp": 156, "check_input_dict": 158, "check_logged_dataset": 159, "cosine_kernel": 160, "defaultdict_to_dict": 161, "epanechnikov_kernel": 162, "estimate_confidence_interval_by_bootstrap": 163, "estimate_confidence_interval_by_empirical_bernstein": 164, "estimate_confidence_interval_by_hoeffd": 165, "estimate_confidence_interval_by_t_test": 166, "gaussian_kernel": 167, "l2_distanc": 168, "triangular_kernel": 169, "uniform_kernel": 170, "why": 171, "scope": [171, 177, 181, 182], "rl": [171, 177, 181, 182, 191], "motiv": 171, "kei": 171, "contribut": [171, 182, 193, 196, 198], "end": 171, "implement": [171, 172, 177, 182, 184, 193, 196, 198], "offlin": [171, 182, 184, 186, 188], "varieti": 171, "estim": [171, 172, 173, 174, 175, 176, 177, 178, 182, 186, 187, 191], "evalu": [171, 172, 174, 175, 177, 178, 182, 184, 187, 188, 192], "protocol": [171, 192], "cumul": [171, 172, 175, 176, 177, 178, 179, 182, 187, 188], "distribut": [171, 172, 175, 176, 177, 178, 179, 182, 187, 188], "risk": [171, 175, 187, 192], "return": [171, 192], "assess": [171, 173, 177, 178, 192], "comparison": 171, "exist": [171, 192], "platform": 171, "support": [172, 184, 193, 196, 198], "creat": 172, "basic": [172, 174, 176, 177, 179, 182, 188, 193, 196, 198], "off": [172, 174, 175, 177, 178, 179, 182, 187, 188], "direct": 172, "method": [172, 182, 191], "dm": 172, "trajectori": 172, "wise": 172, "import": [172, 174, 175], "sampl": [172, 174, 175], "ti": 172, "per": 172, "decis": 172, "pdi": 172, "doubli": 172, "robust": 172, "dr": 172, "self": 172, "normal": 172, "margin": [172, 174, 182], "doubl": [172, 174, 182], "reinforc": [172, 174, 182, 186, 188], "learn": [172, 174, 182, 184, 186, 188, 191], "drl": 172, "spectrum": [172, 174], "sope": 172, "high": [172, 174, 177, 182], "confid": [172, 174, 177, 182], "hcope": 172, "extens": 172, "action": [172, 174, 182], "space": 172, "cd": [172, 177], "tdr": 172, "metric": [172, 173, 182], "exampl": [173, 174, 175, 176, 177, 178, 179], "code": [173, 174, 175, 176, 177, 178, 179], "prerequisit": [173, 179], "result": [173, 179], "convent": 173, "top": [173, 178, 182, 192], "k": [173, 178, 182, 192], "deploy": [173, 182, 192], "visual": [173, 179, 200], "true": [173, 178], "perform": [173, 178, 182], "log": [174, 175, 178, 180], "model": [174, 175], "scaler": 174, "valu": [174, 175, 182, 187, 191], "weight": [174, 182, 191], "choos": 174, "kernel": 174, "probabl": 174, "bound": 174, "accuraci": 174, "cdf": 175, "mean": 175, "i": 175, "e": 175, "varianc": 175, "condit": 175, "cvar": 175, "interquartil": 175, "rang": 175, "custom": [176, 177, 193, 196, 198], "select": [177, 178, 179, 182, 187, 188], "handl": 177, "multipl": [177, 178], "real": [177, 180], "world": [177, 180], "behavior": [178, 186], "via": [178, 179, 192], "valid": 178, "obtain": 179, "oracl": 179, "call": 179, "from": 179, "cd_ope": 179, "instanc": 179, "guidelin": 180, "prepar": 180, "dict": 180, "faq": 181, "overview": [182, 186, 187, 193, 196, 198], "data": [182, 188], "collect": 182, "meta": 182, "class": [182, 198], "state": 182, "interest": 182, "citat": [182, 183, 192, 193, 196, 198], "googl": 182, "group": 182, "contact": [182, 193, 196, 198], "tabl": 182, "content": 182, "get": 182, "start": 182, "our": 182, "propos": 182, "sub": [182, 195], "packag": [182, 191, 194, 195, 197, 199], "refer": [182, 189, 191, 194, 197, 199], "see": 182, "also": 182, "instal": 183, "gener": [184, 188], "wrapper": [184, 198], "discretehead": 184, "continuoushead": 184, "new": 185, "2023": 185, "On": 186, "gradient": 186, "q": 186, "actor": 186, "critic": 186, "The": 186, "problem": 186, "extrapol": 186, "error": 186, "diverg": 186, "regular": 186, "clone": 186, "uncertainti": 186, "conserv": 186, "implicit": 186, "quickstart": [188, 193, 196, 198], "preprocess": 188, "paper": 189, "project": 189, "releas": 190, "note": 190, "scopr": 191, "modul": [191, 193, 194, 196, 197, 198, 199], "pipelin": 191, "other": [191, 199], "sharpratio": 192, "background": 192, "issu": 192, "tradeoff": 192, "benchmark": 192, "set": [193, 196, 198], "standard": [193, 196, 198], "environ": [193, 196, 198], "configur": [193, 196, 198], "bid": 198, "setup": 198, "tool": 200, "welcom": 201}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinxcontrib.bibtex": 9, "sphinx": 57}, "alltitles": {"basicgym.envs.synthetic": [[0, "module-basicgym.envs.synthetic"]], "basicgym.envs.synthetic.BasicEnv": [[1, "basicgym-envs-synthetic-basicenv"]], "basicgym.envs.simulator.base": [[2, "module-basicgym.envs.simulator.base"]], "basicgym.envs.simulator.base.BaseRewardFunction": [[3, "basicgym-envs-simulator-base-baserewardfunction"]], "basicgym.envs.simulator.base.BaseStateTransitionFunction": [[4, "basicgym-envs-simulator-base-basestatetransitionfunction"]], "basicgym.envs.simulator.function": [[5, "module-basicgym.envs.simulator.function"]], "basicgym.envs.simulator.function.RewardFunction": [[6, "basicgym-envs-simulator-function-rewardfunction"]], "basicgym.envs.simulator.function.StateTransitionFunction": [[7, "basicgym-envs-simulator-function-statetransitionfunction"]], "scope_rl.dataset.base": [[8, "module-scope_rl.dataset.base"]], "scope_rl.dataset.base.BaseDataset": [[9, "scope-rl-dataset-base-basedataset"]], "scope_rl.dataset.synthetic": [[10, "module-scope_rl.dataset.synthetic"]], "scope_rl.dataset.synthetic.SyntheticDataset": [[11, "scope-rl-dataset-synthetic-syntheticdataset"]], "recgym.envs.rec": [[12, "module-recgym.envs.rec"]], "recgym.envs.rec.RECEnv": [[13, "recgym-envs-rec-recenv"]], "recgym.envs.simulator.base": [[14, "module-recgym.envs.simulator.base"]], "recgym.envs.simulator.base.BaseUserModel": [[15, "recgym-envs-simulator-base-baseusermodel"]], "recgym.envs.simulator.function": [[16, "module-recgym.envs.simulator.function"]], "recgym.envs.simulator.function.UserModel": [[17, "recgym-envs-simulator-function-usermodel"]], "rtbgym.envs.rtb": [[18, "module-rtbgym.envs.rtb"]], "rtbgym.envs.rtb.RTBEnv": [[19, "rtbgym-envs-rtb-rtbenv"]], "rtbgym.envs.wrapper_rtb": [[20, "module-rtbgym.envs.wrapper_rtb"]], "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv": [[21, "rtbgym-envs-wrapper-rtb-customizedrtbenv"]], "rtbgym.envs.simulator.base": [[22, "module-rtbgym.envs.simulator.base"]], "rtbgym.envs.simulator.base.BaseClickAndConversionRate": [[23, "rtbgym-envs-simulator-base-baseclickandconversionrate"]], "rtbgym.envs.simulator.base.BaseSimulator": [[24, "rtbgym-envs-simulator-base-basesimulator"]], "rtbgym.envs.simulator.base.BaseWinningPriceDistribution": [[25, "rtbgym-envs-simulator-base-basewinningpricedistribution"]], "rtbgym.envs.simulator.bidder": [[26, "module-rtbgym.envs.simulator.bidder"]], "rtbgym.envs.simulator.bidder.Bidder": [[27, "rtbgym-envs-simulator-bidder-bidder"]], "rtbgym.envs.simulator.function": [[28, "module-rtbgym.envs.simulator.function"]], "rtbgym.envs.simulator.function.ClickThroughRate": [[29, "rtbgym-envs-simulator-function-clickthroughrate"]], "rtbgym.envs.simulator.function.ConversionRate": [[30, "rtbgym-envs-simulator-function-conversionrate"]], "rtbgym.envs.simulator.function.WinningPriceDistribution": [[31, "rtbgym-envs-simulator-function-winningpricedistribution"]], "rtbgym.envs.simulator.rtb_synthetic": [[32, "module-rtbgym.envs.simulator.rtb_synthetic"]], "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator": [[33, "rtbgym-envs-simulator-rtb-synthetic-rtbsyntheticsimulator"]], "rtbgym.utils": [[34, "module-rtbgym.utils"]], "rtbgym.utils.NormalDistribution": [[35, "rtbgym-utils-normaldistribution"]], "rtbgym.utils.check_array": [[36, "rtbgym-utils-check-array"]], "rtbgym.utils.sigmoid": [[37, "rtbgym-utils-sigmoid"]], "scope_rl.ope.continuous.basic_estimators": [[38, "module-scope_rl.ope.continuous.basic_estimators"]], "scope_rl.ope.continuous.basic_estimators.DirectMethod": [[39, "scope-rl-ope-continuous-basic-estimators-directmethod"]], "scope_rl.ope.continuous.basic_estimators.DoublyRobust": [[40, "scope-rl-ope-continuous-basic-estimators-doublyrobust"]], "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling": [[41, "scope-rl-ope-continuous-basic-estimators-perdecisionimportancesampling"]], "scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR": [[42, "scope-rl-ope-continuous-basic-estimators-selfnormalizeddr"]], "scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS": [[43, "scope-rl-ope-continuous-basic-estimators-selfnormalizedpdis"]], "scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS": [[44, "scope-rl-ope-continuous-basic-estimators-selfnormalizedtis"]], "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling": [[45, "scope-rl-ope-continuous-basic-estimators-trajectorywiseimportancesampling"]], "scope_rl.ope.continuous.cumulative_distribution_estimators": [[46, "module-scope_rl.ope.continuous.cumulative_distribution_estimators"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM": [[47, "scope-rl-ope-continuous-cumulative-distribution-estimators-cumulativedistributiondm"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR": [[48, "scope-rl-ope-continuous-cumulative-distribution-estimators-cumulativedistributionsntdr"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS": [[49, "scope-rl-ope-continuous-cumulative-distribution-estimators-cumulativedistributionsntis"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR": [[50, "scope-rl-ope-continuous-cumulative-distribution-estimators-cumulativedistributiontdr"]], "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS": [[51, "scope-rl-ope-continuous-cumulative-distribution-estimators-cumulativedistributiontis"]], "scope_rl.ope.continuous.marginal_estimators": [[52, "module-scope_rl.ope.continuous.marginal_estimators"]], "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning": [[53, "scope-rl-ope-continuous-marginal-estimators-doublereinforcementlearning"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR": [[54, "scope-rl-ope-continuous-marginal-estimators-stateactionmarginaldr"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS": [[55, "scope-rl-ope-continuous-marginal-estimators-stateactionmarginalis"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR": [[56, "scope-rl-ope-continuous-marginal-estimators-stateactionmarginalsndr"]], "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS": [[57, "scope-rl-ope-continuous-marginal-estimators-stateactionmarginalsnis"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM": [[58, "scope-rl-ope-continuous-marginal-estimators-statemarginaldm"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR": [[59, "scope-rl-ope-continuous-marginal-estimators-statemarginaldr"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS": [[60, "scope-rl-ope-continuous-marginal-estimators-statemarginalis"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR": [[61, "scope-rl-ope-continuous-marginal-estimators-statemarginalsndr"]], "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS": [[62, "scope-rl-ope-continuous-marginal-estimators-statemarginalsnis"]], "scope_rl.ope.discrete.basic_estimators": [[63, "module-scope_rl.ope.discrete.basic_estimators"]], "scope_rl.ope.discrete.basic_estimators.DirectMethod": [[64, "scope-rl-ope-discrete-basic-estimators-directmethod"]], "scope_rl.ope.discrete.basic_estimators.DoublyRobust": [[65, "scope-rl-ope-discrete-basic-estimators-doublyrobust"]], "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling": [[66, "scope-rl-ope-discrete-basic-estimators-perdecisionimportancesampling"]], "scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR": [[67, "scope-rl-ope-discrete-basic-estimators-selfnormalizeddr"]], "scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS": [[68, "scope-rl-ope-discrete-basic-estimators-selfnormalizedpdis"]], "scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS": [[69, "scope-rl-ope-discrete-basic-estimators-selfnormalizedtis"]], "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling": [[70, "scope-rl-ope-discrete-basic-estimators-trajectorywiseimportancesampling"]], "scope_rl.ope.discrete.cumulative_distribution_estimators": [[71, "module-scope_rl.ope.discrete.cumulative_distribution_estimators"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM": [[72, "scope-rl-ope-discrete-cumulative-distribution-estimators-cumulativedistributiondm"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR": [[73, "scope-rl-ope-discrete-cumulative-distribution-estimators-cumulativedistributionsntdr"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS": [[74, "scope-rl-ope-discrete-cumulative-distribution-estimators-cumulativedistributionsntis"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR": [[75, "scope-rl-ope-discrete-cumulative-distribution-estimators-cumulativedistributiontdr"]], "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS": [[76, "scope-rl-ope-discrete-cumulative-distribution-estimators-cumulativedistributiontis"]], "scope_rl.ope.discrete.marginal_estimators": [[77, "module-scope_rl.ope.discrete.marginal_estimators"]], "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning": [[78, "scope-rl-ope-discrete-marginal-estimators-doublereinforcementlearning"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR": [[79, "scope-rl-ope-discrete-marginal-estimators-stateactionmarginaldr"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS": [[80, "scope-rl-ope-discrete-marginal-estimators-stateactionmarginalis"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR": [[81, "scope-rl-ope-discrete-marginal-estimators-stateactionmarginalsndr"]], "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS": [[82, "scope-rl-ope-discrete-marginal-estimators-stateactionmarginalsnis"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM": [[83, "scope-rl-ope-discrete-marginal-estimators-statemarginaldm"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR": [[84, "scope-rl-ope-discrete-marginal-estimators-statemarginaldr"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS": [[85, "scope-rl-ope-discrete-marginal-estimators-statemarginalis"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR": [[86, "scope-rl-ope-discrete-marginal-estimators-statemarginalsndr"]], "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS": [[87, "scope-rl-ope-discrete-marginal-estimators-statemarginalsnis"]], "scope_rl.ope.estimators_base": [[88, "module-scope_rl.ope.estimators_base"]], "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator": [[89, "scope-rl-ope-estimators-base-basecumulativedistributionopeestimator"]], "scope_rl.ope.estimators_base.BaseMarginalOPEEstimator": [[90, "scope-rl-ope-estimators-base-basemarginalopeestimator"]], "scope_rl.ope.estimators_base.BaseOffPolicyEstimator": [[91, "scope-rl-ope-estimators-base-baseoffpolicyestimator"]], "scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator": [[92, "scope-rl-ope-estimators-base-basestateactionmarginalopeestimator"]], "scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator": [[93, "scope-rl-ope-estimators-base-basestatemarginalopeestimator"]], "scope_rl.ope.input": [[94, "module-scope_rl.ope.input"]], "scope_rl.ope.input.CreateOPEInput": [[95, "scope-rl-ope-input-createopeinput"]], "scope_rl.ope.online": [[96, "module-scope_rl.ope.online"]], "scope_rl.ope.online.calc_on_policy_conditional_value_at_risk": [[97, "scope-rl-ope-online-calc-on-policy-conditional-value-at-risk"]], "scope_rl.ope.online.calc_on_policy_cumulative_distribution_function": [[98, "scope-rl-ope-online-calc-on-policy-cumulative-distribution-function"]], "scope_rl.ope.online.calc_on_policy_interquartile_range": [[99, "scope-rl-ope-online-calc-on-policy-interquartile-range"]], "scope_rl.ope.online.calc_on_policy_policy_value": [[100, "scope-rl-ope-online-calc-on-policy-policy-value"]], "scope_rl.ope.online.calc_on_policy_policy_value_interval": [[101, "scope-rl-ope-online-calc-on-policy-policy-value-interval"]], "scope_rl.ope.online.calc_on_policy_statistics": [[102, "scope-rl-ope-online-calc-on-policy-statistics"]], "scope_rl.ope.online.calc_on_policy_variance": [[103, "scope-rl-ope-online-calc-on-policy-variance"]], "scope_rl.ope.online.rollout_policy_online": [[104, "scope-rl-ope-online-rollout-policy-online"]], "scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk": [[105, "scope-rl-ope-online-visualize-on-policy-conditional-value-at-risk"]], "scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function": [[106, "scope-rl-ope-online-visualize-on-policy-cumulative-distribution-function"]], "scope_rl.ope.online.visualize_on_policy_interquartile_range": [[107, "scope-rl-ope-online-visualize-on-policy-interquartile-range"]], "scope_rl.ope.online.visualize_on_policy_policy_value": [[108, "scope-rl-ope-online-visualize-on-policy-policy-value"]], "scope_rl.ope.online.visualize_on_policy_policy_value_with_variance": [[109, "scope-rl-ope-online-visualize-on-policy-policy-value-with-variance"]], "scope_rl.ope.ope": [[110, "module-scope_rl.ope.ope"]], "scope_rl.ope.ope.CumulativeDistributionOPE": [[111, "scope-rl-ope-ope-cumulativedistributionope"]], "scope_rl.ope.ope.OffPolicyEvaluation": [[112, "scope-rl-ope-ope-offpolicyevaluation"]], "scope_rl.ope.ops": [[113, "module-scope_rl.ope.ops"]], "scope_rl.ope.ops.OffPolicySelection": [[114, "scope-rl-ope-ops-offpolicyselection"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous": [[115, "module-scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning": [[116, "scope-rl-ope-weight-value-learning-augmented-lagrangian-learning-continuous-continuousdicestateactionwightvaluelearning"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning": [[117, "scope-rl-ope-weight-value-learning-augmented-lagrangian-learning-continuous-continuousdicestatewightvaluelearning"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete": [[118, "module-scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning": [[119, "scope-rl-ope-weight-value-learning-augmented-lagrangian-learning-discrete-discretedicestateactionwightvaluelearning"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning": [[120, "scope-rl-ope-weight-value-learning-augmented-lagrangian-learning-discrete-discretedicestatewightvaluelearning"]], "scope_rl.ope.weight_value_learning.base": [[121, "module-scope_rl.ope.weight_value_learning.base"]], "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner": [[122, "scope-rl-ope-weight-value-learning-base-baseweightvaluelearner"]], "scope_rl.ope.weight_value_learning.function": [[123, "module-scope_rl.ope.weight_value_learning.function"]], "scope_rl.ope.weight_value_learning.function.ContinuousQFunction": [[124, "scope-rl-ope-weight-value-learning-function-continuousqfunction"]], "scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction": [[125, "scope-rl-ope-weight-value-learning-function-continuousstateactionweightfunction"]], "scope_rl.ope.weight_value_learning.function.DiscreteQFunction": [[126, "scope-rl-ope-weight-value-learning-function-discreteqfunction"]], "scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction": [[127, "scope-rl-ope-weight-value-learning-function-discretestateactionweightfunction"]], "scope_rl.ope.weight_value_learning.function.StateWeightFunction": [[128, "scope-rl-ope-weight-value-learning-function-stateweightfunction"]], "scope_rl.ope.weight_value_learning.function.VFunction": [[129, "scope-rl-ope-weight-value-learning-function-vfunction"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous": [[130, "module-scope_rl.ope.weight_value_learning.minimax_value_learning_continuous"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning": [[131, "scope-rl-ope-weight-value-learning-minimax-value-learning-continuous-continuousminimaxstateactionvaluelearning"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning": [[132, "scope-rl-ope-weight-value-learning-minimax-value-learning-continuous-continuousminimaxstatevaluelearning"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete": [[133, "module-scope_rl.ope.weight_value_learning.minimax_value_learning_discrete"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning": [[134, "scope-rl-ope-weight-value-learning-minimax-value-learning-discrete-discreteminimaxstateactionvaluelearning"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning": [[135, "scope-rl-ope-weight-value-learning-minimax-value-learning-discrete-discreteminimaxstatevaluelearning"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous": [[136, "module-scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning": [[137, "scope-rl-ope-weight-value-learning-minimax-weight-learning-continuous-continuousminimaxstateactionweightlearning"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning": [[138, "scope-rl-ope-weight-value-learning-minimax-weight-learning-continuous-continuousminimaxstateweightlearning"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete": [[139, "module-scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning": [[140, "scope-rl-ope-weight-value-learning-minimax-weight-learning-discrete-discreteminimaxstateactionweightlearning"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning": [[141, "scope-rl-ope-weight-value-learning-minimax-weight-learning-discrete-discreteminimaxstateweightlearning"]], "scope_rl.policy.head": [[142, "module-scope_rl.policy.head"]], "scope_rl.policy.head.BaseHead": [[143, "scope-rl-policy-head-basehead"]], "scope_rl.policy.head.ContinuousEvalHead": [[144, "scope-rl-policy-head-continuousevalhead"]], "scope_rl.policy.head.EpsilonGreedyHead": [[145, "scope-rl-policy-head-epsilongreedyhead"]], "scope_rl.policy.head.GaussianHead": [[146, "scope-rl-policy-head-gaussianhead"]], "scope_rl.policy.head.OnlineHead": [[147, "scope-rl-policy-head-onlinehead"]], "scope_rl.policy.head.SoftmaxHead": [[148, "scope-rl-policy-head-softmaxhead"]], "scope_rl.policy.head.TruncatedGaussianHead": [[149, "scope-rl-policy-head-truncatedgaussianhead"]], "scope_rl.utils": [[150, "module-scope_rl.utils"]], "scope_rl.utils.MinMaxActionScaler": [[151, "scope-rl-utils-minmaxactionscaler"]], "scope_rl.utils.MinMaxScaler": [[152, "scope-rl-utils-minmaxscaler"]], "scope_rl.utils.MultipleInputDict": [[153, "scope-rl-utils-multipleinputdict"]], "scope_rl.utils.MultipleLoggedDataset": [[154, "scope-rl-utils-multipleloggeddataset"]], "scope_rl.utils.NewGymAPIWrapper": [[155, "scope-rl-utils-newgymapiwrapper"]], "scope_rl.utils.OldGymAPIWrapper": [[156, "scope-rl-utils-oldgymapiwrapper"]], "scope_rl.utils.check_array": [[157, "scope-rl-utils-check-array"]], "scope_rl.utils.check_input_dict": [[158, "scope-rl-utils-check-input-dict"]], "scope_rl.utils.check_logged_dataset": [[159, "scope-rl-utils-check-logged-dataset"]], "scope_rl.utils.cosine_kernel": [[160, "scope-rl-utils-cosine-kernel"]], "scope_rl.utils.defaultdict_to_dict": [[161, "scope-rl-utils-defaultdict-to-dict"]], "scope_rl.utils.epanechnikov_kernel": [[162, "scope-rl-utils-epanechnikov-kernel"]], "scope_rl.utils.estimate_confidence_interval_by_bootstrap": [[163, "scope-rl-utils-estimate-confidence-interval-by-bootstrap"]], "scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein": [[164, "scope-rl-utils-estimate-confidence-interval-by-empirical-bernstein"]], "scope_rl.utils.estimate_confidence_interval_by_hoeffding": [[165, "scope-rl-utils-estimate-confidence-interval-by-hoeffding"]], "scope_rl.utils.estimate_confidence_interval_by_t_test": [[166, "scope-rl-utils-estimate-confidence-interval-by-t-test"]], "scope_rl.utils.gaussian_kernel": [[167, "scope-rl-utils-gaussian-kernel"]], "scope_rl.utils.l2_distance": [[168, "scope-rl-utils-l2-distance"]], "scope_rl.utils.triangular_kernel": [[169, "scope-rl-utils-triangular-kernel"]], "scope_rl.utils.uniform_kernel": [[170, "scope-rl-utils-uniform-kernel"]], "Why SCOPE-RL?": [[171, "why-scope-rl"]], "Motivation": [[171, "motivation"]], "Key contributions": [[171, "key-contributions"]], "End-to-end implementation of Offline RL and OPE": [[171, "end-to-end-implementation-of-offline-rl-and-ope"]], "Variety of OPE estimators and evaluation protocol of OPE": [[171, "variety-of-ope-estimators-and-evaluation-protocol-of-ope"]], "Cumulative Distribution OPE for risk function estimation": [[171, "cumulative-distribution-ope-for-risk-function-estimation"]], "Risk-Return Assessments of OPS": [[171, "risk-return-assessments-of-ops"]], "Comparisons with the existing platforms": [[171, "comparisons-with-the-existing-platforms"]], "Supported Implementation": [[172, "supported-implementation"], [184, "supported-implementation"], [193, "supported-implementation"], [196, "supported-implementation"], [198, "supported-implementation"]], "Create OPE Input": [[172, "create-ope-input"]], "Basic Off-Policy Evaluation (OPE)": [[172, "basic-off-policy-evaluation-ope"]], "Direct Method (DM)": [[172, "direct-method-dm"], [172, "implementation-cd-dm"]], "Trajectory-wise Importance Sampling (TIS)": [[172, "trajectory-wise-importance-sampling-tis"], [172, "implementation-cd-tis"]], "Per-Decision Importance Sampling (PDIS)": [[172, "per-decision-importance-sampling-pdis"]], "Doubly Robust (DR)": [[172, "doubly-robust-dr"]], "Self-Normalized estimators": [[172, "self-normalized-estimators"]], "Marginalized Importance Sampling Estimators": [[172, "marginalized-importance-sampling-estimators"]], "Double Reinforcement Learning (DRL)": [[172, "double-reinforcement-learning-drl"]], "Spectrum of Off-Policy Estimators (SOPE)": [[172, "spectrum-of-off-policy-estimators-sope"]], "High Confidence Off-Policy Evaluation (HCOPE)": [[172, "high-confidence-off-policy-evaluation-hcope"]], "Extension to the Continuous Action Space": [[172, "extension-to-the-continuous-action-space"]], "Cumulative Distribution Off-Policy Evaluation (CD-OPE)": [[172, "cumulative-distribution-off-policy-evaluation-cd-ope"]], "Trajectory-wise Doubly Robust (TDR)": [[172, "trajectory-wise-doubly-robust-tdr"]], "Evaluation Metrics of OPE/OPS": [[172, "evaluation-metrics-of-ope-ops"]], "Example Codes for Assessing OPE Estimators": [[173, "example-codes-for-assessing-ope-estimators"]], "Prerequisite": [[173, "prerequisite"], [179, "prerequisite"]], "Assessing OPE/OPS results": [[173, "assessing-ope-ops-results"]], "Assessments with conventional metrics": [[173, "assessments-with-conventional-metrics"]], "Assessments with top-k deployment results": [[173, "assessments-with-top-k-deployment-results"]], "Visualizing top-k deployment results": [[173, "visualizing-top-k-deployment-results"]], "Visualizing the true and estimated policy performances": [[173, "visualizing-the-true-and-estimated-policy-performances"]], "Example Codes for Basic Off-Policy Evaluation": [[174, "example-codes-for-basic-off-policy-evaluation"]], "Logged Dataset": [[174, "logged-dataset"], [175, "logged-dataset"], [178, "logged-dataset"], [180, "logged-dataset"]], "Inputs": [[174, "inputs"], [175, "inputs"], [178, "inputs"]], "OPE with importance sampling-based estimators": [[174, "ope-with-importance-sampling-based-estimators"], [175, "ope-with-importance-sampling-based-estimators"]], "OPE with model-based estimators": [[174, "ope-with-model-based-estimators"], [175, "ope-with-model-based-estimators"]], "OPE with marginal importance sampling-based estimators": [[174, "ope-with-marginal-importance-sampling-based-estimators"]], "OPE with Double Reinforcement Learning": [[174, "ope-with-double-reinforcement-learning"]], "Scalers for value and weight learning": [[174, "scalers-for-value-and-weight-learning"]], "Off-Policy Evaluation": [[174, "off-policy-evaluation"], [175, "off-policy-evaluation"], [178, "off-policy-evaluation"], [187, "off-policy-evaluation"]], "Choosing the \u201cSpectrum\u201d of OPE for marginal estimators": [[174, "choosing-the-spectrum-of-ope-for-marginal-estimators"]], "Choosing a kernel for continuous-action OPE": [[174, "choosing-a-kernel-for-continuous-action-ope"]], "Choosing a probability bound for high confidence OPE": [[174, "choosing-a-probability-bound-for-high-confidence-ope"]], "Evaluating the \u201caccuracy\u201d of OPE": [[174, "evaluating-the-accuracy-of-ope"]], "Example Codes for Cumulative Distribution OPE": [[175, "example-codes-for-cumulative-distribution-ope"]], "Estimating Cumulative Distribution Function (CDF)": [[175, "estimating-cumulative-distribution-function-cdf"]], "Estimating Mean (i.e., policy value)": [[175, "estimating-mean-i-e-policy-value"]], "Estimating Variance": [[175, "estimating-variance"]], "Estimating Conditional Value at Risk (CVaR)": [[175, "estimating-conditional-value-at-risk-cvar"]], "Estimating Interquartile Range": [[175, "estimating-interquartile-range"]], "Example Codes for Custom OPE Estimators": [[176, "example-codes-for-custom-ope-estimators"]], "Estimator for Basic OPE": [[176, "estimator-for-basic-ope"]], "Estimator for Cumulative Distribution OPE": [[176, "estimator-for-cumulative-distribution-ope"]], "Example Codes": [[177, "example-codes"]], "SCOPE-RL": [[177, "scope-rl"], [181, "scope-rl"], [182, "scope-rl"]], "Basic and High-Confidence Off-Policy Evaluation (OPE):": [[177, "basic-and-high-confidence-off-policy-evaluation-ope"]], "Cumulative Distribution OPE (CD-OPE):": [[177, "cumulative-distribution-ope-cd-ope"]], "Off-Policy Selection": [[177, "off-policy-selection"], [178, "off-policy-selection"], [179, "off-policy-selection"], [187, "off-policy-selection"]], "Assessing OPE Estimators": [[177, "assessing-ope-estimators"]], "Implementing Custom OPE Estimators:": [[177, "implementing-custom-ope-estimators"]], "Handling Multiple Datasets:": [[177, "handling-multiple-datasets"]], "Handling Real-World Datasets:": [[177, "handling-real-world-datasets"]], "Example Codes with Multiple Logged Dataset and Behavior Policies": [[178, "example-codes-with-multiple-logged-dataset-and-behavior-policies"]], "Cumulative Distribution Off-Policy Evaluation": [[178, "cumulative-distribution-off-policy-evaluation"]], "Assessments of OPE via top-k Policy Selection": [[178, "assessments-of-ope-via-top-k-policy-selection"]], "Validating True and Estimated Policy Performance": [[178, "validating-true-and-estimated-policy-performance"]], "Example Codes for Off-Policy Selection": [[179, "example-codes-for-off-policy-selection"]], "OPS via basic OPE": [[179, "ops-via-basic-ope"]], "OPS via cumulative distribution OPE": [[179, "ops-via-cumulative-distribution-ope"]], "Obtaining oracle selection results": [[179, "obtaining-oracle-selection-results"]], "Calling visualization functions from ope / cd_ope instances": [[179, "calling-visualization-functions-from-ope-cd-ope-instances"]], "Guidelines for Preparing Real-World Datasets": [[180, "guidelines-for-preparing-real-world-datasets"]], "Input Dict": [[180, "input-dict"]], "FAQs": [[181, "faqs"]], "Overview": [[182, "overview"], [186, "overview"], [187, "overview"], [193, "overview"], [196, "overview"], [198, "overview"]], "Implementation": [[182, "implementation"]], "Data Collection Policy and Offline RL": [[182, "data-collection-policy-and-offline-rl"]], "Meta class": [[182, "meta-class"]], "Discrete": [[182, "discrete"]], "Continuous": [[182, "continuous"]], "Basic OPE": [[182, "basic-ope"], [188, "basic-ope"]], "Basic estimators": [[182, "basic-estimators"]], "State Marginal Estimators": [[182, "state-marginal-estimators"]], "State-Action Marginal Estimators": [[182, "state-action-marginal-estimators"]], "Double Reinforcement Learning": [[182, "double-reinforcement-learning"]], "Weight and Value Learning Methods": [[182, "weight-and-value-learning-methods"]], "High Confidence OPE": [[182, "high-confidence-ope"]], "Cumulative Distribution OPE": [[182, "cumulative-distribution-ope"], [188, "cumulative-distribution-ope"]], "Estimators": [[182, "estimators"]], "Metrics of Interest": [[182, "metrics-of-interest"]], "Off-Policy Selection Metrics": [[182, "off-policy-selection-metrics"]], "OPE metrics": [[182, "ope-metrics"]], "OPS metrics (performance of top k deployment policies)": [[182, "ops-metrics-performance-of-top-k-deployment-policies"]], "Citation": [[182, "citation"], [183, "citation"], [192, "citation"], [193, "citation"], [196, "citation"], [198, "citation"]], "Google Group": [[182, "google-group"]], "Contact": [[182, "contact"], [193, "contact"], [196, "contact"], [198, "contact"]], "Contribution": [[182, "contribution"], [193, "contribution"], [196, "contribution"], [198, "contribution"]], "Table of Contents": [[182, "table-of-contents"]], "Getting Started:": [[182, null]], "Online & Offline RL:": [[182, null]], "Off-Policy Evaluation & Selection:": [[182, null]], "Our Proposal:": [[182, null]], "Sub-packages:": [[182, null]], "Package References:": [[182, null]], "See also:": [[182, null]], "Installation": [[183, "installation"]], "Synthetic Dataset Generation": [[184, "synthetic-dataset-generation"]], "Offline Learning": [[184, "offline-learning"]], "Policy Wrapper": [[184, "policy-wrapper"]], "DiscreteHead": [[184, "discretehead"]], "ContinuousHead": [[184, "continuoushead"]], "OnlineHead": [[184, "onlinehead"]], "Online Evaluation": [[184, "online-evaluation"]], "News": [[185, "news"]], "2023": [[185, "id1"]], "Online Reinforcement Learning": [[186, "online-reinforcement-learning"]], "On-Policy Policy Gradient": [[186, "on-policy-policy-gradient"]], "Q-Learning": [[186, "q-learning"]], "Actor-Critic": [[186, "actor-critic"]], "Offline Reinforcement Learning": [[186, "offline-reinforcement-learning"], [188, "offline-reinforcement-learning"]], "The problem of Extrapolation Error": [[186, "the-problem-of-extrapolation-error"]], "Divergence Regularization and Behavior Cloning": [[186, "divergence-regularization-and-behavior-cloning"]], "Uncertainty Estimation": [[186, "uncertainty-estimation"]], "Conservative Q-Learning": [[186, "conservative-q-learning"]], "Implicit Q-Learning": [[186, "implicit-q-learning"]], "Policy Value Estimation": [[187, "policy-value-estimation"]], "Cumulative Distribution and Risk Function Estimation": [[187, "cumulative-distribution-and-risk-function-estimation"]], "Quickstart": [[188, "quickstart"]], "Synthetic Dataset Generation and Data Preprocessing": [[188, "synthetic-dataset-generation-and-data-preprocessing"]], "Off-Policy Evaluation (OPE) and Selection (OPS)": [[188, "off-policy-evaluation-ope-and-selection-ops"]], "Off-Policy Selection and Evaluation of OPE/OPS": [[188, "off-policy-selection-and-evaluation-of-ope-ops"]], "References": [[189, "references"]], "Papers": [[189, "papers"]], "Projects": [[189, "projects"]], "Release Notes": [[190, "release-notes"]], "SCOPR-RL Package Reference": [[191, "scopr-rl-package-reference"]], "dataset module": [[191, "dataset-module"]], "policy module": [[191, "policy-module"]], "ope module": [[191, "ope-module"]], "pipeline": [[191, "pipeline"]], "OPE estimators": [[191, "ope-estimators"]], "weight and value learning methods": [[191, "weight-and-value-learning-methods"]], "others": [[191, "others"], [191, "scope-rl-api-utils"], [199, "others"]], "Risk-Return Assessments of OPE via SharpRatio@k": [[192, "risk-return-assessments-of-ope-via-sharpratio-k"]], "Background": [[192, "background"]], "Issues of existing evaluation protocols of OPE/OPS": [[192, "issues-of-existing-evaluation-protocols-of-ope-ops"]], "Evaluating the top-k risk-return tradeoff in policy deployment": [[192, "evaluating-the-top-k-risk-return-tradeoff-in-policy-deployment"]], "OPE benchmarks with SharpRatio@k": [[192, "ope-benchmarks-with-sharpratio-k"]], "BasicGym": [[193, "basicgym"]], "Basic Setting": [[193, "basic-setting"], [196, "basic-setting"], [198, "basic-setting"]], "Standard Environment": [[193, "standard-environment"], [196, "standard-environment"], [198, "standard-environment"]], "Custom Environment": [[193, "custom-environment"], [196, "custom-environment"], [198, "custom-environment"]], "Configurative Modules": [[193, "configurative-modules"], [196, "configurative-modules"], [198, "configurative-modules"]], "Quickstart and Configurations": [[193, "quickstart-and-configurations"], [196, "quickstart-and-configurations"], [198, "quickstart-and-configurations"]], "Standard BasicEnv": [[193, "standard-basicenv"]], "Customized BasicEnv": [[193, "customized-basicenv"]], "BasicGym Package Reference": [[194, "basicgym-package-reference"]], "env module": [[194, "env-module"], [197, "env-module"], [199, "env-module"]], "simulation module": [[194, "simulation-module"], [197, "simulation-module"], [199, "simulation-module"]], "Sub-packages": [[195, "sub-packages"]], "RECGym": [[196, "recgym"]], "Standard RECEnv": [[196, "standard-recenv"]], "Customized RECEnv": [[196, "customized-recenv"]], "RECGym Package Reference": [[197, "recgym-package-reference"]], "RTBGym": [[198, "rtbgym"]], "Standard RTBEnv": [[198, "standard-rtbenv"]], "Customized RTBEnv": [[198, "customized-rtbenv"]], "Wrapper class for custom bidding setup": [[198, "wrapper-class-for-custom-bidding-setup"]], "RTBGym Package Reference": [[199, "rtbgym-package-reference"]], "Visualization Tools": [[200, "visualization-tools"]], "Welcome!": [[201, "welcome"]]}, "indexentries": {"basicenv (class in basicgym.envs.synthetic)": [[0, "basicgym.envs.synthetic.BasicEnv"], [1, "basicgym.envs.synthetic.BasicEnv"]], "basicgym.envs.synthetic": [[0, "module-basicgym.envs.synthetic"]], "module": [[0, "module-basicgym.envs.synthetic"], [2, "module-basicgym.envs.simulator.base"], [5, "module-basicgym.envs.simulator.function"], [8, "module-scope_rl.dataset.base"], [10, "module-scope_rl.dataset.synthetic"], [12, "module-recgym.envs.rec"], [14, "module-recgym.envs.simulator.base"], [16, "module-recgym.envs.simulator.function"], [18, "module-rtbgym.envs.rtb"], [20, "module-rtbgym.envs.wrapper_rtb"], [22, "module-rtbgym.envs.simulator.base"], [26, "module-rtbgym.envs.simulator.bidder"], [28, "module-rtbgym.envs.simulator.function"], [32, "module-rtbgym.envs.simulator.rtb_synthetic"], [34, "module-rtbgym.utils"], [38, "module-scope_rl.ope.continuous.basic_estimators"], [46, "module-scope_rl.ope.continuous.cumulative_distribution_estimators"], [52, "module-scope_rl.ope.continuous.marginal_estimators"], [63, "module-scope_rl.ope.discrete.basic_estimators"], [71, "module-scope_rl.ope.discrete.cumulative_distribution_estimators"], [77, "module-scope_rl.ope.discrete.marginal_estimators"], [88, "module-scope_rl.ope.estimators_base"], [94, "module-scope_rl.ope.input"], [96, "module-scope_rl.ope.online"], [110, "module-scope_rl.ope.ope"], [113, "module-scope_rl.ope.ops"], [115, "module-scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous"], [118, "module-scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete"], [121, "module-scope_rl.ope.weight_value_learning.base"], [123, "module-scope_rl.ope.weight_value_learning.function"], [130, "module-scope_rl.ope.weight_value_learning.minimax_value_learning_continuous"], [133, "module-scope_rl.ope.weight_value_learning.minimax_value_learning_discrete"], [136, "module-scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous"], [139, "module-scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete"], [142, "module-scope_rl.policy.head"], [150, "module-scope_rl.utils"]], "reset() (basicgym.envs.synthetic.basicenv method)": [[0, "basicgym.envs.synthetic.BasicEnv.reset"], [1, "basicgym.envs.synthetic.BasicEnv.reset"]], "step() (basicgym.envs.synthetic.basicenv method)": [[0, "basicgym.envs.synthetic.BasicEnv.step"], [1, "basicgym.envs.synthetic.BasicEnv.step"]], "baserewardfunction (class in basicgym.envs.simulator.base)": [[2, "basicgym.envs.simulator.base.BaseRewardFunction"], [3, "basicgym.envs.simulator.base.BaseRewardFunction"]], "basestatetransitionfunction (class in basicgym.envs.simulator.base)": [[2, "basicgym.envs.simulator.base.BaseStateTransitionFunction"], [4, "basicgym.envs.simulator.base.BaseStateTransitionFunction"]], "basicgym.envs.simulator.base": [[2, "module-basicgym.envs.simulator.base"]], "mean_reward_function() (basicgym.envs.simulator.base.baserewardfunction method)": [[2, "basicgym.envs.simulator.base.BaseRewardFunction.mean_reward_function"], [3, "basicgym.envs.simulator.base.BaseRewardFunction.mean_reward_function"]], "sample_reward() (basicgym.envs.simulator.base.baserewardfunction method)": [[2, "basicgym.envs.simulator.base.BaseRewardFunction.sample_reward"], [3, "basicgym.envs.simulator.base.BaseRewardFunction.sample_reward"]], "step() (basicgym.envs.simulator.base.basestatetransitionfunction method)": [[2, "basicgym.envs.simulator.base.BaseStateTransitionFunction.step"], [4, "basicgym.envs.simulator.base.BaseStateTransitionFunction.step"]], "rewardfunction (class in basicgym.envs.simulator.function)": [[5, "basicgym.envs.simulator.function.RewardFunction"], [6, "basicgym.envs.simulator.function.RewardFunction"]], "statetransitionfunction (class in basicgym.envs.simulator.function)": [[5, "basicgym.envs.simulator.function.StateTransitionFunction"], [7, "basicgym.envs.simulator.function.StateTransitionFunction"]], "basicgym.envs.simulator.function": [[5, "module-basicgym.envs.simulator.function"]], "mean_reward_function() (basicgym.envs.simulator.function.rewardfunction method)": [[5, "basicgym.envs.simulator.function.RewardFunction.mean_reward_function"], [6, "basicgym.envs.simulator.function.RewardFunction.mean_reward_function"]], "step() (basicgym.envs.simulator.function.statetransitionfunction method)": [[5, "basicgym.envs.simulator.function.StateTransitionFunction.step"], [7, "basicgym.envs.simulator.function.StateTransitionFunction.step"]], "sample_reward() (basicgym.envs.simulator.function.rewardfunction method)": [[6, "basicgym.envs.simulator.function.RewardFunction.sample_reward"]], "basedataset (class in scope_rl.dataset.base)": [[8, "scope_rl.dataset.base.BaseDataset"], [9, "scope_rl.dataset.base.BaseDataset"]], "obtain_episodes() (scope_rl.dataset.base.basedataset method)": [[8, "scope_rl.dataset.base.BaseDataset.obtain_episodes"], [9, "scope_rl.dataset.base.BaseDataset.obtain_episodes"]], "obtain_steps() (scope_rl.dataset.base.basedataset method)": [[8, "scope_rl.dataset.base.BaseDataset.obtain_steps"], [9, "scope_rl.dataset.base.BaseDataset.obtain_steps"]], "scope_rl.dataset.base": [[8, "module-scope_rl.dataset.base"]], "syntheticdataset (class in scope_rl.dataset.synthetic)": [[10, "scope_rl.dataset.synthetic.SyntheticDataset"], [11, "scope_rl.dataset.synthetic.SyntheticDataset"]], "obtain_episodes() (scope_rl.dataset.synthetic.syntheticdataset method)": [[10, "scope_rl.dataset.synthetic.SyntheticDataset.obtain_episodes"], [11, "scope_rl.dataset.synthetic.SyntheticDataset.obtain_episodes"]], "obtain_steps() (scope_rl.dataset.synthetic.syntheticdataset method)": [[10, "scope_rl.dataset.synthetic.SyntheticDataset.obtain_steps"], [11, "scope_rl.dataset.synthetic.SyntheticDataset.obtain_steps"]], "scope_rl.dataset.synthetic": [[10, "module-scope_rl.dataset.synthetic"]], "recenv (class in recgym.envs.rec)": [[12, "recgym.envs.rec.RECEnv"], [13, "recgym.envs.rec.RECEnv"]], "recgym.envs.rec": [[12, "module-recgym.envs.rec"]], "reset() (recgym.envs.rec.recenv method)": [[12, "recgym.envs.rec.RECEnv.reset"], [13, "recgym.envs.rec.RECEnv.reset"]], "step() (recgym.envs.rec.recenv method)": [[12, "recgym.envs.rec.RECEnv.step"], [13, "recgym.envs.rec.RECEnv.step"]], "baseusermodel (class in recgym.envs.simulator.base)": [[14, "recgym.envs.simulator.base.BaseUserModel"], [15, "recgym.envs.simulator.base.BaseUserModel"]], "recgym.envs.simulator.base": [[14, "module-recgym.envs.simulator.base"]], "reward_function() (recgym.envs.simulator.base.baseusermodel method)": [[14, "recgym.envs.simulator.base.BaseUserModel.reward_function"], [15, "recgym.envs.simulator.base.BaseUserModel.reward_function"]], "user_preference_dynamics() (recgym.envs.simulator.base.baseusermodel method)": [[14, "recgym.envs.simulator.base.BaseUserModel.user_preference_dynamics"], [15, "recgym.envs.simulator.base.BaseUserModel.user_preference_dynamics"]], "usermodel (class in recgym.envs.simulator.function)": [[16, "recgym.envs.simulator.function.UserModel"], [17, "recgym.envs.simulator.function.UserModel"]], "recgym.envs.simulator.function": [[16, "module-recgym.envs.simulator.function"]], "reward_function() (recgym.envs.simulator.function.usermodel method)": [[16, "recgym.envs.simulator.function.UserModel.reward_function"], [17, "recgym.envs.simulator.function.UserModel.reward_function"]], "user_preference_dynamics() (recgym.envs.simulator.function.usermodel method)": [[16, "recgym.envs.simulator.function.UserModel.user_preference_dynamics"], [17, "recgym.envs.simulator.function.UserModel.user_preference_dynamics"]], "rtbenv (class in rtbgym.envs.rtb)": [[18, "rtbgym.envs.rtb.RTBEnv"], [19, "rtbgym.envs.rtb.RTBEnv"]], "reset() (rtbgym.envs.rtb.rtbenv method)": [[18, "rtbgym.envs.rtb.RTBEnv.reset"], [19, "rtbgym.envs.rtb.RTBEnv.reset"]], "rtbgym.envs.rtb": [[18, "module-rtbgym.envs.rtb"]], "step() (rtbgym.envs.rtb.rtbenv method)": [[18, "rtbgym.envs.rtb.RTBEnv.step"], [19, "rtbgym.envs.rtb.RTBEnv.step"]], "customizedrtbenv (class in rtbgym.envs.wrapper_rtb)": [[20, "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv"], [21, "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv"]], "reset() (rtbgym.envs.wrapper_rtb.customizedrtbenv method)": [[20, "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv.reset"], [21, "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv.reset"]], "rtbgym.envs.wrapper_rtb": [[20, "module-rtbgym.envs.wrapper_rtb"]], "step() (rtbgym.envs.wrapper_rtb.customizedrtbenv method)": [[20, "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv.step"], [21, "rtbgym.envs.wrapper_rtb.CustomizedRTBEnv.step"]], "baseclickandconversionrate (class in rtbgym.envs.simulator.base)": [[22, "rtbgym.envs.simulator.base.BaseClickAndConversionRate"], [23, "rtbgym.envs.simulator.base.BaseClickAndConversionRate"]], "basesimulator (class in rtbgym.envs.simulator.base)": [[22, "rtbgym.envs.simulator.base.BaseSimulator"], [24, "rtbgym.envs.simulator.base.BaseSimulator"]], "basewinningpricedistribution (class in rtbgym.envs.simulator.base)": [[22, "rtbgym.envs.simulator.base.BaseWinningPriceDistribution"], [25, "rtbgym.envs.simulator.base.BaseWinningPriceDistribution"]], "calc_and_sample_outcome() (rtbgym.envs.simulator.base.basesimulator method)": [[22, "rtbgym.envs.simulator.base.BaseSimulator.calc_and_sample_outcome"], [24, "rtbgym.envs.simulator.base.BaseSimulator.calc_and_sample_outcome"]], "calc_prob() (rtbgym.envs.simulator.base.baseclickandconversionrate method)": [[22, "rtbgym.envs.simulator.base.BaseClickAndConversionRate.calc_prob"], [23, "rtbgym.envs.simulator.base.BaseClickAndConversionRate.calc_prob"]], "generate_auction() (rtbgym.envs.simulator.base.basesimulator method)": [[22, "rtbgym.envs.simulator.base.BaseSimulator.generate_auction"], [24, "rtbgym.envs.simulator.base.BaseSimulator.generate_auction"]], "map_idx_to_features() (rtbgym.envs.simulator.base.basesimulator method)": [[22, "rtbgym.envs.simulator.base.BaseSimulator.map_idx_to_features"], [24, "rtbgym.envs.simulator.base.BaseSimulator.map_idx_to_features"]], "rtbgym.envs.simulator.base": [[22, "module-rtbgym.envs.simulator.base"]], "sample_outcome() (rtbgym.envs.simulator.base.baseclickandconversionrate method)": [[22, "rtbgym.envs.simulator.base.BaseClickAndConversionRate.sample_outcome"], [23, "rtbgym.envs.simulator.base.BaseClickAndConversionRate.sample_outcome"]], "sample_outcome() (rtbgym.envs.simulator.base.basewinningpricedistribution method)": [[22, "rtbgym.envs.simulator.base.BaseWinningPriceDistribution.sample_outcome"], [25, "rtbgym.envs.simulator.base.BaseWinningPriceDistribution.sample_outcome"]], "bidder (class in rtbgym.envs.simulator.bidder)": [[26, "rtbgym.envs.simulator.bidder.Bidder"], [27, "rtbgym.envs.simulator.bidder.Bidder"]], "auto_fit_scaler() (rtbgym.envs.simulator.bidder.bidder method)": [[26, "rtbgym.envs.simulator.bidder.Bidder.auto_fit_scaler"], [27, "rtbgym.envs.simulator.bidder.Bidder.auto_fit_scaler"]], "custom_set_reward_predictor() (rtbgym.envs.simulator.bidder.bidder method)": [[26, "rtbgym.envs.simulator.bidder.Bidder.custom_set_reward_predictor"], [27, "rtbgym.envs.simulator.bidder.Bidder.custom_set_reward_predictor"]], "custom_set_scaler() (rtbgym.envs.simulator.bidder.bidder method)": [[26, "rtbgym.envs.simulator.bidder.Bidder.custom_set_scaler"], [27, "rtbgym.envs.simulator.bidder.Bidder.custom_set_scaler"]], "determine_bid_price() (rtbgym.envs.simulator.bidder.bidder method)": [[26, "rtbgym.envs.simulator.bidder.Bidder.determine_bid_price"], [27, "rtbgym.envs.simulator.bidder.Bidder.determine_bid_price"]], "fit_reward_predictor() (rtbgym.envs.simulator.bidder.bidder method)": [[26, "rtbgym.envs.simulator.bidder.Bidder.fit_reward_predictor"], [27, "rtbgym.envs.simulator.bidder.Bidder.fit_reward_predictor"]], "rtbgym.envs.simulator.bidder": [[26, "module-rtbgym.envs.simulator.bidder"]], "clickthroughrate (class in rtbgym.envs.simulator.function)": [[28, "rtbgym.envs.simulator.function.ClickThroughRate"], [29, "rtbgym.envs.simulator.function.ClickThroughRate"]], "conversionrate (class in rtbgym.envs.simulator.function)": [[28, "rtbgym.envs.simulator.function.ConversionRate"], [30, "rtbgym.envs.simulator.function.ConversionRate"]], "winningpricedistribution (class in rtbgym.envs.simulator.function)": [[28, "rtbgym.envs.simulator.function.WinningPriceDistribution"], [31, "rtbgym.envs.simulator.function.WinningPriceDistribution"]], "calc_prob() (rtbgym.envs.simulator.function.clickthroughrate method)": [[28, "rtbgym.envs.simulator.function.ClickThroughRate.calc_prob"], [29, "rtbgym.envs.simulator.function.ClickThroughRate.calc_prob"]], "calc_prob() (rtbgym.envs.simulator.function.conversionrate method)": [[28, "rtbgym.envs.simulator.function.ConversionRate.calc_prob"], [30, "rtbgym.envs.simulator.function.ConversionRate.calc_prob"]], "rtbgym.envs.simulator.function": [[28, "module-rtbgym.envs.simulator.function"]], "sample_outcome() (rtbgym.envs.simulator.function.clickthroughrate method)": [[28, "rtbgym.envs.simulator.function.ClickThroughRate.sample_outcome"], [29, "rtbgym.envs.simulator.function.ClickThroughRate.sample_outcome"]], "sample_outcome() (rtbgym.envs.simulator.function.conversionrate method)": [[28, "rtbgym.envs.simulator.function.ConversionRate.sample_outcome"], [30, "rtbgym.envs.simulator.function.ConversionRate.sample_outcome"]], "sample_outcome() (rtbgym.envs.simulator.function.winningpricedistribution method)": [[28, "rtbgym.envs.simulator.function.WinningPriceDistribution.sample_outcome"], [31, "rtbgym.envs.simulator.function.WinningPriceDistribution.sample_outcome"]], "rtbsyntheticsimulator (class in rtbgym.envs.simulator.rtb_synthetic)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator"]], "rtbsyntheticsimulator.clickthroughrate (class in rtbgym.envs.simulator.rtb_synthetic)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ClickThroughRate"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ClickThroughRate"]], "rtbsyntheticsimulator.conversionrate (class in rtbgym.envs.simulator.rtb_synthetic)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ConversionRate"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ConversionRate"]], "rtbsyntheticsimulator.winningpricedistribution (class in rtbgym.envs.simulator.rtb_synthetic)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.WinningPriceDistribution"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.WinningPriceDistribution"]], "calc_and_sample_outcome() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.calc_and_sample_outcome"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.calc_and_sample_outcome"]], "calc_prob() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator.clickthroughrate method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ClickThroughRate.calc_prob"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ClickThroughRate.calc_prob"]], "calc_prob() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator.conversionrate method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ConversionRate.calc_prob"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ConversionRate.calc_prob"]], "generate_auction() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.generate_auction"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.generate_auction"]], "map_idx_to_features() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.map_idx_to_features"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.map_idx_to_features"]], "rtbgym.envs.simulator.rtb_synthetic": [[32, "module-rtbgym.envs.simulator.rtb_synthetic"]], "sample_outcome() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator.clickthroughrate method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ClickThroughRate.sample_outcome"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ClickThroughRate.sample_outcome"]], "sample_outcome() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator.conversionrate method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ConversionRate.sample_outcome"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.ConversionRate.sample_outcome"]], "sample_outcome() (rtbgym.envs.simulator.rtb_synthetic.rtbsyntheticsimulator.winningpricedistribution method)": [[32, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.WinningPriceDistribution.sample_outcome"], [33, "rtbgym.envs.simulator.rtb_synthetic.RTBSyntheticSimulator.WinningPriceDistribution.sample_outcome"]], "normaldistribution (class in rtbgym.utils)": [[34, "rtbgym.utils.NormalDistribution"], [35, "rtbgym.utils.NormalDistribution"]], "check_array() (in module rtbgym.utils)": [[34, "rtbgym.utils.check_array"], [36, "rtbgym.utils.check_array"]], "rtbgym.utils": [[34, "module-rtbgym.utils"]], "sample() (rtbgym.utils.normaldistribution method)": [[34, "rtbgym.utils.NormalDistribution.sample"], [35, "rtbgym.utils.NormalDistribution.sample"]], "sigmoid() (in module rtbgym.utils)": [[34, "rtbgym.utils.sigmoid"], [37, "rtbgym.utils.sigmoid"]], "directmethod (class in scope_rl.ope.continuous.basic_estimators)": [[38, "scope_rl.ope.continuous.basic_estimators.DirectMethod"], [39, "scope_rl.ope.continuous.basic_estimators.DirectMethod"]], "doublyrobust (class in scope_rl.ope.continuous.basic_estimators)": [[38, "scope_rl.ope.continuous.basic_estimators.DoublyRobust"], [40, "scope_rl.ope.continuous.basic_estimators.DoublyRobust"]], "perdecisionimportancesampling (class in scope_rl.ope.continuous.basic_estimators)": [[38, "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling"], [41, "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling"]], "selfnormalizeddr (class in scope_rl.ope.continuous.basic_estimators)": [[38, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR"], [42, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR"]], "selfnormalizedpdis (class in scope_rl.ope.continuous.basic_estimators)": [[38, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS"], [43, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS"]], "selfnormalizedtis (class in scope_rl.ope.continuous.basic_estimators)": [[38, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS"], [44, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS"]], "trajectorywiseimportancesampling (class in scope_rl.ope.continuous.basic_estimators)": [[38, "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling"], [45, "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling"]], "estimate_interval() (scope_rl.ope.continuous.basic_estimators.directmethod method)": [[38, "scope_rl.ope.continuous.basic_estimators.DirectMethod.estimate_interval"], [39, "scope_rl.ope.continuous.basic_estimators.DirectMethod.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.basic_estimators.doublyrobust method)": [[38, "scope_rl.ope.continuous.basic_estimators.DoublyRobust.estimate_interval"], [40, "scope_rl.ope.continuous.basic_estimators.DoublyRobust.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.basic_estimators.perdecisionimportancesampling method)": [[38, "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling.estimate_interval"], [41, "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.basic_estimators.trajectorywiseimportancesampling method)": [[38, "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling.estimate_interval"], [45, "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.basic_estimators.directmethod method)": [[38, "scope_rl.ope.continuous.basic_estimators.DirectMethod.estimate_policy_value"], [39, "scope_rl.ope.continuous.basic_estimators.DirectMethod.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.basic_estimators.doublyrobust method)": [[38, "scope_rl.ope.continuous.basic_estimators.DoublyRobust.estimate_policy_value"], [40, "scope_rl.ope.continuous.basic_estimators.DoublyRobust.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.basic_estimators.perdecisionimportancesampling method)": [[38, "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling.estimate_policy_value"], [41, "scope_rl.ope.continuous.basic_estimators.PerDecisionImportanceSampling.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.basic_estimators.trajectorywiseimportancesampling method)": [[38, "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling.estimate_policy_value"], [45, "scope_rl.ope.continuous.basic_estimators.TrajectoryWiseImportanceSampling.estimate_policy_value"]], "scope_rl.ope.continuous.basic_estimators": [[38, "module-scope_rl.ope.continuous.basic_estimators"]], "estimate_interval() (scope_rl.ope.continuous.basic_estimators.selfnormalizeddr method)": [[42, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.basic_estimators.selfnormalizeddr method)": [[42, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedDR.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.continuous.basic_estimators.selfnormalizedpdis method)": [[43, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.basic_estimators.selfnormalizedpdis method)": [[43, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedPDIS.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.continuous.basic_estimators.selfnormalizedtis method)": [[44, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.basic_estimators.selfnormalizedtis method)": [[44, "scope_rl.ope.continuous.basic_estimators.SelfNormalizedTIS.estimate_policy_value"]], "cumulativedistributiondm (class in scope_rl.ope.continuous.cumulative_distribution_estimators)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM"], [47, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM"]], "cumulativedistributionsntdr (class in scope_rl.ope.continuous.cumulative_distribution_estimators)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR"], [48, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR"]], "cumulativedistributionsntis (class in scope_rl.ope.continuous.cumulative_distribution_estimators)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS"], [49, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS"]], "cumulativedistributiontdr (class in scope_rl.ope.continuous.cumulative_distribution_estimators)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR"], [50, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR"]], "cumulativedistributiontis (class in scope_rl.ope.continuous.cumulative_distribution_estimators)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS"], [51, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS"]], "estimate_conditional_value_at_risk() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiondm method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_conditional_value_at_risk"], [47, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_conditional_value_at_risk"]], "estimate_conditional_value_at_risk() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_conditional_value_at_risk"], [50, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_conditional_value_at_risk"]], "estimate_conditional_value_at_risk() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontis method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_conditional_value_at_risk"], [51, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_conditional_value_at_risk"]], "estimate_cumulative_distribution_function() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiondm method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_cumulative_distribution_function"], [47, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_cumulative_distribution_function"], [48, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_cumulative_distribution_function"], [49, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_cumulative_distribution_function"], [50, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontis method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_cumulative_distribution_function"], [51, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_cumulative_distribution_function"]], "estimate_interquartile_range() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiondm method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_interquartile_range"], [47, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_interquartile_range"]], "estimate_interquartile_range() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_interquartile_range"], [50, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_interquartile_range"]], "estimate_interquartile_range() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontis method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_interquartile_range"], [51, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_interquartile_range"]], "estimate_mean() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiondm method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_mean"], [47, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_mean"]], "estimate_mean() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_mean"], [50, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_mean"]], "estimate_mean() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontis method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_mean"], [51, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_mean"]], "estimate_variance() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiondm method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_variance"], [47, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_variance"]], "estimate_variance() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_variance"], [50, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_variance"]], "estimate_variance() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributiontis method)": [[46, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_variance"], [51, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_variance"]], "scope_rl.ope.continuous.cumulative_distribution_estimators": [[46, "module-scope_rl.ope.continuous.cumulative_distribution_estimators"]], "estimate_conditional_value_at_risk() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[48, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_conditional_value_at_risk"]], "estimate_interquartile_range() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[48, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_interquartile_range"]], "estimate_mean() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[48, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_mean"]], "estimate_variance() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[48, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_variance"]], "estimate_conditional_value_at_risk() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[49, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_conditional_value_at_risk"]], "estimate_interquartile_range() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[49, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_interquartile_range"]], "estimate_mean() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[49, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_mean"]], "estimate_variance() (scope_rl.ope.continuous.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[49, "scope_rl.ope.continuous.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_variance"]], "doublereinforcementlearning (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning"], [53, "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning"]], "stateactionmarginaldr (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR"], [54, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR"]], "stateactionmarginalis (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS"], [55, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS"]], "stateactionmarginalsndr (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR"], [56, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR"]], "stateactionmarginalsnis (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS"], [57, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS"]], "statemarginaldm (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM"], [58, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM"]], "statemarginaldr (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR"], [59, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR"]], "statemarginalis (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS"], [60, "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS"]], "statemarginalsndr (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR"], [61, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR"]], "statemarginalsnis (class in scope_rl.ope.continuous.marginal_estimators)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS"], [62, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.doublereinforcementlearning method)": [[52, "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning.estimate_interval"], [53, "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginaldr method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR.estimate_interval"], [54, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginalis method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS.estimate_interval"], [55, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.statemarginaldm method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM.estimate_interval"], [58, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.statemarginaldr method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR.estimate_interval"], [59, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR.estimate_interval"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.statemarginalis method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS.estimate_interval"], [60, "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.doublereinforcementlearning method)": [[52, "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning.estimate_policy_value"], [53, "scope_rl.ope.continuous.marginal_estimators.DoubleReinforcementLearning.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginaldr method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR.estimate_policy_value"], [54, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalDR.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginalis method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS.estimate_policy_value"], [55, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalIS.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.statemarginaldm method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM.estimate_policy_value"], [58, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDM.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.statemarginaldr method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR.estimate_policy_value"], [59, "scope_rl.ope.continuous.marginal_estimators.StateMarginalDR.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.statemarginalis method)": [[52, "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS.estimate_policy_value"], [60, "scope_rl.ope.continuous.marginal_estimators.StateMarginalIS.estimate_policy_value"]], "scope_rl.ope.continuous.marginal_estimators": [[52, "module-scope_rl.ope.continuous.marginal_estimators"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginalsndr method)": [[56, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginalsndr method)": [[56, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNDR.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginalsnis method)": [[57, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.stateactionmarginalsnis method)": [[57, "scope_rl.ope.continuous.marginal_estimators.StateActionMarginalSNIS.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.statemarginalsndr method)": [[61, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.statemarginalsndr method)": [[61, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNDR.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.continuous.marginal_estimators.statemarginalsnis method)": [[62, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.continuous.marginal_estimators.statemarginalsnis method)": [[62, "scope_rl.ope.continuous.marginal_estimators.StateMarginalSNIS.estimate_policy_value"]], "directmethod (class in scope_rl.ope.discrete.basic_estimators)": [[63, "scope_rl.ope.discrete.basic_estimators.DirectMethod"], [64, "scope_rl.ope.discrete.basic_estimators.DirectMethod"]], "doublyrobust (class in scope_rl.ope.discrete.basic_estimators)": [[63, "scope_rl.ope.discrete.basic_estimators.DoublyRobust"], [65, "scope_rl.ope.discrete.basic_estimators.DoublyRobust"]], "perdecisionimportancesampling (class in scope_rl.ope.discrete.basic_estimators)": [[63, "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling"], [66, "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling"]], "selfnormalizeddr (class in scope_rl.ope.discrete.basic_estimators)": [[63, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR"], [67, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR"]], "selfnormalizedpdis (class in scope_rl.ope.discrete.basic_estimators)": [[63, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS"], [68, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS"]], "selfnormalizedtis (class in scope_rl.ope.discrete.basic_estimators)": [[63, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS"], [69, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS"]], "trajectorywiseimportancesampling (class in scope_rl.ope.discrete.basic_estimators)": [[63, "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling"], [70, "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling"]], "estimate_interval() (scope_rl.ope.discrete.basic_estimators.directmethod method)": [[63, "scope_rl.ope.discrete.basic_estimators.DirectMethod.estimate_interval"], [64, "scope_rl.ope.discrete.basic_estimators.DirectMethod.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.basic_estimators.doublyrobust method)": [[63, "scope_rl.ope.discrete.basic_estimators.DoublyRobust.estimate_interval"], [65, "scope_rl.ope.discrete.basic_estimators.DoublyRobust.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.basic_estimators.perdecisionimportancesampling method)": [[63, "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling.estimate_interval"], [66, "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.basic_estimators.trajectorywiseimportancesampling method)": [[63, "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling.estimate_interval"], [70, "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.basic_estimators.directmethod method)": [[63, "scope_rl.ope.discrete.basic_estimators.DirectMethod.estimate_policy_value"], [64, "scope_rl.ope.discrete.basic_estimators.DirectMethod.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.basic_estimators.doublyrobust method)": [[63, "scope_rl.ope.discrete.basic_estimators.DoublyRobust.estimate_policy_value"], [65, "scope_rl.ope.discrete.basic_estimators.DoublyRobust.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.basic_estimators.perdecisionimportancesampling method)": [[63, "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling.estimate_policy_value"], [66, "scope_rl.ope.discrete.basic_estimators.PerDecisionImportanceSampling.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.basic_estimators.trajectorywiseimportancesampling method)": [[63, "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling.estimate_policy_value"], [70, "scope_rl.ope.discrete.basic_estimators.TrajectoryWiseImportanceSampling.estimate_policy_value"]], "scope_rl.ope.discrete.basic_estimators": [[63, "module-scope_rl.ope.discrete.basic_estimators"]], "estimate_interval() (scope_rl.ope.discrete.basic_estimators.selfnormalizeddr method)": [[67, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.basic_estimators.selfnormalizeddr method)": [[67, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedDR.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.discrete.basic_estimators.selfnormalizedpdis method)": [[68, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.basic_estimators.selfnormalizedpdis method)": [[68, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedPDIS.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.discrete.basic_estimators.selfnormalizedtis method)": [[69, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.basic_estimators.selfnormalizedtis method)": [[69, "scope_rl.ope.discrete.basic_estimators.SelfNormalizedTIS.estimate_policy_value"]], "cumulativedistributiondm (class in scope_rl.ope.discrete.cumulative_distribution_estimators)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM"], [72, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM"]], "cumulativedistributionsntdr (class in scope_rl.ope.discrete.cumulative_distribution_estimators)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR"], [73, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR"]], "cumulativedistributionsntis (class in scope_rl.ope.discrete.cumulative_distribution_estimators)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS"], [74, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS"]], "cumulativedistributiontdr (class in scope_rl.ope.discrete.cumulative_distribution_estimators)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR"], [75, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR"]], "cumulativedistributiontis (class in scope_rl.ope.discrete.cumulative_distribution_estimators)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS"], [76, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS"]], "estimate_conditional_value_at_risk() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiondm method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_conditional_value_at_risk"], [72, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_conditional_value_at_risk"]], "estimate_conditional_value_at_risk() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_conditional_value_at_risk"], [75, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_conditional_value_at_risk"]], "estimate_conditional_value_at_risk() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontis method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_conditional_value_at_risk"], [76, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_conditional_value_at_risk"]], "estimate_cumulative_distribution_function() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiondm method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_cumulative_distribution_function"], [72, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_cumulative_distribution_function"], [73, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_cumulative_distribution_function"], [74, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_cumulative_distribution_function"], [75, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_cumulative_distribution_function"]], "estimate_cumulative_distribution_function() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontis method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_cumulative_distribution_function"], [76, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_cumulative_distribution_function"]], "estimate_interquartile_range() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiondm method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_interquartile_range"], [72, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_interquartile_range"]], "estimate_interquartile_range() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_interquartile_range"], [75, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_interquartile_range"]], "estimate_interquartile_range() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontis method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_interquartile_range"], [76, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_interquartile_range"]], "estimate_mean() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiondm method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_mean"], [72, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_mean"]], "estimate_mean() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_mean"], [75, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_mean"]], "estimate_mean() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontis method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_mean"], [76, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_mean"]], "estimate_variance() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiondm method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_variance"], [72, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionDM.estimate_variance"]], "estimate_variance() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontdr method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_variance"], [75, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTDR.estimate_variance"]], "estimate_variance() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributiontis method)": [[71, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_variance"], [76, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionTIS.estimate_variance"]], "scope_rl.ope.discrete.cumulative_distribution_estimators": [[71, "module-scope_rl.ope.discrete.cumulative_distribution_estimators"]], "estimate_conditional_value_at_risk() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[73, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_conditional_value_at_risk"]], "estimate_interquartile_range() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[73, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_interquartile_range"]], "estimate_mean() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[73, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_mean"]], "estimate_variance() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntdr method)": [[73, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTDR.estimate_variance"]], "estimate_conditional_value_at_risk() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[74, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_conditional_value_at_risk"]], "estimate_interquartile_range() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[74, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_interquartile_range"]], "estimate_mean() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[74, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_mean"]], "estimate_variance() (scope_rl.ope.discrete.cumulative_distribution_estimators.cumulativedistributionsntis method)": [[74, "scope_rl.ope.discrete.cumulative_distribution_estimators.CumulativeDistributionSNTIS.estimate_variance"]], "doublereinforcementlearning (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning"], [78, "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning"]], "stateactionmarginaldr (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR"], [79, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR"]], "stateactionmarginalis (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS"], [80, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS"]], "stateactionmarginalsndr (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR"], [81, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR"]], "stateactionmarginalsnis (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS"], [82, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS"]], "statemarginaldm (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM"], [83, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM"]], "statemarginaldr (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR"], [84, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR"]], "statemarginalis (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS"], [85, "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS"]], "statemarginalsndr (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR"], [86, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR"]], "statemarginalsnis (class in scope_rl.ope.discrete.marginal_estimators)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS"], [87, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.doublereinforcementlearning method)": [[77, "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning.estimate_interval"], [78, "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginaldr method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR.estimate_interval"], [79, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginalis method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS.estimate_interval"], [80, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.statemarginaldm method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM.estimate_interval"], [83, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.statemarginaldr method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR.estimate_interval"], [84, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR.estimate_interval"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.statemarginalis method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS.estimate_interval"], [85, "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.doublereinforcementlearning method)": [[77, "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning.estimate_policy_value"], [78, "scope_rl.ope.discrete.marginal_estimators.DoubleReinforcementLearning.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginaldr method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR.estimate_policy_value"], [79, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalDR.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginalis method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS.estimate_policy_value"], [80, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalIS.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.statemarginaldm method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM.estimate_policy_value"], [83, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDM.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.statemarginaldr method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR.estimate_policy_value"], [84, "scope_rl.ope.discrete.marginal_estimators.StateMarginalDR.estimate_policy_value"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.statemarginalis method)": [[77, "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS.estimate_policy_value"], [85, "scope_rl.ope.discrete.marginal_estimators.StateMarginalIS.estimate_policy_value"]], "scope_rl.ope.discrete.marginal_estimators": [[77, "module-scope_rl.ope.discrete.marginal_estimators"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginalsndr method)": [[81, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginalsndr method)": [[81, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNDR.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginalsnis method)": [[82, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.stateactionmarginalsnis method)": [[82, "scope_rl.ope.discrete.marginal_estimators.StateActionMarginalSNIS.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.statemarginalsndr method)": [[86, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.statemarginalsndr method)": [[86, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNDR.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.discrete.marginal_estimators.statemarginalsnis method)": [[87, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.discrete.marginal_estimators.statemarginalsnis method)": [[87, "scope_rl.ope.discrete.marginal_estimators.StateMarginalSNIS.estimate_policy_value"]], "basecumulativedistributionopeestimator (class in scope_rl.ope.estimators_base)": [[88, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator"], [89, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator"]], "basemarginalopeestimator (class in scope_rl.ope.estimators_base)": [[88, "scope_rl.ope.estimators_base.BaseMarginalOPEEstimator"], [90, "scope_rl.ope.estimators_base.BaseMarginalOPEEstimator"]], "baseoffpolicyestimator (class in scope_rl.ope.estimators_base)": [[88, "scope_rl.ope.estimators_base.BaseOffPolicyEstimator"], [91, "scope_rl.ope.estimators_base.BaseOffPolicyEstimator"]], "basestateactionmarginalopeestimator (class in scope_rl.ope.estimators_base)": [[88, "scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator"], [92, "scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator"]], "basestatemarginalopeestimator (class in scope_rl.ope.estimators_base)": [[88, "scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator"], [93, "scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator"]], "estimate_conditional_value_at_risk() (scope_rl.ope.estimators_base.basecumulativedistributionopeestimator method)": [[88, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_conditional_value_at_risk"], [89, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_conditional_value_at_risk"]], "estimate_cumulative_distribution_function() (scope_rl.ope.estimators_base.basecumulativedistributionopeestimator method)": [[88, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_cumulative_distribution_function"], [89, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_cumulative_distribution_function"]], "estimate_interquartile_range() (scope_rl.ope.estimators_base.basecumulativedistributionopeestimator method)": [[88, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_interquartile_range"], [89, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_interquartile_range"]], "estimate_interval() (scope_rl.ope.estimators_base.baseoffpolicyestimator method)": [[88, "scope_rl.ope.estimators_base.BaseOffPolicyEstimator.estimate_interval"], [91, "scope_rl.ope.estimators_base.BaseOffPolicyEstimator.estimate_interval"]], "estimate_mean() (scope_rl.ope.estimators_base.basecumulativedistributionopeestimator method)": [[88, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_mean"], [89, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_mean"]], "estimate_policy_value() (scope_rl.ope.estimators_base.baseoffpolicyestimator method)": [[88, "scope_rl.ope.estimators_base.BaseOffPolicyEstimator.estimate_policy_value"], [91, "scope_rl.ope.estimators_base.BaseOffPolicyEstimator.estimate_policy_value"]], "estimate_variance() (scope_rl.ope.estimators_base.basecumulativedistributionopeestimator method)": [[88, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_variance"], [89, "scope_rl.ope.estimators_base.BaseCumulativeDistributionOPEEstimator.estimate_variance"]], "scope_rl.ope.estimators_base": [[88, "module-scope_rl.ope.estimators_base"]], "estimate_interval() (scope_rl.ope.estimators_base.basemarginalopeestimator method)": [[90, "scope_rl.ope.estimators_base.BaseMarginalOPEEstimator.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.estimators_base.basemarginalopeestimator method)": [[90, "scope_rl.ope.estimators_base.BaseMarginalOPEEstimator.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.estimators_base.basestateactionmarginalopeestimator method)": [[92, "scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.estimators_base.basestateactionmarginalopeestimator method)": [[92, "scope_rl.ope.estimators_base.BaseStateActionMarginalOPEEstimator.estimate_policy_value"]], "estimate_interval() (scope_rl.ope.estimators_base.basestatemarginalopeestimator method)": [[93, "scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator.estimate_interval"]], "estimate_policy_value() (scope_rl.ope.estimators_base.basestatemarginalopeestimator method)": [[93, "scope_rl.ope.estimators_base.BaseStateMarginalOPEEstimator.estimate_policy_value"]], "createopeinput (class in scope_rl.ope.input)": [[94, "scope_rl.ope.input.CreateOPEInput"], [95, "scope_rl.ope.input.CreateOPEInput"]], "build_and_fit_fqe() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.build_and_fit_FQE"], [95, "scope_rl.ope.input.CreateOPEInput.build_and_fit_FQE"]], "build_and_fit_state_action_dual_model() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_action_dual_model"], [95, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_action_dual_model"]], "build_and_fit_state_action_value_model() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_action_value_model"], [95, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_action_value_model"]], "build_and_fit_state_action_weight_model() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_action_weight_model"], [95, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_action_weight_model"]], "build_and_fit_state_dual_model() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_dual_model"], [95, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_dual_model"]], "build_and_fit_state_value_model() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_value_model"], [95, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_value_model"]], "build_and_fit_state_weight_model() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_weight_model"], [95, "scope_rl.ope.input.CreateOPEInput.build_and_fit_state_weight_model"]], "obtain_evaluation_policy_action() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_evaluation_policy_action"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_evaluation_policy_action"]], "obtain_evaluation_policy_action_dist() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_evaluation_policy_action_dist"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_evaluation_policy_action_dist"]], "obtain_evaluation_policy_action_prob_for_observed_state_action() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_evaluation_policy_action_prob_for_observed_state_action"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_evaluation_policy_action_prob_for_observed_state_action"]], "obtain_initial_state() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_initial_state"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_initial_state"]], "obtain_initial_state_value_prediction() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_initial_state_value_prediction"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_initial_state_value_prediction"]], "obtain_state_action_marginal_importance_weight() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_state_action_marginal_importance_weight"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_state_action_marginal_importance_weight"]], "obtain_state_action_value_prediction() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_state_action_value_prediction"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_state_action_value_prediction"]], "obtain_state_marginal_importance_weight() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_state_marginal_importance_weight"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_state_marginal_importance_weight"]], "obtain_whole_inputs() (scope_rl.ope.input.createopeinput method)": [[94, "scope_rl.ope.input.CreateOPEInput.obtain_whole_inputs"], [95, "scope_rl.ope.input.CreateOPEInput.obtain_whole_inputs"]], "scope_rl.ope.input": [[94, "module-scope_rl.ope.input"]], "calc_on_policy_conditional_value_at_risk() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.calc_on_policy_conditional_value_at_risk"], [97, "scope_rl.ope.online.calc_on_policy_conditional_value_at_risk"]], "calc_on_policy_cumulative_distribution_function() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.calc_on_policy_cumulative_distribution_function"], [98, "scope_rl.ope.online.calc_on_policy_cumulative_distribution_function"]], "calc_on_policy_interquartile_range() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.calc_on_policy_interquartile_range"], [99, "scope_rl.ope.online.calc_on_policy_interquartile_range"]], "calc_on_policy_policy_value() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.calc_on_policy_policy_value"], [100, "scope_rl.ope.online.calc_on_policy_policy_value"]], "calc_on_policy_policy_value_interval() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.calc_on_policy_policy_value_interval"], [101, "scope_rl.ope.online.calc_on_policy_policy_value_interval"]], "calc_on_policy_statistics() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.calc_on_policy_statistics"], [102, "scope_rl.ope.online.calc_on_policy_statistics"]], "calc_on_policy_variance() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.calc_on_policy_variance"], [103, "scope_rl.ope.online.calc_on_policy_variance"]], "rollout_policy_online() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.rollout_policy_online"], [104, "scope_rl.ope.online.rollout_policy_online"]], "scope_rl.ope.online": [[96, "module-scope_rl.ope.online"]], "visualize_on_policy_conditional_value_at_risk() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk"], [105, "scope_rl.ope.online.visualize_on_policy_conditional_value_at_risk"]], "visualize_on_policy_cumulative_distribution_function() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function"], [106, "scope_rl.ope.online.visualize_on_policy_cumulative_distribution_function"]], "visualize_on_policy_interquartile_range() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.visualize_on_policy_interquartile_range"], [107, "scope_rl.ope.online.visualize_on_policy_interquartile_range"]], "visualize_on_policy_policy_value() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.visualize_on_policy_policy_value"], [108, "scope_rl.ope.online.visualize_on_policy_policy_value"]], "visualize_on_policy_policy_value_with_variance() (in module scope_rl.ope.online)": [[96, "scope_rl.ope.online.visualize_on_policy_policy_value_with_variance"], [109, "scope_rl.ope.online.visualize_on_policy_policy_value_with_variance"]], "cumulativedistributionope (class in scope_rl.ope.ope)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE"]], "offpolicyevaluation (class in scope_rl.ope.ope)": [[110, "scope_rl.ope.ope.OffPolicyEvaluation"], [112, "scope_rl.ope.ope.OffPolicyEvaluation"]], "estimate_conditional_value_at_risk() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_conditional_value_at_risk"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_conditional_value_at_risk"]], "estimate_cumulative_distribution_function() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_cumulative_distribution_function"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_cumulative_distribution_function"]], "estimate_interquartile_range() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_interquartile_range"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_interquartile_range"]], "estimate_intervals() (scope_rl.ope.ope.offpolicyevaluation method)": [[110, "scope_rl.ope.ope.OffPolicyEvaluation.estimate_intervals"], [112, "scope_rl.ope.ope.OffPolicyEvaluation.estimate_intervals"]], "estimate_mean() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_mean"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_mean"]], "estimate_policy_value() (scope_rl.ope.ope.offpolicyevaluation method)": [[110, "scope_rl.ope.ope.OffPolicyEvaluation.estimate_policy_value"], [112, "scope_rl.ope.ope.OffPolicyEvaluation.estimate_policy_value"]], "estimate_variance() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_variance"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.estimate_variance"]], "evaluate_performance_of_ope_estimators() (scope_rl.ope.ope.offpolicyevaluation method)": [[110, "scope_rl.ope.ope.OffPolicyEvaluation.evaluate_performance_of_ope_estimators"], [112, "scope_rl.ope.ope.OffPolicyEvaluation.evaluate_performance_of_ope_estimators"]], "obtain_reward_scale() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.obtain_reward_scale"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.obtain_reward_scale"]], "scope_rl.ope.ope": [[110, "module-scope_rl.ope.ope"]], "summarize_off_policy_estimates() (scope_rl.ope.ope.offpolicyevaluation method)": [[110, "scope_rl.ope.ope.OffPolicyEvaluation.summarize_off_policy_estimates"], [112, "scope_rl.ope.ope.OffPolicyEvaluation.summarize_off_policy_estimates"]], "visualize_conditional_value_at_risk() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_conditional_value_at_risk"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_conditional_value_at_risk"]], "visualize_conditional_value_at_risk_with_multiple_estimates() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_conditional_value_at_risk_with_multiple_estimates"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_conditional_value_at_risk_with_multiple_estimates"]], "visualize_cumulative_distribution_function() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_cumulative_distribution_function"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_cumulative_distribution_function"]], "visualize_cumulative_distribution_function_with_multiple_estimates() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_cumulative_distribution_function_with_multiple_estimates"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_cumulative_distribution_function_with_multiple_estimates"]], "visualize_interquartile_range() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_interquartile_range"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_interquartile_range"]], "visualize_lower_quartile_with_multiple_estimates() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_lower_quartile_with_multiple_estimates"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_lower_quartile_with_multiple_estimates"]], "visualize_off_policy_estimates() (scope_rl.ope.ope.offpolicyevaluation method)": [[110, "scope_rl.ope.ope.OffPolicyEvaluation.visualize_off_policy_estimates"], [112, "scope_rl.ope.ope.OffPolicyEvaluation.visualize_off_policy_estimates"]], "visualize_policy_value() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_policy_value"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_policy_value"]], "visualize_policy_value_with_multiple_estimates() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_policy_value_with_multiple_estimates"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_policy_value_with_multiple_estimates"]], "visualize_policy_value_with_multiple_estimates() (scope_rl.ope.ope.offpolicyevaluation method)": [[110, "scope_rl.ope.ope.OffPolicyEvaluation.visualize_policy_value_with_multiple_estimates"], [112, "scope_rl.ope.ope.OffPolicyEvaluation.visualize_policy_value_with_multiple_estimates"]], "visualize_variance_with_multiple_estimates() (scope_rl.ope.ope.cumulativedistributionope method)": [[110, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_variance_with_multiple_estimates"], [111, "scope_rl.ope.ope.CumulativeDistributionOPE.visualize_variance_with_multiple_estimates"]], "offpolicyselection (class in scope_rl.ope.ops)": [[113, "scope_rl.ope.ops.OffPolicySelection"], [114, "scope_rl.ope.ops.OffPolicySelection"]], "obtain_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope"]], "obtain_topk_conditional_value_at_risk_selected_by_standard_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_conditional_value_at_risk_selected_by_standard_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_conditional_value_at_risk_selected_by_standard_ope"]], "obtain_topk_lower_quartile_selected_by_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_lower_quartile_selected_by_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_lower_quartile_selected_by_cumulative_distribution_ope"]], "obtain_topk_lower_quartile_selected_by_standard_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_lower_quartile_selected_by_standard_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_lower_quartile_selected_by_standard_ope"]], "obtain_topk_policy_value_selected_by_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_policy_value_selected_by_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_policy_value_selected_by_cumulative_distribution_ope"]], "obtain_topk_policy_value_selected_by_lower_bound() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_policy_value_selected_by_lower_bound"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_policy_value_selected_by_lower_bound"]], "obtain_topk_policy_value_selected_by_standard_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_policy_value_selected_by_standard_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_topk_policy_value_selected_by_standard_ope"]], "obtain_true_selection_result() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.obtain_true_selection_result"], [114, "scope_rl.ope.ops.OffPolicySelection.obtain_true_selection_result"]], "scope_rl.ope.ops": [[113, "module-scope_rl.ope.ops"]], "select_by_conditional_value_at_risk() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.select_by_conditional_value_at_risk"], [114, "scope_rl.ope.ops.OffPolicySelection.select_by_conditional_value_at_risk"]], "select_by_lower_quartile() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.select_by_lower_quartile"], [114, "scope_rl.ope.ops.OffPolicySelection.select_by_lower_quartile"]], "select_by_policy_value() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.select_by_policy_value"], [114, "scope_rl.ope.ops.OffPolicySelection.select_by_policy_value"]], "select_by_policy_value_lower_bound() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.select_by_policy_value_lower_bound"], [114, "scope_rl.ope.ops.OffPolicySelection.select_by_policy_value_lower_bound"]], "select_by_policy_value_via_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.select_by_policy_value_via_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.select_by_policy_value_via_cumulative_distribution_ope"]], "visualize_conditional_value_at_risk_for_selection() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_conditional_value_at_risk_for_selection"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_conditional_value_at_risk_for_selection"]], "visualize_conditional_value_at_risk_for_validation() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_conditional_value_at_risk_for_validation"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_conditional_value_at_risk_for_validation"]], "visualize_conditional_value_at_risk_with_multiple_estimates() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_conditional_value_at_risk_with_multiple_estimates"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_conditional_value_at_risk_with_multiple_estimates"]], "visualize_cumulative_distribution_function_for_selection() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_cumulative_distribution_function_for_selection"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_cumulative_distribution_function_for_selection"]], "visualize_cumulative_distribution_function_with_multiple_estimates() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_cumulative_distribution_function_with_multiple_estimates"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_cumulative_distribution_function_with_multiple_estimates"]], "visualize_interquartile_range_for_selection() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_interquartile_range_for_selection"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_interquartile_range_for_selection"]], "visualize_lower_quartile_for_validation() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_lower_quartile_for_validation"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_lower_quartile_for_validation"]], "visualize_lower_quartile_with_multiple_estimates() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_lower_quartile_with_multiple_estimates"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_lower_quartile_with_multiple_estimates"]], "visualize_policy_value_for_selection() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_for_selection"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_for_selection"]], "visualize_policy_value_for_validation() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_for_validation"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_for_validation"]], "visualize_policy_value_lower_bound_for_validation() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_lower_bound_for_validation"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_lower_bound_for_validation"]], "visualize_policy_value_of_cumulative_distribution_ope_for_selection() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_of_cumulative_distribution_ope_for_selection"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_of_cumulative_distribution_ope_for_selection"]], "visualize_policy_value_of_cumulative_distribution_ope_for_validation() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_of_cumulative_distribution_ope_for_validation"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_of_cumulative_distribution_ope_for_validation"]], "visualize_policy_value_with_multiple_estimates_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_with_multiple_estimates_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_with_multiple_estimates_cumulative_distribution_ope"]], "visualize_policy_value_with_multiple_estimates_standard_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_with_multiple_estimates_standard_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_policy_value_with_multiple_estimates_standard_ope"]], "visualize_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope"]], "visualize_topk_conditional_value_at_risk_selected_by_standard_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_conditional_value_at_risk_selected_by_standard_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_conditional_value_at_risk_selected_by_standard_ope"]], "visualize_topk_lower_quartile_selected_by_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_lower_quartile_selected_by_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_lower_quartile_selected_by_cumulative_distribution_ope"]], "visualize_topk_lower_quartile_selected_by_standard_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_lower_quartile_selected_by_standard_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_lower_quartile_selected_by_standard_ope"]], "visualize_topk_policy_value_selected_by_cumulative_distribution_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_policy_value_selected_by_cumulative_distribution_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_policy_value_selected_by_cumulative_distribution_ope"]], "visualize_topk_policy_value_selected_by_lower_bound() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_policy_value_selected_by_lower_bound"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_policy_value_selected_by_lower_bound"]], "visualize_topk_policy_value_selected_by_standard_ope() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_policy_value_selected_by_standard_ope"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_topk_policy_value_selected_by_standard_ope"]], "visualize_variance_for_validation() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_variance_for_validation"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_variance_for_validation"]], "visualize_variance_with_multiple_estimates() (scope_rl.ope.ops.offpolicyselection method)": [[113, "scope_rl.ope.ops.OffPolicySelection.visualize_variance_with_multiple_estimates"], [114, "scope_rl.ope.ops.OffPolicySelection.visualize_variance_with_multiple_estimates"]], "continuousdicestateactionwightvaluelearning (class in scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning"]], "continuousdicestatewightvaluelearning (class in scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning"]], "fit() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.fit"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.fit"]], "fit() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestatewightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.fit"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.fit"]], "fit_predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.fit_predict"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.fit_predict"]], "fit_predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestatewightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.fit_predict"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.fit_predict"]], "load() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.load"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.load"]], "load() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestatewightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.load"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.load"]], "predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict"]], "predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestatewightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.predict"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.predict"]], "predict_q_function() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_q_function"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_q_function"]], "predict_v_function() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_v_function"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_v_function"]], "predict_value() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_value"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_value"]], "predict_value() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestatewightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.predict_value"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.predict_value"]], "predict_weight() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_weight"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.predict_weight"]], "predict_weight() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestatewightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.predict_weight"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.predict_weight"]], "save() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestateactionwightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.save"], [116, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateActionWightValueLearning.save"]], "save() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.continuousdicestatewightvaluelearning method)": [[115, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.save"], [117, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous.ContinuousDiceStateWightValueLearning.save"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous": [[115, "module-scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_continuous"]], "discretedicestateactionwightvaluelearning (class in scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning"]], "discretedicestatewightvaluelearning (class in scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning"]], "fit() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.fit"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.fit"]], "fit() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestatewightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.fit"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.fit"]], "fit_predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.fit_predict"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.fit_predict"]], "fit_predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestatewightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.fit_predict"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.fit_predict"]], "load() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.load"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.load"]], "load() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestatewightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.load"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.load"]], "predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict"]], "predict() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestatewightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.predict"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.predict"]], "predict_q_function() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_q_function"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_q_function"]], "predict_q_function_for_all_actions() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_q_function_for_all_actions"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_q_function_for_all_actions"]], "predict_v_function() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_v_function"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_v_function"]], "predict_value() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_value"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_value"]], "predict_value() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestatewightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.predict_value"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.predict_value"]], "predict_weight() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_weight"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.predict_weight"]], "predict_weight() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestatewightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.predict_weight"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.predict_weight"]], "save() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestateactionwightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.save"], [119, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateActionWightValueLearning.save"]], "save() (scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.discretedicestatewightvaluelearning method)": [[118, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.save"], [120, "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete.DiscreteDiceStateWightValueLearning.save"]], "scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete": [[118, "module-scope_rl.ope.weight_value_learning.augmented_lagrangian_learning_discrete"]], "baseweightvaluelearner (class in scope_rl.ope.weight_value_learning.base)": [[121, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner"], [122, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner"]], "fit() (scope_rl.ope.weight_value_learning.base.baseweightvaluelearner method)": [[121, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.fit"], [122, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.fit"]], "load() (scope_rl.ope.weight_value_learning.base.baseweightvaluelearner method)": [[121, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.load"], [122, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.load"]], "predict() (scope_rl.ope.weight_value_learning.base.baseweightvaluelearner method)": [[121, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.predict"], [122, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.predict"]], "save() (scope_rl.ope.weight_value_learning.base.baseweightvaluelearner method)": [[121, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.save"], [122, "scope_rl.ope.weight_value_learning.base.BaseWeightValueLearner.save"]], "scope_rl.ope.weight_value_learning.base": [[121, "module-scope_rl.ope.weight_value_learning.base"]], "continuousqfunction (class in scope_rl.ope.weight_value_learning.function)": [[123, "scope_rl.ope.weight_value_learning.function.ContinuousQFunction"], [124, "scope_rl.ope.weight_value_learning.function.ContinuousQFunction"]], "continuousstateactionweightfunction (class in scope_rl.ope.weight_value_learning.function)": [[123, "scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction"], [125, "scope_rl.ope.weight_value_learning.function.ContinuousStateActionWeightFunction"]], "discreteqfunction (class in scope_rl.ope.weight_value_learning.function)": [[123, "scope_rl.ope.weight_value_learning.function.DiscreteQFunction"], [126, "scope_rl.ope.weight_value_learning.function.DiscreteQFunction"]], "discretestateactionweightfunction (class in scope_rl.ope.weight_value_learning.function)": [[123, "scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction"], [127, "scope_rl.ope.weight_value_learning.function.DiscreteStateActionWeightFunction"]], "stateweightfunction (class in scope_rl.ope.weight_value_learning.function)": [[123, "scope_rl.ope.weight_value_learning.function.StateWeightFunction"], [128, "scope_rl.ope.weight_value_learning.function.StateWeightFunction"]], "vfunction (class in scope_rl.ope.weight_value_learning.function)": [[123, "scope_rl.ope.weight_value_learning.function.VFunction"], [129, "scope_rl.ope.weight_value_learning.function.VFunction"]], "scope_rl.ope.weight_value_learning.function": [[123, "module-scope_rl.ope.weight_value_learning.function"]], "continuousminimaxstateactionvaluelearning (class in scope_rl.ope.weight_value_learning.minimax_value_learning_continuous)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning"]], "continuousminimaxstatevaluelearning (class in scope_rl.ope.weight_value_learning.minimax_value_learning_continuous)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning"]], "fit() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.fit"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.fit"]], "fit() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstatevaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.fit"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.fit"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.fit_predict"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.fit_predict"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstatevaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.fit_predict"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.fit_predict"]], "load() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.load"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.load"]], "load() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstatevaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.load"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.load"]], "predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict"]], "predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstatevaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.predict"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.predict"]], "predict_q_function() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict_q_function"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict_q_function"]], "predict_v_function() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict_v_function"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict_v_function"]], "predict_v_function() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstatevaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.predict_v_function"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.predict_v_function"]], "predict_value() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict_value"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.predict_value"]], "predict_value() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstatevaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.predict_value"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.predict_value"]], "save() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstateactionvaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.save"], [131, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateActionValueLearning.save"]], "save() (scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.continuousminimaxstatevaluelearning method)": [[130, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.save"], [132, "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous.ContinuousMinimaxStateValueLearning.save"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_continuous": [[130, "module-scope_rl.ope.weight_value_learning.minimax_value_learning_continuous"]], "discreteminimaxstateactionvaluelearning (class in scope_rl.ope.weight_value_learning.minimax_value_learning_discrete)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning"]], "discreteminimaxstatevaluelearning (class in scope_rl.ope.weight_value_learning.minimax_value_learning_discrete)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning"]], "fit() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.fit"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.fit"]], "fit() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstatevaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.fit"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.fit"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.fit_predict"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.fit_predict"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstatevaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.fit_predict"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.fit_predict"]], "load() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.load"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.load"]], "load() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstatevaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.load"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.load"]], "predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict"]], "predict() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstatevaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.predict"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.predict"]], "predict_q_function() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_q_function"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_q_function"]], "predict_q_function_for_all_actions() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_q_function_for_all_actions"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_q_function_for_all_actions"]], "predict_v_function() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_v_function"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_v_function"]], "predict_v_function() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstatevaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.predict_v_function"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.predict_v_function"]], "predict_value() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_value"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.predict_value"]], "predict_value() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstatevaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.predict_value"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.predict_value"]], "save() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstateactionvaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.save"], [134, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateActionValueLearning.save"]], "save() (scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.discreteminimaxstatevaluelearning method)": [[133, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.save"], [135, "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete.DiscreteMinimaxStateValueLearning.save"]], "scope_rl.ope.weight_value_learning.minimax_value_learning_discrete": [[133, "module-scope_rl.ope.weight_value_learning.minimax_value_learning_discrete"]], "continuousminimaxstateactionweightlearning (class in scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning"], [137, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning"]], "continuousminimaxstateweightlearning (class in scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning"]], "fit() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateactionweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.fit"], [137, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.fit"]], "fit() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.fit"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.fit"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateactionweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.fit_predict"], [137, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.fit_predict"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.fit_predict"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.fit_predict"]], "load() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateactionweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.load"], [137, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.load"]], "load() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.load"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.load"]], "predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateactionweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.predict"], [137, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.predict"]], "predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict"]], "predict_state_action_marginal_importance_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict_state_action_marginal_importance_weight"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict_state_action_marginal_importance_weight"]], "predict_state_marginal_importance_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict_state_marginal_importance_weight"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict_state_marginal_importance_weight"]], "predict_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateactionweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.predict_weight"], [137, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.predict_weight"]], "predict_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict_weight"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.predict_weight"]], "save() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateactionweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.save"], [137, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateActionWeightLearning.save"]], "save() (scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.continuousminimaxstateweightlearning method)": [[136, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.save"], [138, "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous.ContinuousMinimaxStateWeightLearning.save"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous": [[136, "module-scope_rl.ope.weight_value_learning.minimax_weight_learning_continuous"]], "discreteminimaxstateactionweightlearning (class in scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning"], [140, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning"]], "discreteminimaxstateweightlearning (class in scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning"]], "fit() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateactionweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.fit"], [140, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.fit"]], "fit() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.fit"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.fit"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateactionweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.fit_predict"], [140, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.fit_predict"]], "fit_predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.fit_predict"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.fit_predict"]], "load() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateactionweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.load"], [140, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.load"]], "load() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.load"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.load"]], "predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateactionweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.predict"], [140, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.predict"]], "predict() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict"]], "predict_state_action_marginal_importance_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict_state_action_marginal_importance_weight"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict_state_action_marginal_importance_weight"]], "predict_state_marginal_importance_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict_state_marginal_importance_weight"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict_state_marginal_importance_weight"]], "predict_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateactionweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.predict_weight"], [140, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.predict_weight"]], "predict_weight() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict_weight"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.predict_weight"]], "save() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateactionweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.save"], [140, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateActionWeightLearning.save"]], "save() (scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.discreteminimaxstateweightlearning method)": [[139, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.save"], [141, "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete.DiscreteMinimaxStateWeightLearning.save"]], "scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete": [[139, "module-scope_rl.ope.weight_value_learning.minimax_weight_learning_discrete"]], "basehead (class in scope_rl.policy.head)": [[142, "scope_rl.policy.head.BaseHead"], [143, "scope_rl.policy.head.BaseHead"]], "continuousevalhead (class in scope_rl.policy.head)": [[142, "scope_rl.policy.head.ContinuousEvalHead"], [144, "scope_rl.policy.head.ContinuousEvalHead"]], "epsilongreedyhead (class in scope_rl.policy.head)": [[142, "scope_rl.policy.head.EpsilonGreedyHead"], [145, "scope_rl.policy.head.EpsilonGreedyHead"]], "gaussianhead (class in scope_rl.policy.head)": [[142, "scope_rl.policy.head.GaussianHead"], [146, "scope_rl.policy.head.GaussianHead"]], "onlinehead (class in scope_rl.policy.head)": [[142, "scope_rl.policy.head.OnlineHead"], [147, "scope_rl.policy.head.OnlineHead"]], "softmaxhead (class in scope_rl.policy.head)": [[142, "scope_rl.policy.head.SoftmaxHead"], [148, "scope_rl.policy.head.SoftmaxHead"]], "truncatedgaussianhead (class in scope_rl.policy.head)": [[142, "scope_rl.policy.head.TruncatedGaussianHead"], [149, "scope_rl.policy.head.TruncatedGaussianHead"]], "calc_action_choice_probability() (scope_rl.policy.head.basehead method)": [[142, "scope_rl.policy.head.BaseHead.calc_action_choice_probability"], [143, "scope_rl.policy.head.BaseHead.calc_action_choice_probability"]], "calc_action_choice_probability() (scope_rl.policy.head.epsilongreedyhead method)": [[142, "scope_rl.policy.head.EpsilonGreedyHead.calc_action_choice_probability"], [145, "scope_rl.policy.head.EpsilonGreedyHead.calc_action_choice_probability"]], "calc_action_choice_probability() (scope_rl.policy.head.onlinehead method)": [[142, "scope_rl.policy.head.OnlineHead.calc_action_choice_probability"], [147, "scope_rl.policy.head.OnlineHead.calc_action_choice_probability"]], "calc_action_choice_probability() (scope_rl.policy.head.softmaxhead method)": [[142, "scope_rl.policy.head.SoftmaxHead.calc_action_choice_probability"], [148, "scope_rl.policy.head.SoftmaxHead.calc_action_choice_probability"]], "calc_pscore_given_action() (scope_rl.policy.head.basehead method)": [[142, "scope_rl.policy.head.BaseHead.calc_pscore_given_action"], [143, "scope_rl.policy.head.BaseHead.calc_pscore_given_action"]], "calc_pscore_given_action() (scope_rl.policy.head.epsilongreedyhead method)": [[142, "scope_rl.policy.head.EpsilonGreedyHead.calc_pscore_given_action"], [145, "scope_rl.policy.head.EpsilonGreedyHead.calc_pscore_given_action"]], "calc_pscore_given_action() (scope_rl.policy.head.gaussianhead method)": [[142, "scope_rl.policy.head.GaussianHead.calc_pscore_given_action"], [146, "scope_rl.policy.head.GaussianHead.calc_pscore_given_action"]], "calc_pscore_given_action() (scope_rl.policy.head.onlinehead method)": [[142, "scope_rl.policy.head.OnlineHead.calc_pscore_given_action"], [147, "scope_rl.policy.head.OnlineHead.calc_pscore_given_action"]], "calc_pscore_given_action() (scope_rl.policy.head.softmaxhead method)": [[142, "scope_rl.policy.head.SoftmaxHead.calc_pscore_given_action"], [148, "scope_rl.policy.head.SoftmaxHead.calc_pscore_given_action"]], "calc_pscore_given_action() (scope_rl.policy.head.truncatedgaussianhead method)": [[142, "scope_rl.policy.head.TruncatedGaussianHead.calc_pscore_given_action"], [149, "scope_rl.policy.head.TruncatedGaussianHead.calc_pscore_given_action"]], "predict_online() (scope_rl.policy.head.basehead method)": [[142, "scope_rl.policy.head.BaseHead.predict_online"], [143, "scope_rl.policy.head.BaseHead.predict_online"]], "predict_value_online() (scope_rl.policy.head.basehead method)": [[142, "scope_rl.policy.head.BaseHead.predict_value_online"], [143, "scope_rl.policy.head.BaseHead.predict_value_online"]], "sample_action_and_output_pscore() (scope_rl.policy.head.basehead method)": [[142, "scope_rl.policy.head.BaseHead.sample_action_and_output_pscore"], [143, "scope_rl.policy.head.BaseHead.sample_action_and_output_pscore"]], "sample_action_and_output_pscore() (scope_rl.policy.head.epsilongreedyhead method)": [[142, "scope_rl.policy.head.EpsilonGreedyHead.sample_action_and_output_pscore"], [145, "scope_rl.policy.head.EpsilonGreedyHead.sample_action_and_output_pscore"]], "sample_action_and_output_pscore() (scope_rl.policy.head.gaussianhead method)": [[142, "scope_rl.policy.head.GaussianHead.sample_action_and_output_pscore"], [146, "scope_rl.policy.head.GaussianHead.sample_action_and_output_pscore"]], "sample_action_and_output_pscore() (scope_rl.policy.head.onlinehead method)": [[142, "scope_rl.policy.head.OnlineHead.sample_action_and_output_pscore"], [147, "scope_rl.policy.head.OnlineHead.sample_action_and_output_pscore"]], "sample_action_and_output_pscore() (scope_rl.policy.head.softmaxhead method)": [[142, "scope_rl.policy.head.SoftmaxHead.sample_action_and_output_pscore"], [148, "scope_rl.policy.head.SoftmaxHead.sample_action_and_output_pscore"]], "sample_action_and_output_pscore() (scope_rl.policy.head.truncatedgaussianhead method)": [[142, "scope_rl.policy.head.TruncatedGaussianHead.sample_action_and_output_pscore"], [149, "scope_rl.policy.head.TruncatedGaussianHead.sample_action_and_output_pscore"]], "sample_action_and_output_pscore_online() (scope_rl.policy.head.basehead method)": [[142, "scope_rl.policy.head.BaseHead.sample_action_and_output_pscore_online"], [143, "scope_rl.policy.head.BaseHead.sample_action_and_output_pscore_online"]], "sample_action_online() (scope_rl.policy.head.basehead method)": [[142, "scope_rl.policy.head.BaseHead.sample_action_online"], [143, "scope_rl.policy.head.BaseHead.sample_action_online"]], "scope_rl.policy.head": [[142, "module-scope_rl.policy.head"]], "minmaxactionscaler (class in scope_rl.utils)": [[150, "scope_rl.utils.MinMaxActionScaler"], [151, "scope_rl.utils.MinMaxActionScaler"]], "minmaxscaler (class in scope_rl.utils)": [[150, "scope_rl.utils.MinMaxScaler"], [152, "scope_rl.utils.MinMaxScaler"]], "multipleinputdict (class in scope_rl.utils)": [[150, "scope_rl.utils.MultipleInputDict"], [153, "scope_rl.utils.MultipleInputDict"]], "multipleloggeddataset (class in scope_rl.utils)": [[150, "scope_rl.utils.MultipleLoggedDataset"], [154, "scope_rl.utils.MultipleLoggedDataset"]], "newgymapiwrapper (class in scope_rl.utils)": [[150, "scope_rl.utils.NewGymAPIWrapper"], [155, "scope_rl.utils.NewGymAPIWrapper"]], "oldgymapiwrapper (class in scope_rl.utils)": [[150, "scope_rl.utils.OldGymAPIWrapper"], [156, "scope_rl.utils.OldGymAPIWrapper"]], "add() (scope_rl.utils.multipleinputdict method)": [[150, "scope_rl.utils.MultipleInputDict.add"], [153, "scope_rl.utils.MultipleInputDict.add"]], "add() (scope_rl.utils.multipleloggeddataset method)": [[150, "scope_rl.utils.MultipleLoggedDataset.add"], [154, "scope_rl.utils.MultipleLoggedDataset.add"]], "check_array() (in module scope_rl.utils)": [[150, "scope_rl.utils.check_array"], [157, "scope_rl.utils.check_array"]], "check_input_dict() (in module scope_rl.utils)": [[150, "scope_rl.utils.check_input_dict"], [158, "scope_rl.utils.check_input_dict"]], "check_logged_dataset() (in module scope_rl.utils)": [[150, "scope_rl.utils.check_logged_dataset"], [159, "scope_rl.utils.check_logged_dataset"]], "cosine_kernel() (in module scope_rl.utils)": [[150, "scope_rl.utils.cosine_kernel"], [160, "scope_rl.utils.cosine_kernel"]], "defaultdict_to_dict() (in module scope_rl.utils)": [[150, "scope_rl.utils.defaultdict_to_dict"], [161, "scope_rl.utils.defaultdict_to_dict"]], "epanechnikov_kernel() (in module scope_rl.utils)": [[150, "scope_rl.utils.epanechnikov_kernel"], [162, "scope_rl.utils.epanechnikov_kernel"]], "estimate_confidence_interval_by_bootstrap() (in module scope_rl.utils)": [[150, "scope_rl.utils.estimate_confidence_interval_by_bootstrap"], [163, "scope_rl.utils.estimate_confidence_interval_by_bootstrap"]], "estimate_confidence_interval_by_empirical_bernstein() (in module scope_rl.utils)": [[150, "scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein"], [164, "scope_rl.utils.estimate_confidence_interval_by_empirical_bernstein"]], "estimate_confidence_interval_by_hoeffding() (in module scope_rl.utils)": [[150, "scope_rl.utils.estimate_confidence_interval_by_hoeffding"], [165, "scope_rl.utils.estimate_confidence_interval_by_hoeffding"]], "estimate_confidence_interval_by_t_test() (in module scope_rl.utils)": [[150, "scope_rl.utils.estimate_confidence_interval_by_t_test"], [166, "scope_rl.utils.estimate_confidence_interval_by_t_test"]], "fit() (scope_rl.utils.minmaxactionscaler method)": [[150, "scope_rl.utils.MinMaxActionScaler.fit"], [151, "scope_rl.utils.MinMaxActionScaler.fit"]], "fit() (scope_rl.utils.minmaxscaler method)": [[150, "scope_rl.utils.MinMaxScaler.fit"], [152, "scope_rl.utils.MinMaxScaler.fit"]], "fit_with_env() (scope_rl.utils.minmaxactionscaler method)": [[150, "scope_rl.utils.MinMaxActionScaler.fit_with_env"], [151, "scope_rl.utils.MinMaxActionScaler.fit_with_env"]], "fit_with_env() (scope_rl.utils.minmaxscaler method)": [[150, "scope_rl.utils.MinMaxScaler.fit_with_env"], [152, "scope_rl.utils.MinMaxScaler.fit_with_env"]], "gaussian_kernel() (in module scope_rl.utils)": [[150, "scope_rl.utils.gaussian_kernel"], [167, "scope_rl.utils.gaussian_kernel"]], "get() (scope_rl.utils.multipleinputdict method)": [[150, "scope_rl.utils.MultipleInputDict.get"], [153, "scope_rl.utils.MultipleInputDict.get"]], "get() (scope_rl.utils.multipleloggeddataset method)": [[150, "scope_rl.utils.MultipleLoggedDataset.get"], [154, "scope_rl.utils.MultipleLoggedDataset.get"]], "get_params() (scope_rl.utils.minmaxactionscaler method)": [[150, "scope_rl.utils.MinMaxActionScaler.get_params"], [151, "scope_rl.utils.MinMaxActionScaler.get_params"]], "get_params() (scope_rl.utils.minmaxscaler method)": [[150, "scope_rl.utils.MinMaxScaler.get_params"], [152, "scope_rl.utils.MinMaxScaler.get_params"]], "l2_distance() (in module scope_rl.utils)": [[150, "scope_rl.utils.l2_distance"], [168, "scope_rl.utils.l2_distance"]], "n_eval_policies (scope_rl.utils.multipleinputdict property)": [[150, "scope_rl.utils.MultipleInputDict.n_eval_policies"], [153, "scope_rl.utils.MultipleInputDict.n_eval_policies"]], "reverse_transform() (scope_rl.utils.minmaxactionscaler method)": [[150, "scope_rl.utils.MinMaxActionScaler.reverse_transform"], [151, "scope_rl.utils.MinMaxActionScaler.reverse_transform"]], "reverse_transform() (scope_rl.utils.minmaxscaler method)": [[150, "scope_rl.utils.MinMaxScaler.reverse_transform"], [152, "scope_rl.utils.MinMaxScaler.reverse_transform"]], "reverse_transform_numpy() (scope_rl.utils.minmaxactionscaler method)": [[150, "scope_rl.utils.MinMaxActionScaler.reverse_transform_numpy"], [151, "scope_rl.utils.MinMaxActionScaler.reverse_transform_numpy"]], "scope_rl.utils": [[150, "module-scope_rl.utils"]], "transform() (scope_rl.utils.minmaxactionscaler method)": [[150, "scope_rl.utils.MinMaxActionScaler.transform"], [151, "scope_rl.utils.MinMaxActionScaler.transform"]], "transform() (scope_rl.utils.minmaxscaler method)": [[150, "scope_rl.utils.MinMaxScaler.transform"], [152, "scope_rl.utils.MinMaxScaler.transform"]], "triangular_kernel() (in module scope_rl.utils)": [[150, "scope_rl.utils.triangular_kernel"], [169, "scope_rl.utils.triangular_kernel"]], "uniform_kernel() (in module scope_rl.utils)": [[150, "scope_rl.utils.uniform_kernel"], [170, "scope_rl.utils.uniform_kernel"]], "use_same_eval_policy_across_dataset (scope_rl.utils.multipleinputdict property)": [[150, "scope_rl.utils.MultipleInputDict.use_same_eval_policy_across_dataset"], [153, "scope_rl.utils.MultipleInputDict.use_same_eval_policy_across_dataset"]], "get_type() (scope_rl.utils.minmaxactionscaler method)": [[151, "scope_rl.utils.MinMaxActionScaler.get_type"]], "get_type() (scope_rl.utils.minmaxscaler method)": [[152, "scope_rl.utils.MinMaxScaler.get_type"]]}})
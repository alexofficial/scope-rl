==========
Supported Implementation
==========

Please also refer to `quickstart <>`_ for the example usages.

Basic Off-Policy Evaluation (OPE)
~~~~~~~~~~
The goal of (basic) OPE is to evaluate the following expected trajectory-wise reward of a policy (referred to as policy value).

.. math::

    J(\pi) := \mathbb{E}_{\tau} \left [ \sum_{t=0}^{T-1} \gamma^t r_{t} \mid \pi \right ],

where :math:`\pi` is the (evaluation) policy, :math:`\tau` is the trajectory observed by the evaluation policy, and :math:`r_t` is the immediate reward at each timestep. 
(Please refer to the `problem setup <>`_ for additional notations.)


Here, we describe the class for conducting OPE and the implemented OPE estimators for estimating the policy value. 
We begin with the :class:`OffPolicyEvaluation` class to streamline the OPE procedure.

.. code-block:: python

    # initialize the OPE class
    >>> from ofrl.ope import OffPolicyEvaluation as OPE
    >> ope = OPE(
            logged_dataset=logged_dataset,
            ope_estimators=[DM(), TIS(), PDIS(), DR()],
        )

Using the OPE class, we can obtain the OPE results of various estimators at once as follows.

.. code-block:: python

    >>> ope_dict = ope.estimate_policy_value(input_dict)

where :class:`input_dict` is generated by the :class:`CreateOPEInput` class as follows.

.. code-block:: python

    # create input for OPE class
    >>> from ofrl.ope import CreateOPEInput
    >>> prep = CreateOPEInput(
            env=env,
            logged_dataset=logged_dataset,
            require_value_prediction=True,  # use model-based prediction
        )
    >>> input_dict = prep.obtain_whole_inputs(
            evaluation_policies=evaluation_policies,
            n_trajectories_on_policy_evaluation=100,
            random_state=random_state,
        )

Note that, for the description of the :class:`logged_dataset`, please refer `here <>`_.

The OPE class implements the following functions.

(OPE)

* :class:`estimate_policy_value`
* :class:`estimate_intervals`
* :class:`summarize_off_policy_estimates`

(Evaluation of OPE estimators)

* :class:`evaluate_performance_of_ope_estimators`

(Visualization)

* :class:`visualize_off_policy_estimates`

Below, we describe the implemented OPE estimators.

Direct Method (DM)
----------
DM is a model-based approach which uses the initial state value estimated by Fitted Q Evaluation (FQE).
It first learns the Q-function and then leverages the learned Q-function as follows.

.. math::

    \hat{J}_{\mathrm{DM}} (\pi; \mathcal{D}) := \mathbb{E}_n \left[ \mathbb{E}_{a_0 \sim \pi(a_0 | s_0)} [\hat{Q}(s_0, a_0)] \right],

where :math:`\mathcal{D}=\{\{(s_t, a_t, r_t)\}_{t=0}^T\}_{i=1}^n` is the logged dataset with :math:`n` trajectories of data.
:math:`T` indicates step per episode. :math:`\hat{Q}(s_t, a_t)` is estimated state-action value.

DM has low variance, but can incur bias caused by approximation errors.

Note that, we use the implementation of FQE provided in `d3rlpy <https://github.com/takuseno/d3rlpy>`_.

Trajectory-wise Importance Sampling (TIS)
----------

TIS uses importance sampling technique to correct the distribution shift between :math:`\pi` and :math:`\pi_0` as follows.

.. math::

    \hat{J}_{\mathrm{TIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t w_{1:T-1} r_t \right],

where :math:`w_{0:T-1} := \prod_{t=0}^{T-1} (\pi(a_t | s_t) / \pi_0(a_t | s_t))` is the importance weight.

TIS enables an unbiased estimation of the policy value. However, when the trajectory length :math:`T` is large, TIS suffers from high variance
due to the product of importance weights.

Per-Decision Importance Sampling (PDIS)
----------
PDIS leverages the sequential nature of the MDP to reduce the variance of TIS.
Specifically, since :math:`s_t` only depends on :math:`s_0, \ldots, s_{t-1}` and :math:`a_0, \ldots, a_{t-1}` and is independent of :math:`s_{t+1}, \ldots, s_{T}` and :math:`a_{t+1}, \ldots, a_{T}`,
PDIS only considers the importance weight of the past interactions when estimating :math:`r_t` as follows.

.. math::

    \hat{J}_{\mathrm{PDIS}} (\pi; \mathcal{D}) := \mathbb{E}_{n} \left[ \sum_{t=0}^{T-1} \gamma^t w_{0:t} r_t \right],

where :math:`w_{0:t} := \prod_{t'=0}^t (\pi_e(a_{t'} \mid s_{t'}) / \pi_b(a_{t'} \mid s_{t'}))` is the importance weight of past interactions.

PDIS remains unbiased while reducing the variance of TIS. However, when :math:`t` is large, PDIS still suffers from high variance.

Doubly Robust (DR)
----------
DR is a hybrid of model-based estimation and importance sampling.
It introduces :math:`\hat{Q}` as a baseline estimation in the recursive form of PDIS and applies importance weighting only on its residual.

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} \left[\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a | s_t)}[\hat{Q}(s_t, a)])\right],

DR is unbiased and reduces the variance of IPS when :math:`\hat{Q}(\cdot)` is reasonably accurate to satisfy :math:`0 < \hat{Q}(\cdot) < 2 Q(\cdot)`.


Self-Normalized estimators
----------
Self-normalized estimators aims to reduce the scale of importance weight for the variance reduction purpose.
Specifically, it substitute importance weight :math:`w_{\ast}` as follows.

.. math::

    \tilde{w}_{\ast} := w_{\ast} / \mathbb{E}_{n}[w_{\ast}]

where :math:`\tilde{w}_{\ast}` is the self-normalized importance weight.

Self-normalized estimators has variance bounded by :math:`r_{max}^2` while also being consistent.


Marginalized Importance Sampling Estimators
----------
When the length of trajectory (:math:`T`) is large, even per-decision importance weights can exponentially large in the latter part of the trajectory.
To alleviate this, state marginal or state-action marginal importance weights can be used instead of the per-decision importance weight as follows.

.. math::

    w_{s, a}(s, a) := d^{\pi}(s, a) / d^{\pi_0}(s, a)

    w_s(s) := d^{\pi}(s) / d^{\pi_0}(s)

Then, the importance weight is replaced as follows.

.. math::

    w(s_t, a_t) = w_{s, a}(s_t, a_t)

    w(s_t, a_t) = w_{s}(s_t) \frac{\pi(a_t | s_t)}{\pi_0(a_t | s_t)}

This estimator is particularly useful when policy visits the same or similar states among different trajectories or different timestep.
(e.g., when the state transition is something like :math:`cdots \rightarrow s_1 \rightarrow s_2 \rightarrow s_1 \rightarrow s_2 \rightarrow cdots` or when the trajectories always visits some particular state as :math:`\cdots \rightarrow s_{*} \rightarrow s_{1} \rightarrow s_{*} \rightarrow \cdots`)

Note that, to use marginalized importance sampling estimators, we need to first estimate the state marginal or state-action marginal importance weight.
A dominant way to do this is to leverage the following relationship between the importance weights and the state-action value function under the assumption that the state visitation probability is consistent across various timesteps.

.. math::

    \mathbb{E}_{(s, a, r, s') \sim \mathcal{D_{\pi_0}}}[w(s, a) r] 
    = \mathbb{E}_{(s, a, r, s') \sim \mathcal{D_{\pi_0}}}[w(s, a)(Q_{\pi}(s, a) - \gamma \mathbb{E}_{a' \sim \pi(a' | s')}[Q(s', a')])]
    = (1 - \gamma) \mathbb{E}_{s_0 \sim d^{\pi}(s_0), a_0 \sim \pi(a_0 | s_0)}[Q_{\pi}(s_0, a_0)]

The objective of weight learning is to minimize the difference between the middle term and the last term of the above equation when Q-function adversarially maximizes the difference.
In particular, we provide the following algorithms to estimate state marginal and state-action marginal importance weights (and corresponding state-action value function) via minimax learning.

* Augmented Lagrangian Method (ALM): 
This method simultaneously optimize both :math:`w(s, a)` and :math:`Q(s, a)`. By setting different hyperparameters, ALM can be identical to BestDICE, DualDICE, GenDICE, AlgaeDICE, and MQL/MWL. 

* Minimax Q-Learning and Weight Learning (MQL/MWL): 
This method assumes that one of the value function or weight function is expressed by a function class in a reproducing kernel Hilbert space (RKHS) and optimizes only either value function or weight function. 

We implement state marginal and state-action marginal OPE estimators in the following classes (both for :class:`Discrete-` and :class:`Continuous-` action spaces).

(State Marginal Estimators)

* :class:`StateMarginalDirectMethod`
* :class:`StateMarginalImportanceSampling`
* :class:`StateMarginalDoublyRobust`
* :class:`StateMarginalSelfNormalizedImportanceSampling`
* :class:`StateMarginalSelfNormalizedDoublyRobust`

(State-Action Marginal Estimators)

* :class:`StateActionMarginalImportanceSampling`
* :class:`StateActionMarginalDoublyRobust`
* :class:`StateActionMarginalSelfNormalizedImportanceSampling`
* :class:`StateActionMarginalSelfNormalizedDoublyRobust`

Note that, Doubly Robust (DR) estimators exploit slightly different formulations when using the standard OPE estimator and marginal OPE estimator as follows.

(DR in standard OPE)

.. math::

    \hat{J}_{\mathrm{DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} [\sum_{t=0}^{T-1} \gamma^t (w_{0:t} (r_t - \hat{Q}(s_t, a_t)) + w_{0:t-1} \mathbb{E}_{a \sim \pi(a \mid s_t)}[\hat{Q}(s_t, a)])],

(DR in marginal OPE)

.. math::

    \hat{J}_{\mathrm{SAM-DR}} (\pi; \mathcal{D})
    := \mathbb{E}_{n} [\hat{Q}(s_0, a_0)]
        + \mathbb{E}_{n} [\sum_{t=0}^{T-1} \gamma^t w_{s, a}(s_t, a_t) (r_t + \gamma \mathbb{E}_{a \sim \pi(a \mid s_t)}[\hat{Q}(s_{t+1}, a)] - \hat{Q}(s_t, a_t))],


Double Reinforcement Learning (DRL)
----------
We have seen that DR in standard OPE and marginal OPE have a slightly different formulation. 
Then, a natural question arises, would it be possible to use marginal importance weight in DR in the standard formulation?

DRL leverages the marginal importance sampling in the standard OPE formulation as follows.

.. math::

    \hat{J}_{\mathrm{DRL}} (\pi; \mathcal{D})
    := \frac{1}{n} \sum_{k=1}^K \sum_{i=1}^{n_k} \sum_{t=0}^{T-1} (w_s^j(s_{i,t}, a_{i, t}) (r_{i, t} - Q^j(s_{i, t}, a_{i, t}))
        + w_s^j(s_{i, t-1}, a_{i, t-1}) \mathbb{E}_{a \sim \pi(a \mid s_t)}[Q^j(s_{i, t}, a)] )

Note that, DRL uses "cross-fitting" as an additional strategy to achieve a statistical efficiency.
Specifically, let :math:`K` is the number of folds and :math:`\mathcal{D}_j` is the :math:`j`-th split of logged data consisting of :math:`n_k` samples.
Cross-fitting trains :math:`w^j` and :math:`Q^j` on the subset of data used for OPE, i.e., :math:`\mathcal{D} \setminus \mathcal{D}_j`.


Spectrum of Off-Policy Estimators (SOPE)
----------
While state marginal or state-action marginal importance weight effectively alleviates the variance of per-decision importance weight, the estimation error of marginal importance weights
may introduce some bias in estimation. To alleviate this and control the bias-variance tradeoff more flexibly, SOPE uses the following interpolated importance weights.

.. math::

    w(s_t, a_t) = 
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        w_{s, a}(s_{t-k}, a_{t-k}) \prod_{t'=t-k+1}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases}

    w(s_t, a_t) = 
    \begin{cases}
        \prod_{t'=0}^{k-1} w_t(s_{t'}, a_{t'}) & \mathrm{if} \, t < k \\
        w_{s}(s_{t-k}) \prod_{t'=t-k}^{t} w_t(s_{t'}, a_{t'}) & \mathrm{otherwise}
    \end{cases}
    
where we denote :math:`w_t(s_t, a_t) := \pi(a_t | s_t) / \pi_0(a_t | s_t)`.

SOPE is available by specifying :class:`n_step_pdis` in the state marginal and state-action marginal estimators.

.. code-block:: python

    >> ope = OPE(
        logged_dataset=logged_dataset,
        ope_estimators=[SMIS(), SMDR(), SAMIS(), SAMDR()],  # marginal estimators
    )
    >>> ope.estimate_policy_value(input_dict, n_step_pdis=5)

:class:`n_step_pdis=0` is equivalent to the original marginal OPE estimators.


Extension to the continuous action space
----------
When the action space is continuous, the naive importance weight :math:`w_t = \pi(a_t|s_t) / \pi_0(a_t|s_t) = (\pi(a |s_t) / \pi_0(a_t|s_t)) \cdot \mathbb{I}(a = a_t)` rejects almost every actions,
as :math:`\mathbb{I}(a = a_t)` filters only the action observed in the logged data.

To address this issue, continuous-action OPE estimators apply kernel density estimation technique to smooth the importance weight.

.. math::

    \overline{w}_t = \int_{a \in \mathcal{A}} \frac{\pi(a \mid s_t)}{\pi_0(a_t | s_t)} \cdot \frac{1}{h} K \left( \frac{a - a_t}{h} \right) da,

where :math:`K(\cdot)` denotes a kernel function and :math:`h` is the bandwidth hyperparameter.
We can use any function as :math:`K(\cdot)` that meets the following qualities:

* 1) :math:`\int xK(x) dx = 0`,
* 2) :math:`\int K(x) dx = 1`,
* 3) :math:`\lim _{x \rightarrow-\infty} K(x)=\lim _{x \rightarrow+\infty} K(x)=0`,
* 4) :math:`K(x) \geq 0, \forall x`.

In our implementation, we use the (distance-based) Gaussian kernel :math:`K(x)=\frac{1}{\sqrt{2 \pi}} e^{-\frac{x^{2}}{2}}`.


High Confidence Off-Policy Evaluation (HC-OPE)
----------
To alleviate the risk of optimistic estimation, we are often interested in the confidence intervals and the lower bound of the estimated policy value.
We implement four methods to estimate the confidence intervals.

* Hoeffding: 

.. math::

    |\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \hat{J}_{\max} \displaystyle \sqrt{\frac{\log(1 / \alpha)}{2 n}}.

* Empirical Bernstein: 

.. math::

    |\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \displaystyle \frac{7 \hat{J}_{\max} \log(2 / \alpha)}{3 (n - 1)} + \displaystyle \sqrt{\frac{2 \hat{\mathbb{V}}_{\mathcal{D}}(\hat{J}) \log(2 / \alpha)}{(n - 1)}}.

* Student T-test: 

.. math::

    |\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \displaystyle \frac{T_{\mathrm{test}}(1 - \alpha, n-1)}{\sqrt{n} / \hat{\sigma}}.

* Bootstrapping: 

.. math::

    |\hat{J}(\pi; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{J}(\pi; \mathcal{D})]| \leq \mathrm{Bootstrap}(1 - \alpha).

Note that, all the above bound holds with probability :math:`1 - \alpha`.
For notations, we denote :math:`\hat{\mathbb{V}}_{\mathcal{D}}(\cdot)` to be the sample variance,
:math:`T_{\mathrm{test}}(\cdot,\cdot)` to be T value,
and :math:`\sigma` to be the standard deviation.

Among the above high confidence interval estimation, hoeffding and empirical bernstein derives lower bound without any distribution assumption of :math:`p(\hat{J})`, which sometimes leads to quite conservative estimation.
On the other hand, T-test is based on the assumption that each sample of :math:`p(\hat{J})` follows the normal distribution.

For further descriptions, please also refer to `package reference <>`_.
The quickstart example is also available `here <>`_.


Cumulative Distribution Off-Policy Evaluation (CD-OPE)
~~~~~~~~~~

While the basic OPE aims to estimate the average policy performance, we are often also interested in the performance distribution of the evaluation policy.
Cumulative distribution OPE enables flexible estimation of various risk functions such as variance and conditional value at risk (CVaR) using the cumulative distribution function (CDF).

(Cumulative Distribution Function)

.. math::

    F(m, \pi) := \mathbb{E} \left[ \mathbb{I} \left \{ \sum_{t=0}^{T-1} \gamma^t r_t \leq m \right \} \mid \pi \right]
    
(Risk Functions derived by CDF)

* Mean: :math:`\mu(F) := \int_{G} G \, \mathrm{d}F(G)`
* Variance: :math:`\sigma^2(F) := \int_{G} (G - \mu(F))^2 \, \mathrm{d}F(G)`
* :math:`\alpha`-quartile: :math:`Q^{\alpha}(F) := \min \{ G \mid F(G) \leq \alpha \}`
* Conditional Value at Risk (CVaR): :math:`\int_{G} G \, \mathbb{I}\{ G \leq Q^{\alpha}(F) \} \, \mathrm{d}F(G)`

where we let :math:`G := \sum_{t=0}^{T-1} \gamma^t r_t` to represent the random variable of trajectory wise reward
and :math:`dF(G) := \mathrm{lim}_{\Delta \rightarrow 0} F(G) - F(G- \Delta)`.

To estimate both CDF and various risk functions, we provide the following :class:`CumulativeDistributionOffPolicyEvaluation` class.

.. code-block:: python

    # initialize the OPE class
    >>> from ofrl.ope import CumulativeDistributionOffPolicyEvaluation as CumulativeDistributionOPE
    >>> cd_ope = CumulativeDistributionOPE(
            logged_dataset=logged_dataset,
            ope_estimators=[CD_DM(), CD_IS(), CD_DR()],
        )

It estimates the cumulative distribution of the trajectory wise reward and various risk functions as follows.

.. code-block:: python

    >>> cdf_dict = cd_ope.estimate_cumulative_distribution_function(input_dict)
    >>> variance_dict = cd_ope.estimate_variance(input_dict)

The cumulative distribution OPE class implements the following functions.

(Cumulative Distribution Function)

* :class:`estimate_cumulative_distribution_function`

(Risk Functions and Statistics)

* :class:`estimate_mean`
* :class:`estimate_variance`
* :class:`estimate_conditional_value_at_risk`
* :class:`estimate_interquartile_range`

(Visualization)

* :class:`visualize_policy_value`
* :class:`visualize_policy_value_with_variance`
* :class:`visualize_conditional_value_at_risk`
* :class:`visualize_interquartile_range`
* :class:`visualize_cumulative_distribution_function`

(Others)

* :class:`obtain_reward_scale`


Direct Method (DM)
----------

DM adopts model-based approach to estimate the cumulative distribution function.

.. math::

        \hat{F}_{\mathrm{DM}}(m, \pi; \mathcal{D}) := \mathbb{E}_{n} \left[ \mathbb{E}_{a_0 \sim \pi(a_0 | s_0)} \hat{G}(m; s_0, a_0) \right]

where :math:`\hat{F}(\cdot)` is the estimated cumulative distribution function and :math:`\hat{G}(\cdot)` is the estimated conditional distribution.

DM is vulnerable to the approximation error, but has low variance.

Trajectory-wise Importance Sampling (TIS)
----------

TIS corrects the distribution shift by applying importance sampling technique on the cumulative distribution estimation.

.. math::

        \hat{F}_{\mathrm{TIS}}(m, \pi; \mathcal{D}) := \mathbb{E}_{n} \left[ w_{0:T-1} \mathbb{I} \left \{\sum_{t=0}^{T-1} \gamma^t r_t \leq m \right \} \right]

TIS is unbiased but can suffer from high variance.
In particular, :math:`\hat{F}_{\mathrm{TIS}}(\cdot)` sometimes becomes more than one when the variance is high.
Therefore, we correct CDF as :math:`\hat{F}^{\ast}_{\mathrm{TIS}}(m, \pi; \mathcal{D}) := \min(\max_{m' \leq m} \hat{F}_{\mathrm{TIS}}(m', \pi; \mathcal{D}), 1)` :cite:`huang2021off`.


Trajectory-wise Doubly Robust (TDR)
----------

TDR combines TIS and DM to reduce the variance while being unbiased.

.. math::

    \hat{F}_{\mathrm{TDR}}(m, \pi; \mathcal{D})
    := \mathbb{E}_{n} \left[ w_{0:T-1} \left( \mathbb{I} \left \{\sum_{t=0}^{T-1} \gamma^t r_t \leq m \right \} - \hat{G}(m; s_0, a_0) \right) \right]
    + \hat{F}_{\mathrm{DM}}(m, \pi; \mathcal{D})

TDR reduces the variance of TIS while being unbiased, leveraging the model-based estimate (i.e., DM) as a control variate.
Since :math:`\hat{F}_{\mathrm{TDR}}(\cdot)` may be less than zero or more than one, we should apply the following transformation to bound :math:`\hat{F}_{\mathrm{TDR}}(\cdot) \in [0, 1]` :cite:`huang2021off`.

.. math::

    \hat{F}^{\ast}_{\mathrm{TIS}}(m, \pi; \mathcal{D}) := \mathrm{clip}(\max_{m' \leq m} \hat{F}_{\mathrm{TIS}}(m', \pi; \mathcal{D}), 0, 1).

Note that, this estimator is not equivalent to the (recursive) DR estimator defined by :cite:`huang2022off`. We are planning to implement the recursive version in a future update of the software.

Finally, we also provide the self-normalized estimators for TIS and TDR.
They use the self-normalized importance weight :math:`\tilde{w}_{\ast} := w_{\ast} / \mathbb{E}_{n}[w_{\ast}]` for the variance reduction purpose.

For further descriptions, please also refer to `package reference <>`_.
The quickstart example is also available `here <>`_.

Evaluation Metrics of OPE/OPS
~~~~~~~~~~
Finally, we describe the metrics to evaluate the quality of OPE estimators and its OPS result.

* Mean Squared Error (MSE): 

This metrics measures the estimation accuracy as :math:`\sum_{\pi \in \Pi} (\hat{J}(\pi; \mathcal{D}) - J(\pi))^2 / |\Pi|`.

* Regret@k: 

This metrics measures how well the selected policy(ies) performs. In particular, Regret@1 indicates the expected performance difference between the (oracle) best policy and the selected policy as :math:`J(\pi^{\ast}) - J(\hat{\pi}^{\ast})`, where :math:`\pi^{\ast} := {\arg\max}_{\pi \in \Pi} J(\pi)` and :math:`\hat{\pi}^{\ast} := {\arg\max}_{\pi \in \Pi} \hat{J}(\pi; \mathcal{D})`.

* Spearman's Rank Correlation Coefficient: 

This metrics measures how well the raking of the candidate estimators are preserved in the OPE result.

* Type I and Type II Error Rate: 

This metrics measures how well an OPE estimator validates whether the policy performance surpasses the given safety threshold or not.

To ease the comparison of candidate (evaluation) policies and the OPE estimators, we provide the :class:`OffPolicySelection` class.

.. code-block:: python

    # Initialize the OPS class
    >>> from ofrl.ope import OffPolicySelection
    >>> ops = OffPolicySelection(
            ope=ope,
            cumulative_distribution_ope=cd_ope,
        )

The OPS class returns both the OPE results and the OPS metrics as follows.

.. code-block:: python

    >>> ranking_df, metric_df = ops.select_by_policy_value(
            input_dict,
            return_metrics=True,
            return_by_dataframe=True,
        )

Moreover, the OPS class enables us to validate the best/worst/mean performance of top k deployment and how well the safety requirement is satisfied.

.. code-block:: python

    >>> ops.visualize_topk_policy_value_selected_by_standard_ope(
        input_dict=input_dict,
        safety_criteria=1.0,
    )

Finally, the OPS class also implements the modules to compare the OPE result and the true policy metric as follows.

.. code-block:: python

    >>> ops.visualize_policy_value_for_validation(
            input_dict=input_dict,
            n_cols=4,
            share_axes=True,
        )

The OPS class implements the following functions.

(OPS)

* :class:`obtain_oracle_selection_result`
* :class:`select_by_policy_value`
* :class:`select_by_policy_value_via_cumulative_distribution_ope`
* :class:`select_by_policy_value_lower_bound`
* :class:`select_by_lower_quartile`
* :class:`select_by_conditional_value_at_risk`

(Visualization)

* :class:`visualize_policy_value_for_selection`
* :class:`visualize_cumulative_distribution_function_for_selection`
* :class:`visualize_policy_value_for_selection`
* :class:`visualize_policy_value_of_cumulative_distribution_ope_for_selection`
* :class:`visualize_conditional_value_at_risk_for_selection`
* :class:`visualize_interquartile_range_for_selection`

(Visualization of top k performance)

* :class:`visualize_topk_policy_value_selected_by_standard_ope`
* :class:`visualize_topk_policy_value_selected_by_cumulative_distribution_ope`
* :class:`visualize_topk_policy_value_selected_by_lower_bound`
* :class:`visualize_topk_conditional_value_at_risk_selected_by_standard_ope`
* :class:`visualize_topk_conditional_value_at_risk_selected_by_cumulative_distribution_ope`
* :class:`visualize_topk_lower_quartile_selected_by_standard_ope`
* :class:`visualize_topk_lower_quartile_selected_by_cumulative_distribution_ope`

(Validation Visualization)

* :class:`visualize_policy_value_for_validation`
* :class:`visualize_policy_value_of_cumulative_distribution_ope_for_validation`
* :class:`visualize_policy_value_lower_bound_for_validation`
* :class:`visualize_variance_for_validation`
* :class:`visualize_lower_quartile_for_validation`
* :class:`visualize_conditional_value_at_risk_for_validation`

For further descriptions, please also refer to `package reference <>`_.
The quickstart example is also available `here <>`_.
